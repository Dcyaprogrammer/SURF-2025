{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_61BfNpTX1L"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaMjP4WH151p"
      },
      "outputs": [],
      "source": [
        "!pip install gymnax\n",
        "!pip install distrax\n",
        "!pip install git+https://github.com/riiswa/pointax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper"
      ],
      "metadata": {
        "id": "MbXVIHPyIyu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.environments import environment, spaces\n",
        "from pointax.types import EnvState, EnvParams\n",
        "import jax\n",
        "\n",
        "class PMwrapper(environment.Environment[EnvState, EnvParams]):\n",
        "    def __init__(self, pm_env):\n",
        "        super().__init__()\n",
        "        self.pm_env = pm_env\n",
        "\n",
        "    def default_params(self):\n",
        "        # return EnvParams()\n",
        "        return self.pm_env.default_params\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        obs, state, reward, done, info = self.pm_env.step_env(key, state, action, params)\n",
        "        n_obs = obs[:4]\n",
        "        goal = obs[4:]\n",
        "        info[\"goal_position\"] = goal\n",
        "        return n_obs, state, reward, done, info\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        obs, state = self.pm_env.reset_env(key, params)\n",
        "        obs = obs[:4]\n",
        "        return obs, state\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        obs = self.pm_env.get_obs(state, params)\n",
        "        obs = obs[:4]\n",
        "        return obs\n",
        "\n",
        "    def name(self):\n",
        "        suffix = \"Dense\" if self.pm_env.reward_type_str == \"dense\" else \"\"\n",
        "        return f\"pointax/PointMaze_{self.pm_env.maze_id}{suffix}\"\n",
        "\n",
        "    def num_actions(self):\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params=None):\n",
        "        return self.pm_env.action_space(params)\n",
        "\n",
        "    def observation_space(self, params):\n",
        "        return spaces.Box(low=-jnp.inf, high=jnp.inf, shape=(4,), dtype=jnp.float32)\n",
        ""
      ],
      "metadata": {
        "id": "LKrZbSUWI06I"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pointax\n",
        "env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "params = env.default_params()\n",
        "\n",
        "# Reset and step\n",
        "key = jax.random.PRNGKey(42)\n",
        "obs, state = env.reset_env(key, params)\n",
        "\n",
        "action = jax.numpy.array([0.5, 0.0])  # Move right\n",
        "obs, state, reward, done, info = env.step_env(key, state, action, params)\n",
        "\n",
        "print(obs, obs.shape)\n",
        "print(f\"Reward: {reward}, Success: {info['is_success']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNv0jJ4GkWh",
        "outputId": "218c3ea6-b417-41d1-8ba0-9ba13f980409"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.02 0.91 0.5  0.  ] (4,)\n",
            "Reward: 0.0, Success: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6LAZ_lUbbC"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aHxBAU0DucTa"
      },
      "outputs": [],
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import pointax\n",
        "\n",
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def update(self,obs,actions,next_obs,dones,info):\n",
        "      #update variable parameters\n",
        "        return #{\"kl\":KL} MI = E KL\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __call__(self,observations,rng):\n",
        "\n",
        "        return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "from gymnax.environments.environment import Environment\n",
        "class CustomRolloutWrapper:\n",
        "    \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_or_name: Union[str,Environment] = \"Pendulum-v1\",\n",
        "        num_env_steps: Optional[int] = None,\n",
        "        env_kwargs: Any | None = None,\n",
        "        env_params: Any | None = None,\n",
        "    ):\n",
        "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "        # Define the RL environment & network forward function\n",
        "        if env_kwargs is None:\n",
        "            env_kwargs = {}\n",
        "        if env_params is None:\n",
        "            env_params = {}\n",
        "        if isinstance(env_or_name,Environment):\n",
        "            self.env = env_or_name\n",
        "            self.env_params = env_or_name.default_params\n",
        "        else:\n",
        "            self.env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "            self.env_params = self.env.default_params()\n",
        "        self.env_params = self.env_params.replace(**env_params)\n",
        "\n",
        "        if num_env_steps is None:\n",
        "            self.num_env_steps = self.env_params.max_steps_in_episode\n",
        "        else:\n",
        "            self.num_env_steps = num_env_steps\n",
        "\n",
        "#    @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_reset(self,rng_input):\n",
        "        batch_reset = jax.vmap(self.single_reset_state)\n",
        "        return batch_reset(rng_input)\n",
        "\n",
        " #   @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def single_reset_state(self,rng_input):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "        obs, state = self.env.reset(rng_reset, self.env_params)\n",
        "        return state\n",
        "\n",
        "   # @functools.partial(nnx.jit, static_argnums=(0,4))\n",
        "    def batch_rollout(self, rng_eval, model:UnsupervisedExplorer,\n",
        "                      env_state=None,num_steps=1):\n",
        "        \"\"\"Evaluate a generation of networks on RL/Supervised/etc. task.\"\"\"\n",
        "        # vmap over different MC fitness evaluations for single network\n",
        "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0, None,0,None))\n",
        "        return batch_rollout(rng_eval, model, env_state,num_steps)\n",
        "\n",
        "    # @functools.partial(nnx.jit, static_argnums=(0,4))\n",
        "    def single_rollout(self, rng_input, model:UnsupervisedExplorer,\n",
        "                       env_state=None,num_steps=1):\n",
        "        \"\"\"Rollout a pendulum episode with lax.scan.\"\"\"\n",
        "        # Reset the environment\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "\n",
        "        if env_state is None:\n",
        "            obs, env_state = self.env.reset(rng_reset, self.env_params)\n",
        "        else:\n",
        "            obs = self.env.get_obs(env_state)\n",
        "\n",
        "        def policy_step(state_input, _):\n",
        "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "            obs, state,  rng, cum_reward, valid_mask = state_input\n",
        "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "            if model is not None:\n",
        "                action,info = model(obs, rng_net)\n",
        "            else:\n",
        "                action = self.env.action_space(self.env_params).sample(rng_net)\n",
        "                info = {}\n",
        "        #    print (\"policy step action\",action.shape)\n",
        "            next_obs, next_state, reward, done, step_info = self.env.step(\n",
        "                rng_step, state, action, self.env_params\n",
        "            )\n",
        "            info.update(step_info)\n",
        "            new_cum_reward = cum_reward + reward * valid_mask\n",
        "            new_valid_mask = valid_mask * (1 - done)\n",
        "            carry = [\n",
        "                next_obs,\n",
        "                next_state,\n",
        "                rng,\n",
        "                new_cum_reward,\n",
        "                new_valid_mask,\n",
        "            ]\n",
        "            y = [obs, action, reward, next_obs, done, state, info]\n",
        "            return carry, y\n",
        "\n",
        "        # Scan over episode step loop\n",
        "        carry_out, scan_out = jax.lax.scan(\n",
        "            policy_step,\n",
        "            [\n",
        "                obs,\n",
        "                env_state,\n",
        "                rng_episode,\n",
        "                jnp.array([0.0]),\n",
        "                jnp.array([1.0]),\n",
        "            ],\n",
        "            (),\n",
        "            num_steps,\n",
        "        )\n",
        "        # Return the sum of rewards accumulated by agent in episode rollout\n",
        "        obs, action, reward, next_obs, done, state, info = scan_out\n",
        "        cum_return = carry_out[-2]\n",
        "        info[\"last_state\"] = carry_out[1]\n",
        "        return obs, action, reward, next_obs, done,state, info, cum_return\n",
        "\n",
        "class UnsupervisedRolloutWrapper(CustomRolloutWrapper):\n",
        "\n",
        " #   @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_update(self, rng_update,model, obs, action,next_obs,done,info):\n",
        "        if model is None: return {}\n",
        "        return model.update(rng_update,obs, action,next_obs,done,info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ekign_g124jb"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as nn\n",
        "from flax import nnx\n",
        "\n",
        "jnp.set_printoptions(precision=3,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "\n",
        "def batch_random_split(batch_key,num=2):\n",
        "    split_keys = jax.vmap(jax.random.split,in_axes=(0,None))(batch_key,num)\n",
        "    return [split_keys[:, i]  for i in range(num) ]\n",
        "@jax.jit\n",
        "def compute_info_gain_normal(mean,prec,l_prec, next_obs):\n",
        "    \"\"\"\n",
        "    mean: (batch, obs_dim)\n",
        "    prec: (batch, obs_dim)  N(u;mean(s,a),(prec(s,a))^-0.5) N(next_obs;u,(l_prec(s,a))^-0.5)\n",
        "    l_prec: (batch, obs_dim)    likelihood_precision\n",
        "    next_obs: (batch, obs_dim)\n",
        "\n",
        "    output: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "    prec = jnp.maximum(prec, 1e-6)\n",
        "    posteior_prec = prec + l_prec\n",
        "    prec_ratio = prec / posteior_prec\n",
        "\n",
        "    posterior_mean = (prec * mean + next_obs * l_prec) /posteior_prec\n",
        "\n",
        "    delta_mean =  next_obs - posterior_mean\n",
        "\n",
        "    kl  = delta_mean * delta_mean * prec   #* ( l_prec / posteior_prec ) ** 2\n",
        "    kl = kl + prec_ratio - jnp.log(prec_ratio) - 1\n",
        "    kl = 0.5 * jnp.sum(kl,axis=-1)\n",
        "    return kl, delta_mean\n",
        "\n",
        "@jax.jit\n",
        "def compute_expected_info_gain_normal(prec,l_prec):\n",
        "    \"\"\"\n",
        "    prec: (batch, obs_dim)\n",
        "    l_prec: (batch, obs_dim)    likelihood_precision\n",
        "\n",
        "    output: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    prec = jnp.maximum(prec, 1e-6)\n",
        "    prec_ratio = l_prec / prec\n",
        "    mi_matrix = 0.5 * jnp.sum( jnp.log(1+prec_ratio),axis=-1)\n",
        "    return mi_matrix\n",
        "\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "    def __init__(self, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.linear1 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm1 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm2 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm3 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array,rng):\n",
        "\n",
        "\n",
        "        dist_distrax =  distrax.MultivariateNormalDiag(x,1e-1*jnp.ones_like(x))\n",
        "        x = dist_distrax.sample(seed=rng, sample_shape=())\n",
        "        x = self.layer_norm0(x)\n",
        "        h0 = self.linear1(x)\n",
        "        h = nn.relu(h0)\n",
        "        h = self.layer_norm1(h) +h0\n",
        "        h0 = self.linear2(h)\n",
        "        h = self.layer_norm2(h)+h0\n",
        "        return  self.layer_norm3(h)\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.linear = nnx.Linear(input_dim,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        h = self.linear(x)\n",
        "        return  self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "    def __init__(self, num_actions: int, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.embed = nnx.Embed(num_actions,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "    def __call__(self, x: jax.Array):\n",
        "        return  self.layer_norm0(self.embed(x))\n",
        "from jax import lax\n",
        "import distrax\n",
        "\n",
        "class Actor(nnx.Module):\n",
        "    log_std_min: float = -4\n",
        "    log_std_max: float = 2\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim,hidden_dim, rngs: nnx.Rngs):\n",
        "\n",
        "        self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "        self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "   #     self.linear1 = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "     #   print (\"x\",x.shape)\n",
        "        mean = self.mean(x)\n",
        "      #  print (\"mean\",mean.shape)\n",
        "        log_std = self.log_std(x)\n",
        "       # print (\"log_std\",log_std.shape)\n",
        "        log_std = jnp.clip(log_std, self.log_std_min, self.log_std_max)\n",
        "        return mean, log_std\n",
        "\n",
        "class Likelihood_Prec(nnx.Module):\n",
        "    log_std_min: float = -2\n",
        "    log_std_max: float = 2\n",
        "\n",
        "    def __init__(self, obs_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "\n",
        "        self.linear = nnx.Linear(hidden_dim, obs_dim, rngs=rngs)\n",
        "   #     self.linear1 = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        log_std = self.linear(x)\n",
        "        log_std = jnp.clip(log_std, self.log_std_min, self.log_std_max)\n",
        "        return jnp.exp(-log_std)\n",
        "\n",
        "\n",
        "def show_variable(model,text):\n",
        "\n",
        "    graphdef, params, vars,others = nnx.split(model, nnx.Param, nnx.Variable,...)\n",
        "\n",
        "    print(text,vars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "random & deepbayesian"
      ],
      "metadata": {
        "id": "c6VB-RUzrKam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, num_actions):\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def update(self,rng,obs,action,next_obs,done,info):\n",
        "      #update variable parameters\n",
        "        return {} #MI = E KL\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "        if observations.ndim == 1:\n",
        "            actions = jax.random.randint(rng, (), 0, self.num_actions)\n",
        "            return actions, {}\n",
        "        actions = jax.random.randint(rng, (observations.shape[0],), 0, self.num_actions)\n",
        "        return actions, {}\n",
        "\n",
        "class DeepBayesianExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, obs_dim, num_actions,hidden_dim, rngs: nnx.Rngs\n",
        "                 ,l_prec=1.0,weight_decay=1e-2,ent_lambda=1e-3,depth=2):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.num_actions = num_actions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)))\n",
        "        self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)))\n",
        "\n",
        "        self.trainable_likelihood_prec = Likelihood_Prec(obs_dim,hidden_dim,rngs)\n",
        "\n",
        "       # self.trainable_actor = Actor(obs_dim, action_dim,hidden_dim, rngs=rngs)\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.obs_embeds = Encoder(obs_dim,hidden_dim,rngs)\n",
        "        self.action_embeds = ActionEncoder(num_actions,hidden_dim,rngs)\n",
        "        self.joint_embeds =JointEncoder(hidden_dim,rngs)\n",
        "        self.depth = depth\n",
        "        self.ent_lambda = ent_lambda\n",
        "\n",
        "    def update(self,rng,obs,action,next_obs,done,info):\n",
        "     #   next_obs = next_obs[\"observation\"]\n",
        "        mean = info[\"mean\"]\n",
        "        prec = info[\"prec\"]\n",
        "     #   l_prec = jnp.clip( 1 / jnp.pow(mean - next_obs,2), max=10)\n",
        "\n",
        "\n",
        "        def _likelihood_loss(rng, T,mean, prec,next_obs):\n",
        "\n",
        "\n",
        "            # . x embed_size\n",
        "            l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "            mu = mean\n",
        "            sigma = jnp.sqrt( 1 / l_prec + 1 / prec)\n",
        "\n",
        "        #    print (\"mean\",mean.shape)\n",
        "         #   print (\"next_obs\",next_obs.shape)\n",
        "            dist_distrax =  distrax.MultivariateNormalDiag(mu,sigma)\n",
        "\n",
        "            dist_distrax.log_prob(next_obs)\n",
        "\n",
        "            return - dist_distrax.log_prob(next_obs), l_prec #.sum(-1)\n",
        "        predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"],mean, prec,next_obs)\n",
        "\n",
        "        mean_error = mean - next_obs\n",
        "        mean_error = mean_error * mean_error\n",
        "        mean_error = jnp.sum(mean_error,axis=-1)\n",
        "        deepkl, delta_mean = compute_info_gain_normal(mean,prec,l_prec, next_obs)\n",
        "        #batch x  num_hidden\n",
        "        T = info[\"T\"].reshape(-1,self.hidden_dim)\n",
        "\n",
        "        #batch x  obs_dim\n",
        "        l_prec = l_prec.reshape(-1,self.obs_dim)\n",
        "        delta_mean = delta_mean.reshape(-1,self.obs_dim)\n",
        "\n",
        "        # jax.debug.print(\"{}\", T_theta)\n",
        "        T_T = jnp.transpose(T)\n",
        "\n",
        "        covariance = T @ T_T\n",
        "        inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "        T_Map =  T_T @ inv_covariance\n",
        "\n",
        "        delta_precW = T_Map @ l_prec\n",
        "        self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "\n",
        "        delta_meanW = T_Map @ delta_mean\n",
        "        self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "        return {\"kl\":deepkl,\"predictive_loss\":predictive_loss,\"mean_error\":mean_error}\n",
        "\n",
        "   # @nnx.jit\n",
        "    def loss(self,rng, obs,action,next_obs,done,info):\n",
        "      #  next_obs = next_obs[\"observation\"]\n",
        "        def _likelihood_loss(T,mean, prec,next_obs):\n",
        "\n",
        "\n",
        "            # . x embed_size\n",
        "            l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "            mu = mean\n",
        "            sigma = jnp.sqrt( 1 / l_prec + 1 / prec)\n",
        "\n",
        "         #   print (\"mean\",mean.shape)\n",
        "          #  print (\"next_obs\",next_obs.shape)\n",
        "            dist_distrax =  distrax.MultivariateNormalDiag(mu,sigma)\n",
        "\n",
        "        #    dist_distrax.log_prob(next_obs)\n",
        "\n",
        "            return -dist_distrax.log_prob(next_obs) #.sum(-1)\n",
        "\n",
        "    #    print (\"sac_loss\",sac_loss)\n",
        "        T, mean, prec = info[\"T\"],info[\"mean\"],info[\"prec\"]\n",
        "        likelihood_loss = _likelihood_loss(T,mean, prec,next_obs)\n",
        "     #   print (\"likelihood_loss\",likelihood_loss)\n",
        "        return  likelihood_loss\n",
        "    def batch_loss(self,rng, obs,action,next_obs,done,info):\n",
        "        vmapped = jax.vmap(self.loss)\n",
        "        return vmapped(rng, obs,action,next_obs,done,info)\n",
        "    def __call__(self,observations,rng):\n",
        "     #   print (\"observations\",observations.shape)\n",
        "        # obs_dim\n",
        "        return self.recursive_mi(observations,rng,self.depth)\n",
        "\n",
        "    def recursive_mi(self,observations,rng,depth):\n",
        "        # embed_size\n",
        "        obs_embed = self.obs_embeds(observations)#.squeeze() xland embedding/one-hot+cnn\n",
        "        # num_actions x act_dim\n",
        "        action_embed = self.action_embeds(jnp.arange(self.num_actions))#.value\n",
        "        #num_actions x embed_size   deeper NN with direct addition is equivalent to shallow NN with concate and linear layer\n",
        "        embed = action_embed+jnp.expand_dims(obs_embed,0)\n",
        "\n",
        "        # num_actions x embed_size\n",
        "        T = self.joint_embeds(embed,rng)\n",
        "  #     print (\"T\",T.shape)\n",
        "        prec = jnp.maximum(T @ self.prec_w ,1e-3)\n",
        "        # num_actions x obs_dim\n",
        "        mean = T @ self.mean_w\n",
        "        l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "        # num_actions\n",
        "        MI = compute_expected_info_gain_normal(prec,l_prec)\n",
        "\n",
        "        if depth > 0:\n",
        "            vmapped = jax.vmap(self.recursive_mi,in_axes=(0,None,None))\n",
        "            # num_actions x 1\n",
        "            actions, info = vmapped(mean,rng,depth-1)\n",
        "            MI =  MI + info[\"mi\"]\n",
        "\n",
        "        actions = jnp.argmax(MI, axis=0)\n",
        "        T = T[actions]\n",
        "        MI = MI[actions]\n",
        "        l_prec = l_prec[actions]\n",
        "        prec = prec[actions]\n",
        "        mean = mean[actions]\n",
        "        return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                            \"prec\":prec,\"mean\":mean}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99gNpdlLq4fo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ppo"
      ],
      "metadata": {
        "id": "UBVO8MlhtUbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "\n",
        "class ActorCritic(nnx.Module):\n",
        "    def __init__(self, obs_dim, num_actions, hidden_dim, depth, rngs: nnx.Rngs):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.num_actions = num_actions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 共享的特征提取层\n",
        "        self.feature_extractor = [\n",
        "            nnx.Linear(obs_dim, hidden_dim, rngs=rngs),\n",
        "            nnx.tanh\n",
        "        ]\n",
        "        for _ in range(depth-1):\n",
        "            self.feature_extractor.append(nnx.Linear(hidden_dim, hidden_dim, rngs=rngs))\n",
        "            self.feature_extractor.append(nnx.tanh)\n",
        "\n",
        "        # Actor头 (策略)\n",
        "        self.actor_head = nnx.Linear(hidden_dim, num_actions, rngs=rngs)\n",
        "\n",
        "        # Critic头 (值函数)\n",
        "        self.critic_head = nnx.Linear(hidden_dim, 1, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.feature_extractor:\n",
        "            x = layer(x)\n",
        "        logits = self.actor_head(x)\n",
        "        pi = distrax.Categorical(logits=logits)\n",
        "        value = self.critic_head(x)\n",
        "        return pi, jnp.squeeze(value, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "class PPOExplorer(UnsupervisedExplorer):\n",
        "  def __init__(self, obs_dim,\n",
        "                     num_actions,\n",
        "                     hidden_dim,\n",
        "                     rngs: nnx.Rngs,\n",
        "                     depth:int = 2,\n",
        "                     gamma: float = 0.99,\n",
        "                     gae_lambda: float = 0.95,\n",
        "                     clip_eps: float = 0.2,\n",
        "                     ent_coef: float = 0.01,\n",
        "                     vf_coef: float = 0.5,\n",
        "                     max_grad_norm: float = 0.5,\n",
        "                     num_steps: int = 128,\n",
        "                     num_envs: int = 4,\n",
        "                     lr: float = 2.5e-4):\n",
        "\n",
        "    self.obs_dim = obs_dim\n",
        "    self.num_actions = num_actions\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.depth = depth\n",
        "    self.gamma = gamma\n",
        "    self.gae_lambda = gae_lambda\n",
        "    self.clip_eps = clip_eps\n",
        "    self.ent_coef = ent_coef\n",
        "    self.vf_coef = vf_coef\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.num_steps = num_steps\n",
        "    self.num_envs = num_envs\n",
        "\n",
        "    self.trainable_network = ActorCritic(obs_dim, num_actions,hidden_dim, depth, rngs)\n",
        "    # self.optimizer = nnx.Optimizer(self.trainable_network, optax.adam(1e-3), wrt=nnx.Param)\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    pi, value = self.trainable_network(observations)\n",
        "\n",
        "    action = pi.sample(seed=rng)\n",
        "    log_prob = pi.log_prob(action)\n",
        "\n",
        "    return action, {\"log_prob\": log_prob, \"value\": value}\n",
        "\n",
        "  def update(self,rng,obs,actions,next_obs,dones,info):\n",
        "    return {}\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "\n",
        "    _, value = self.trainable_network(obs)\n",
        "    reward = info[\"reward\"]\n",
        "    transition = (obs, actions, next_obs, dones, reward, info)\n",
        "\n",
        "    #GAE\n",
        "    def _calculate_gae(transition, value):\n",
        "      def _get_advantages(gae_and_next_value, transition):\n",
        "        gae, next_value = gae_and_next_value\n",
        "        obs, actions, next_obs, dones, reward, info = transition\n",
        "        delta = reward + self.gamma * next_value * (1 - dones) - info[\"value\"]\n",
        "        gae = (delta + self.gamma * self.gae_lambda * (1 - dones) * gae)\n",
        "        return (gae, value), gae\n",
        "\n",
        "      _, advantages = jax.lax.scan(\n",
        "            _get_advantages,\n",
        "            (jnp.zeros_like(info[\"value\"]), info[\"value\"]),\n",
        "            transition,\n",
        "            reverse=True,\n",
        "            unroll=16,\n",
        "      )\n",
        "      return advantages, advantages + info[\"value\"]\n",
        "\n",
        "    gae, targets = _calculate_gae(transition, value)\n",
        "\n",
        "    #Rerun Network\n",
        "    pi, next_value = self.trainable_network(next_obs)\n",
        "    log_prob = pi.log_prob(actions)\n",
        "\n",
        "    # value loss\n",
        "    value_pred_clipped = value + (next_value - value).clip(-self.clip_eps, self.clip_eps)\n",
        "    value_losses = jnp.square(next_value - targets)\n",
        "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "    value_loss = (0.5 * jnp.maximum(value_losses, value_losses_clipped).mean())\n",
        "\n",
        "    # actor loss\n",
        "    ratio = jnp.exp(log_prob - info[\"log_prob\"])\n",
        "    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
        "    loss_actor1 = ratio * gae\n",
        "    loss_actor2 = (\n",
        "        jnp.clip(\n",
        "            ratio,\n",
        "            1.0 - self.clip_eps,\n",
        "            1.0 + self.clip_eps,\n",
        "        )\n",
        "        * gae\n",
        "    )\n",
        "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
        "    loss_actor = loss_actor.mean()\n",
        "    entropy = pi.entropy().mean()\n",
        "\n",
        "    total_loss = loss_actor + self.vf_coef * value_loss - self.ent_coef * entropy\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "7q1bAyD9tT0g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_Z_V2JSg_h"
      },
      "source": [
        "Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYwuxCahRDTY"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from jax.scipy.special import digamma, gammaln, kl_div\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import flax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any, Dict\n",
        "import distrax\n",
        "import gymnax\n",
        "import functools\n",
        "from gymnax.environments import spaces\n",
        "from gymnax.wrappers import FlattenObservationWrapper, LogWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optax\n",
        "from flax.nnx.helpers import TrainState\n",
        "\n",
        "class MyTrainState(TrainState):\n",
        "    vars: nnx.Variable\n",
        "    others: nnx.State\n",
        "\n",
        "    @property\n",
        "    def need_train(self):\n",
        "        return len(self.params) > 0\n",
        "\n",
        "is_trainable = lambda path, node: (\n",
        "    node.type == nnx.Param and \\\n",
        "    any('trainable' in p_elem for p_elem in path if isinstance(p_elem, str))\n",
        ")\n",
        "\n",
        "def train_state_from_model(model,tx=optax.adam(0.02)):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    print(trainable_params)\n",
        "\n",
        "    state = MyTrainState.create(\n",
        "      tx=tx,\n",
        "      graphdef=graphdef,\n",
        "      params=trainable_params,\n",
        "      vars=vars,\n",
        "      others=others,\n",
        "    )\n",
        "    return state\n",
        "\n",
        "def train_state_update_model(model,state):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    return state.replace(vars=vars,others=others)\n",
        "\n",
        "def model_from_train_state(state):\n",
        "    return nnx.merge(state.graphdef, state.params, state.vars,state.others)\n",
        "# prompt: draw heatmap given sequence of states for MountainCar\n",
        "#state.position, state.velocity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reshape(arr):\n",
        "    if arr.ndim < 3:\n",
        "        raise ValueError(\"Input array must have at least 3 dimensions (n, b, c, ...).\")\n",
        "\n",
        "    # Get the original shape components\n",
        "    n, b, c, *x_dims = arr.shape\n",
        "\n",
        "    # Transpose the first two axes (n, b) to (b, n)\n",
        "    # We construct the axes tuple dynamically for flexibility\n",
        "    transpose_axes = (1, 0) + tuple(range(2, arr.ndim))\n",
        "    transposed_arr = jnp.transpose(arr, axes=transpose_axes)\n",
        "\n",
        "    # Reshape into (b, n*c, x0, x1, ...)\n",
        "    new_shape = (b, n * c, *x_dims)\n",
        "    reshaped_arr = jnp.reshape(transposed_arr, new_shape)\n",
        "\n",
        "    return reshaped_arr\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "def unpack_pytree_by_first_index(pytree: PyTree) -> List[PyTree]:\n",
        "    \"\"\"\n",
        "    Unpacks a PyTree of JAX arrays along their first dimension (id).\n",
        "\n",
        "    This function assumes that all JAX arrays within the PyTree\n",
        "    have a consistent first dimension (the 'id' dimension) and that\n",
        "    you want to create a separate PyTree for each 'id'.\n",
        "\n",
        "    Args:\n",
        "        pytree: A JAX PyTree where the leaves are JAX arrays\n",
        "                with a leading 'id' dimension.\n",
        "\n",
        "    Returns:\n",
        "        A list of PyTrees, where each PyTree corresponds to a single\n",
        "        'id' from the original PyTree.\n",
        "    \"\"\"\n",
        "    # Get the size of the first dimension from any leaf array\n",
        "    # We assume all arrays have the same first dimension size.\n",
        "    first_leaf = jax.tree_util.tree_leaves(pytree)[0]\n",
        "    num_ids = first_leaf.shape[0]\n",
        "\n",
        "    # Create a list to store the unpacked PyTrees\n",
        "    unpacked_pytrees = []\n",
        "\n",
        "    # Iterate through each ID\n",
        "    for i in range(num_ids):\n",
        "        # Use tree_map to slice each array in the PyTree at the current ID\n",
        "        sliced_pytree = jax.tree_util.tree_map(lambda x: x[i], pytree)\n",
        "        unpacked_pytrees.append(sliced_pytree)\n",
        "\n",
        "    return unpacked_pytrees\n",
        "def unpack_states(pytree):\n",
        "    return unpack_pytree_by_first_index(jax.tree.map(reshape, pytree))\n",
        "def draw_mountain_car_heatmap(state,config = {}):\n",
        "    \"\"\"\n",
        "    Draws a heatmap representing the trajectory of the MountainCar environment.\n",
        "\n",
        "    Args:\n",
        "        state_sequence: A sequence of JAX arrays representing the states\n",
        "                        of the MountainCar environment. Each state is expected\n",
        "                        to be a 2-element array [position, velocity].\n",
        "                        ['CartPole-v1',\"MountainCar-v0\",\"Acrobot-v1\"]\n",
        "    \"\"\"\n",
        "    title = config[\"ENV_NAME\"] +' MountainCar Heatmap ' +config[\"MODEL_NAME\"]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if config[\"ENV_NAME\"] == \"MountainCar-v0\":\n",
        "\n",
        "        positions = state.position\n",
        "        velocities = state.velocity\n",
        "\n",
        "        plt.scatter(positions, velocities, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Position')\n",
        "        plt.ylabel('Velocity')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"CartPole-v1\":\n",
        "        x = state.x\n",
        "        theta = state.theta\n",
        "        plt.scatter(x, theta, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('theta')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"Acrobot-v1\":\n",
        "        joint_angle1 = state.joint_angle1\n",
        "        joint_angle2 = state.joint_angle2\n",
        "        plt.scatter(joint_angle1, joint_angle2, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Angle1')\n",
        "        plt.ylabel('Angle2')\n",
        "        plt.grid(True)\n",
        "    if \"TOTAL_TIMESTEPS\" in config:\n",
        "        title += \"_TOTAL_TIMESTEPS_\"+str(config[\"TOTAL_TIMESTEPS\"])\n",
        "    if \"DEPTH\" in config:\n",
        "        title += \"_DEPTH_\"+str(config[\"DEPTH\"])\n",
        "    if \"NUM_HIDDEN\" in config:\n",
        "        title += \"_NUM_HIDDEN_\"+str(config[\"NUM_HIDDEN\"])\n",
        "    plt.title(title)\n",
        "    plt.savefig(title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return plt\n",
        "\n",
        "\n",
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_train(config):\n",
        "\n",
        "    config[\"NUM_UPDATES\"] = (config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"]// config[\"NUM_ENVS\"])\n",
        "\n",
        "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "    rng_batch = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "    manager = UnsupervisedRolloutWrapper(config[\"ENV_NAME\"])\n",
        "    num_actions = manager.env.num_actions()\n",
        "    obs_dim = manager.env.observation_space(manager.env_params).shape[0]\n",
        "\n",
        "\n",
        "    low = manager.env.observation_space(manager.env_params).low\n",
        "    high = manager.env.observation_space(manager.env_params).high\n",
        "\n",
        "    print (\"low\",low)\n",
        "    print (\"high\",high )\n",
        "    if config[\"MODEL_NAME\"] == \"DeepBayesianExplorer\":\n",
        "        model = DeepBayesianExplorer(obs_dim, num_actions,config[\"NUM_HIDDEN\"],\n",
        "                                    nnx.Rngs(config[\"SEED\"]),weight_decay=config[\"WD\"],depth=config[\"DEPTH\"])\n",
        "\n",
        "    if config[\"MODEL_NAME\"] == \"PPOExplorer\":\n",
        "        model = PPOExplorer(obs_dim,num_actions,config[\"NUM_HIDDEN\"],nnx.Rngs(config[\"SEED\"]),)\n",
        "\n",
        "    if config[\"MODEL_NAME\"] == \"RandomExplorer\":\n",
        "        model = RandomExplorer(num_actions)\n",
        "\n",
        "    @nnx.jit\n",
        "    def _train_step(state:MyTrainState, rng_loss, obs, action,next_obs,done,info):\n",
        "\n",
        "      def loss_fn(graphdef,params,vars,others):\n",
        "        model = nnx.merge(graphdef, params, vars,others)\n",
        "        return model.batch_loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "\n",
        "      def opt_step(state,unused):\n",
        "        grads = jax.grad(loss_fn,1)(state.graphdef, state.params, state.vars,state.others)\n",
        "        return state.apply_gradients(grads=grads),None\n",
        "      state, _ = jax.lax.scan(opt_step, state, None, config[\"OPT_STEPS\"])\n",
        "\n",
        "      return state\n",
        "    @nnx.jit\n",
        "    def _rollout_and_update_step(runner_state, unused):\n",
        "        # we have to use train_state for jax.lax.scan\n",
        "        train_state,  rng_batch,last_state = runner_state\n",
        "\n",
        "        model = model_from_train_state(train_state)\n",
        "        rng_batch, rng_step,rng_update,rng_loss = batch_random_split(rng_batch,4)\n",
        "\n",
        "        rollout_results = manager.batch_rollout( rng_batch,model,env_state=last_state,num_steps =  config[\"NUM_STEPS\"])\n",
        "        obs, action, reward, next_obs, done,state,info, cum_ret = rollout_results\n",
        "\n",
        "        # obs: num_envs x\n",
        "        transition = Transition(obs, action, reward, next_obs, done,info)\n",
        "\n",
        "        last_state = info[\"last_state\"]\n",
        "        info[\"reward\"] = reward\n",
        "        update_info = manager.batch_update(rng_update, model,obs, action,next_obs,done,info)\n",
        "        info.update(update_info)\n",
        "        train_state = train_state_update_model(model,train_state)\n",
        "\n",
        "        if train_state.need_train:\n",
        "            train_state = _train_step(train_state, rng_loss, obs, action,next_obs,done,info)\n",
        "\n",
        "        #works for tensors\n",
        "        runner_state = (train_state, rng_batch,last_state)\n",
        "        return runner_state, (transition, state)\n",
        "\n",
        "    def train(rng_batch,model,manager):\n",
        "        # training loop\n",
        "\n",
        "        rng_batch,  rng_reset = batch_random_split(rng_batch, 2)\n",
        "        start_state = manager.batch_reset(rng_reset)\n",
        "\n",
        "        if config[\"TX\"] == \"adamw\":\n",
        "            tx = optax.adamw(config[\"LR\"])\n",
        "        elif config[\"TX\"] == \"sgd\":\n",
        "            tx = optax.sgd(config[\"LR\"])\n",
        "        else:\n",
        "            tx = None\n",
        "            assert False, config[\"TX\"] + \" is not avaliable\"\n",
        "        train_state = train_state_from_model(model,tx)\n",
        "      #  rng, _rng = jax.random.split(rng)\n",
        "        runner_state = (train_state,  rng_batch,start_state)\n",
        "        runner_state, output= jax.lax.scan(_rollout_and_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
        "\n",
        "        transitions,states = output\n",
        "        return {\"runner_state\": runner_state, \"transitions\": transitions,\"states\":states}\n",
        "        # return {\"runner_state\": runner_state, \"collect_data\": collect_data, \"max_mi_history\": max_mi_history}\n",
        "\n",
        "    return train,model, manager,rng_batch\n",
        "\n",
        "def experiment(config):\n",
        "    print(config)\n",
        "    train_fn,model, manager,rng_batch = make_train(config)\n",
        "    train_jit = nnx.jit(train_fn)\n",
        "\n",
        " #   show_variable(model,\"explorer before\")\n",
        "    out = jax.block_until_ready(train_fn(rng_batch,model,manager))\n",
        "    #data shape: rollout groups = [TOTAL_TIMESTEPS//NUM_ENVS //NUM_STEPS] x NUM_ENVS x NUM_STEPS\n",
        "    print(\"data shape:\", jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))\n",
        "\n",
        "    train_state,  rng_batch, last_state = out[\"runner_state\"]\n",
        "\n",
        "    model = model_from_train_state(train_state)\n",
        "    #print (\"model\",model)\n",
        "\n",
        "\n",
        "\n",
        "    # if \"mi\" in out[\"transitions\"].info:\n",
        "    # # Create figure and axis\n",
        "    #     plt.figure(figsize=(10, 6))\n",
        "    #     # Sample JAX NumPy arrays (replace these with your actual arrays)\n",
        "    #     #  print (out[\"transitions\"].info)\n",
        "    #     eig_array = out[\"transitions\"].info[\"mi\"].reshape(-1)\n",
        "    #     big_array = out[\"transitions\"].info[\"kl\"].reshape(-1)\n",
        "    #     # Plot both arrays\n",
        "    #     plt.plot(eig_array, label='EIG', marker='o', linestyle='-', color='blue')\n",
        "    #     plt.plot(big_array, label='BIG', marker='s', linestyle='-', color='red')\n",
        "\n",
        "    #     if \"smi\" in out[\"transitions\"].info:\n",
        "    #         smi_array = out[\"transitions\"].info[\"smi\"].reshape(-1)\n",
        "    #         plt.plot(smi_array, label='SMI', marker='^', linestyle='-', color='green')\n",
        "\n",
        "    #     # Add labels and title\n",
        "    #     plt.xlabel('Num of Updates')\n",
        "    #     plt.ylabel('Information Gain')\n",
        "    #     Title = \"InfoGains for \"+  config[\"MODEL_NAME\"]\n",
        "    #     Title = Title + \"Total InfoGains\" +\"{:10.4f}\".format(big_array.sum().item())\n",
        "    #     Title = Title +  \" with Seed\" +str(config[\"SEED\"])\n",
        "    #     plt.title(Title)\n",
        "\n",
        "    #     # Add grid and legend\n",
        "    #     plt.grid(alpha=0.3)\n",
        "    #     plt.legend()\n",
        "    #     # Show the plot\n",
        "    #   # plt.ylim(0, 40)\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    #     plt.show()\n",
        "    # if \"l_prec\" in  out[\"transitions\"].info:\n",
        "    #     l_prec_mean = out[\"transitions\"].info[\"l_prec\"].mean(axis=(1,2,3),keepdims=False)\n",
        "    #  #   prec_mean = out[\"transitions\"].info[\"prec\"].mean(axis=(1,2,3),keepdims=False)\n",
        "    #     mean_error = out[\"transitions\"].info[\"mean_error\"].mean(axis=(1,2),keepdims=False)\n",
        "\n",
        "    #     # Create figure and axis\n",
        "    #     plt.figure(figsize=(10, 6))\n",
        "\n",
        "    #     # Plot both arrays\n",
        "    #     plt.plot(l_prec_mean, label='l_prec', marker='o', linestyle='-', color='blue')\n",
        "    # #    plt.plot(prec_mean, label='prec', marker='s', linestyle='-', color='red')\n",
        "    #     plt.plot(mean_error, label='mean_error', marker='p', linestyle='-', color='yellow')\n",
        "\n",
        "    #     # Add labels and title\n",
        "    #     plt.xlabel('Num of Updates')\n",
        "    #     plt.ylabel('Mean Precision')\n",
        "    #     Title = \"Comparison of Mean Precisions\"\n",
        "\n",
        "    #     plt.title(Title)\n",
        "\n",
        "    #     # Add grid and legend\n",
        "    #     plt.grid(alpha=0.3)\n",
        "    #     plt.legend()\n",
        "    #     # Show the plot\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    #     plt.show()\n",
        "\n",
        "    # draw_mountain_car_heatmap( unpack_states(out[\"states\"])[0],config)\n",
        "    return out\n",
        "'''\n",
        "\n",
        "result = {}\n",
        "pdfs = []\n",
        "#for i in [8,16,32,64,128,256]:\n",
        "  #  result[i] = {}\n",
        "for MODEL_NAME in [\"BayesianConjugate-v1\",\"DeepBayesianConjugate-v1\",\"DynamicSACBayesianExplorer-v1\",\n",
        "                   \"DeepSACBayesianConjugate-v1\",\"DeepRandomBayesianConjugate-v1\"]:\n",
        "    config[\"MODEL_NAME\"] = MODEL_NAME\n",
        "    result[MODEL_NAME] =[]\n",
        "    for seed in range(5):\n",
        "        config[\"SEED\"] = 423+seed\n",
        "        out , big ,pdf = experiment(config)\n",
        "        result[MODEL_NAME].append(big)\n",
        "        pdfs.append(pdf)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxkw8Bc1jMzG"
      },
      "source": [
        "# IQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kigxkqodYN"
      },
      "source": [
        "### network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qs-bMLHZoXEK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n",
        "\n",
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray, num_actions) -> jnp.ndarray:\n",
        "\n",
        "        # obs_dim = observations.shape[-1]\n",
        "        # if obs_dim == 4:\n",
        "        #     # CartPole: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
        "        #     flat_observations = observations\n",
        "        # elif obs_dim == 2:\n",
        "        #     # MountainCar: [position, velocity]\n",
        "        #     flat_observations = observations\n",
        "        # elif obs_dim == 6:\n",
        "        #     # Acrobot: [cos_angle1, sin_angle1, cos_angle2, sin_angle2, vel1, vel2]\n",
        "        #     flat_observations = observations\n",
        "        # else:\n",
        "        #\n",
        "        #     flat_observations = observations.reshape(batch_size, -1)\n",
        "\n",
        "        if actions.shape[-1] == 1:\n",
        "            actions_squeezed = jnp.squeeze(actions, axis=-1)\n",
        "        else:\n",
        "            actions_squeezed = actions\n",
        "\n",
        "        actions = jax.nn.one_hot(actions_squeezed, num_classes=num_actions) #one-hot encoding\n",
        "\n",
        "        inputs = jnp.concatenate([observations, actions], axis=-1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(observations)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution\n",
        "\n",
        "class CatPolicy(nn.Module):\n",
        "  hidden_dims : Sequence[int]\n",
        "  action_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, observations: jnp.ndarray, temperature: float = 1.0) -> distrax.Distribution:\n",
        "    x = observations\n",
        "    outputs = MLP(self.hidden_dims, activate_final=True)(x)\n",
        "    logits = nn.Dense(self.action_dim, kernel_init=default_init())(outputs)\n",
        "    distribution = distrax.Categorical(logits=logits)\n",
        "    return distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhG35bqUojAd"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jgyF_QGlpQSm"
      },
      "outputs": [],
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray\n",
        "\n",
        "def reshape_transitions(transitions):\n",
        "\n",
        "    num_updates, num_envs, num_steps = transitions.obs.shape[:3]\n",
        "    total_transitions = num_updates * num_envs * num_steps\n",
        "\n",
        "    # (num_updates, num_envs, num_steps) -> (num_envs, num_updates, num_steps)\n",
        "    obs_reshaped = transitions.obs.transpose(1, 0, 2, 3)\n",
        "    obs_reshaped = obs_reshaped.reshape(num_envs, num_updates * num_steps, -1)\n",
        "    obs_reshaped = obs_reshaped.reshape(total_transitions, -1)\n",
        "\n",
        "    observations = obs_reshaped\n",
        "    actions = transitions.action.transpose(1, 0, 2).reshape(total_transitions, -1)\n",
        "    rewards = transitions.reward.transpose(1, 0, 2).reshape(total_transitions)\n",
        "    next_observations = transitions.next_obs.transpose(1, 0, 2, 3).reshape(total_transitions, -1)\n",
        "    dones = transitions.done.transpose(1, 0, 2).reshape(total_transitions)\n",
        "    dones_float = dones.astype(np.float32)\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(observations, dtype=jnp.float32),\n",
        "        actions=jnp.array(actions, dtype=jnp.float32),\n",
        "        rewards=jnp.array(rewards, dtype=jnp.float32),\n",
        "        next_observations=jnp.array(next_observations, dtype=jnp.float32),\n",
        "        dones=jnp.array(dones, dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "\n",
        "    # if 'mi' in transitions.info:\n",
        "    #     offline_data['mi'] = transitions.info['mi'].transpose(1, 0, 2).reshape(total_transitions)\n",
        "    # if 'kl' in transitions.info:\n",
        "    #     offline_data['kl'] = transitions.info['kl'].transpose(1, 0, 2).reshape(total_transitions)\n",
        "    # if 'predictive_loss' in transitions.info:\n",
        "    #     offline_data['predictive_loss'] = transitions.info['predictive_loss'].transpose(1, 0, 2).reshape(total_transitions)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFVO2uuH8OzD"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "yPAsYtRu8QaE"
      },
      "outputs": [],
      "source": [
        "class IQLConfig(BaseModel):\n",
        "  # GENERAL\n",
        "  algo: str = \"IQL\"\n",
        "  project: str = \"train-IQL\"\n",
        "  env_name: str = \"CartPole-v1\"\n",
        "  num_actions: int = 2  # 3 for mc, 3 for acrobot\n",
        "  seed: int = 42\n",
        "  eval_episodes: int = 5\n",
        "  log_interval: int = 100\n",
        "  eval_interval: int = 100000\n",
        "  batch_size: int = 256\n",
        "  max_steps: int = int(1e6)\n",
        "  n_jitted_updates: int = 8\n",
        "  # DATASET\n",
        "  data_size: int = int(1e6)\n",
        "  normalize_state: bool = False\n",
        "  normalize_reward: bool = True\n",
        "  # NETWORK\n",
        "  hidden_dims: Tuple[int, int] = (256, 256)\n",
        "  actor_lr: float = 3e-4\n",
        "  value_lr: float = 3e-4\n",
        "  critic_lr: float = 3e-4\n",
        "  layer_norm: bool = True\n",
        "  opt_decay_schedule: bool = True\n",
        "  # IQL SPECIFIC\n",
        "  expectile: float = (\n",
        "      0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "  )\n",
        "  beta: float = (\n",
        "      3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "  )\n",
        "  tau: float = 0.005\n",
        "  discount: float = 0.99\n",
        "\n",
        "  def __hash__(\n",
        "      self,\n",
        "  ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "      return hash(self.__repr__())\n",
        "\n",
        "class IQLTrainState(NamedTuple):\n",
        "  rng: jax.random.PRNGKey\n",
        "  critic: TrainState\n",
        "  target_critic: TrainState\n",
        "  value: TrainState\n",
        "  actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "  @classmethod\n",
        "  def update_critic(\n",
        "      self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "  ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "      next_v = train_state.value.apply_fn(\n",
        "          train_state.value.params, batch.next_observations\n",
        "      )\n",
        "      target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "      def critic_loss_fn(\n",
        "          critic_params: flax.core.FrozenDict[str, Any]\n",
        "      ) -> jnp.ndarray:\n",
        "          q1, q2 = train_state.critic.apply_fn(\n",
        "              critic_params, batch.observations, batch.actions, config.num_actions\n",
        "          )\n",
        "          critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "          return critic_loss\n",
        "\n",
        "      new_critic, critic_loss = update_by_loss_grad(\n",
        "          train_state.critic, critic_loss_fn\n",
        "      )\n",
        "      return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "  @classmethod\n",
        "  def update_value(\n",
        "      self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "  ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "      q1, q2 = train_state.target_critic.apply_fn(\n",
        "          train_state.target_critic.params, batch.observations, batch.actions, config.num_actions\n",
        "      )\n",
        "      q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "      def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "          v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "          value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "          return value_loss\n",
        "\n",
        "      new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "      return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "  @classmethod\n",
        "  def update_actor(\n",
        "      self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "  ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "      v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "      q1, q2 = train_state.critic.apply_fn(\n",
        "          train_state.target_critic.params, batch.observations, batch.actions, config.num_actions\n",
        "      )\n",
        "      q = jnp.minimum(q1, q2)\n",
        "      exp_a = jnp.exp((q - v) * config.beta)\n",
        "      exp_a = jnp.minimum(exp_a, 100.0)\n",
        "      def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "          dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "          log_probs = dist.log_prob(batch.actions.astype(jnp.int32))\n",
        "          actor_loss = -(exp_a * log_probs).mean()\n",
        "          return actor_loss\n",
        "\n",
        "      new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "      return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "  @classmethod\n",
        "  def update_n_times(\n",
        "      self,\n",
        "      train_state: IQLTrainState,\n",
        "      dataset: Transition,\n",
        "      rng: jax.random.PRNGKey,\n",
        "      config: IQLConfig,\n",
        "  ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "      for _ in range(config.n_jitted_updates):\n",
        "          rng, subkey = jax.random.split(rng)\n",
        "          batch_indices = jax.random.randint(\n",
        "              subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "          )\n",
        "          batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "          train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "          train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "          train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "          new_target_critic = target_update(\n",
        "              train_state.critic, train_state.target_critic, config.tau\n",
        "          )\n",
        "          train_state = train_state._replace(target_critic=new_target_critic)\n",
        "      return train_state, {\n",
        "          \"value_loss\": value_loss,\n",
        "          \"actor_loss\": actor_loss,\n",
        "          \"critic_loss\": critic_loss,\n",
        "      }\n",
        "\n",
        "  @classmethod\n",
        "  def get_action(\n",
        "      self,\n",
        "      train_state: IQLTrainState,\n",
        "      observations: np.ndarray,\n",
        "      seed: jax.random.PRNGKey,\n",
        "      temperature: float = 1.0,\n",
        "      max_action: float = 1.0,\n",
        "  ) -> jnp.ndarray:\n",
        "\n",
        "      # modified for discrete actions\n",
        "      dist = train_state.actor.apply_fn(\n",
        "          train_state.actor.params, observations, temperature=temperature\n",
        "      )\n",
        "      actions = jnp.argmax(dist.logits, axis=-1)\n",
        "      return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz95VsnGNkpp"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "aQLsVFl-NhgM"
      },
      "outputs": [],
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 4\n",
        "\n",
        "    # Gaussian Model\n",
        "    # actor_model = GaussianPolicy(\n",
        "    #     config.hidden_dims,\n",
        "    #     action_dim=action_dim,\n",
        "    #     log_std_min=-5.0,\n",
        "    # )\n",
        "\n",
        "    # Cat Model\n",
        "    actor_model = CatPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim = action_dim\n",
        "    )\n",
        "\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions, config.num_actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions, config.num_actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )\n",
        "\n",
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, rng\n",
        ") -> float:\n",
        "    episode_returns = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      obs, state = env.reset(_rng, env_params)\n",
        "      done = 0\n",
        "\n",
        "      while not done:\n",
        "          # potential case issue\n",
        "          action = policy_fn(observations=obs)\n",
        "\n",
        "          if isinstance(action, (jnp.ndarray, np.ndarray)) and action.shape == (1,):\n",
        "            action = int(action[0])\n",
        "\n",
        "          obs, state, reward, done, _ = env.step(_rng, state, action, env_params)\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns))), float(jnp.std(jnp.array(episode_returns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN0bkGgnWFFW"
      },
      "source": [
        "## Integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dk74R8YRFy3"
      },
      "outputs": [],
      "source": [
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}\n",
        "\n",
        "env_name = 'CartPole-v1'  # @param ['CartPole-v1',\"MountainCar-v0\",\"Acrobot-v1\"] {\"type\":\"raw\"}\n",
        "NUM_ENVS = 4 # @param [1,2,4,8,16,32] {\"type\":\"raw\"}\n",
        "TOTAL_TIMESTEPS = 2048 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 8 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"DeepBayesianExplorer\"  #@param [\"DeepBayesianExplorer\",\"RandomExplorer\",\"PPOExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}\n",
        "\n",
        "\n",
        "out = experiment(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this block is special for running multiple seeds"
      ],
      "metadata": {
        "id": "Omj38Lh4ElXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# out_list = []\n",
        "# seeds = [5]\n",
        "# for seed in seeds:\n",
        "#   config[\"SEED\"] = 423 + seed\n",
        "#   out = experiment(config)\n",
        "#   out_list.append(out)\n",
        "\n",
        "# special for mountaincar\n",
        "# for out in out_list:\n",
        "#   obs = out[\"transitions\"].obs\n",
        "#   obs = obs.sum(axis=(-1))\n",
        "#   print(obs.shape)\n",
        "#   flat_rewards = obs.flatten()\n",
        "#   one_indices = np.where(flat_rewards >= 0.5)\n",
        "#   print(one_indices)\n",
        "\n",
        "#   # if one_indices.size > 0:\n",
        "#   #     # The first occurrence is the smallest index\n",
        "#   #     first_one_index = one_indices[0]\n",
        "#   #     print(first_one_index)\n",
        "#   # else:\n",
        "#   #     print(\"no reward\")\n",
        "\n",
        "# for out in out_list:\n",
        "#   reward = out[\"transitions\"].reward\n",
        "#   flat_rewards = reward.flatten()\n",
        "#   one_indices = np.where(flat_rewards >= -1)\n",
        "\n",
        "#   if one_indices.size > 0:\n",
        "#       # The first occurrence is the smallest index\n",
        "#       first_one_index = one_indices[0]\n",
        "#       print(first_one_index)\n",
        "#   else:\n",
        "#       print(\"no reward\")"
      ],
      "metadata": {
        "id": "HtWBWIRoDFGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###IQL"
      ],
      "metadata": {
        "id": "Tp_I4Us9FtGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npMaQWtsOvte"
      },
      "outputs": [],
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"PT-IQL\"\n",
        "    env_name: str = \"Pointax\"\n",
        "    num_actions: int = 2  #3 for mc, 3 for ab\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 10000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e4)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e4)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "    # EXPLORATION\n",
        "    explore_config: dict = {}\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "# you can add more parameters here for same struture as long as they are in the iql config scope\n",
        "seed = 42  # @param [42, 123, 423] {\"type\": \"raw\"}\n",
        "expectile = 0.7  # @param [0.5, 0.7, 0.9] {\"type\": \"raw\"}\n",
        "\n",
        "# Create the config dictionary from the parameters\n",
        "iql_conf_dict = {\n",
        "    \"env_name\": env_name,\n",
        "    \"seed\": seed,\n",
        "    \"expectile\": expectile,\n",
        "    \"explore_config\": config\n",
        "}\n",
        "\n",
        "# Create the config object\n",
        "iql_config = IQLConfig(**iql_conf_dict)\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray\n",
        "\n",
        "buffer = reshape_transitions(out[\"transitions\"])\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, buffer))\n",
        "\n",
        "wandb.init(config=iql_config, project=iql_config.project)\n",
        "\n",
        "rng = jax.random.PRNGKey(iql_config.seed)\n",
        "rng, _rng = jax.random.split(rng)\n",
        "\n",
        "env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "env_params = env.default_params()\n",
        "\n",
        "# create train_state\n",
        "example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], buffer)\n",
        "train_state: IQLTrainState = create_iql_train_state(\n",
        "    _rng,\n",
        "    example_batch.observations,\n",
        "    example_batch.actions,\n",
        "    iql_config,\n",
        ")\n",
        "\n",
        "algo = IQL()\n",
        "update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "act_fn = jax.jit(algo.get_action)\n",
        "num_steps = iql_config.max_steps // iql_config.n_jitted_updates\n",
        "eval_interval = iql_config.eval_interval // iql_config.n_jitted_updates\n",
        "for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    train_state, update_info = update_fn(train_state, buffer, subkey, iql_config)\n",
        "\n",
        "    if i % iql_config.log_interval == 0:\n",
        "        train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "        wandb.log(train_metrics, step=i)\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        policy_fn = partial(\n",
        "            act_fn,\n",
        "            temperature=0.0,\n",
        "            seed=jax.random.PRNGKey(0),\n",
        "            train_state=train_state,\n",
        "        )\n",
        "        normalized_score, std = evaluate(\n",
        "            policy_fn,\n",
        "            env,\n",
        "            env_params,\n",
        "            rng = _rng,\n",
        "            num_episodes=iql_config.eval_episodes,\n",
        "        )\n",
        "        eval_metrics = {f\"{iql_config.env_name}/normalized_score\": normalized_score,\n",
        "                        f\"{iql_config.env_name}/std\": std}\n",
        "        wandb.log(eval_metrics, step=i)\n",
        "\n",
        "\n",
        "# final evaluation\n",
        "policy_fn = partial(\n",
        "    act_fn,\n",
        "    temperature=0.0,\n",
        "    seed=jax.random.PRNGKey(0),\n",
        "    train_state=train_state,\n",
        ")\n",
        "normalized_score, std = evaluate(\n",
        "    policy_fn,\n",
        "    env,\n",
        "    env_params,\n",
        "    rng = _rng,\n",
        "    num_episodes=iql_config.eval_episodes,\n",
        ")\n",
        "print(\"Final Evaluation\", normalized_score)\n",
        "wandb.log({f\"Test{iql_config.env_name}/normalized_score\": normalized_score,\n",
        "                        f\"Test{iql_config.env_name}/std\": std})\n",
        "wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLljUAZBHpv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Omj38Lh4ElXI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}