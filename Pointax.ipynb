{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_61BfNpTX1L"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kaMjP4WH151p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322123ed-51ba-4830-e3a4-1bfb51f29871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnax\n",
            "  Downloading gymnax-0.0.9-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.5.3)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.10.6)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from gymnax) (1.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gymnax) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from gymnax) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax->gymnax) (0.1.9)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->gymnax) (1.16.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gymnax) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->gymnax) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->gymnax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax->gymnax) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax->gymnax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax->gymnax) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax->gymnax) (3.20.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax->gymnax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->gymnax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (3.23.0)\n",
            "Downloading gymnax-0.0.9-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnax\n",
            "Successfully installed gymnax-0.0.9\n",
            "Collecting distrax\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.1.90)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.5.3)\n",
            "Requirement already satisfied: jaxlib>=0.1.67 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (4.14.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.1.55->distrax) (1.16.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (1.17.2)\n",
            "Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distrax\n",
            "Successfully installed distrax-0.1.5\n",
            "Collecting ogbench\n",
            "  Downloading ogbench-1.1.5-py3-none-any.whl.metadata (946 bytes)\n",
            "Collecting mujoco>=3.1.6 (from ogbench)\n",
            "  Downloading mujoco-3.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm_control>=1.0.20 (from ogbench)\n",
            "  Downloading dm_control-1.0.31-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (from ogbench) (1.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (1.4.0)\n",
            "Collecting dm-env (from dm_control>=1.0.20->ogbench)\n",
            "  Downloading dm_env-1.6-py3-none-any.whl.metadata (966 bytes)\n",
            "Requirement already satisfied: dm-tree!=0.1.2 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (0.1.9)\n",
            "Collecting glfw (from dm_control>=1.0.20->ogbench)\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting labmaze (from dm_control>=1.0.20->ogbench)\n",
            "  Downloading labmaze-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (278 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.19.4 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (5.29.5)\n",
            "Requirement already satisfied: pyopengl>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (3.1.9)\n",
            "Requirement already satisfied: pyparsing>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (2.32.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0 in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (75.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (1.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dm_control>=1.0.20->ogbench) (4.67.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=3.1.6->ogbench) (1.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]->ogbench) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]->ogbench) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]->ogbench) (0.0.4)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]->ogbench) (2.37.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]->ogbench) (25.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree!=0.1.2->dm_control>=1.0.20->ogbench) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree!=0.1.2->dm_control>=1.0.20->ogbench) (1.17.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]->ogbench) (11.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.1.6->ogbench) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.1.6->ogbench) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=3.1.6->ogbench) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dm_control>=1.0.20->ogbench) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dm_control>=1.0.20->ogbench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dm_control>=1.0.20->ogbench) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dm_control>=1.0.20->ogbench) (2025.8.3)\n",
            "Downloading ogbench-1.1.5-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_control-1.0.31-py3-none-any.whl (56.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-3.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading labmaze-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, labmaze, dm-env, mujoco, dm_control, ogbench\n",
            "Successfully installed dm-env-1.6 dm_control-1.0.31 glfw-2.9.0 labmaze-1.0.6 mujoco-3.3.4 ogbench-1.1.5\n",
            "Collecting git+https://github.com/riiswa/pointax.git\n",
            "  Cloning https://github.com/riiswa/pointax.git to /tmp/pip-req-build-xkcsig3_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/riiswa/pointax.git /tmp/pip-req-build-xkcsig3_\n",
            "  Resolved https://github.com/riiswa/pointax.git to commit 03c9448b397de8797a08400b32450bac4aceb126\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: jaxlib>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: gymnax>=0.0.6 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (0.0.9)\n",
            "Requirement already satisfied: chex>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (0.1.90)\n",
            "Requirement already satisfied: flax>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (0.10.6)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from pointax==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.0->pointax==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.0->pointax==0.1.0) (4.14.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.0->pointax==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.6.0->pointax==0.1.0) (0.1.9)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from gymnax>=0.0.6->pointax==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from gymnax>=0.0.6->pointax==0.1.0) (0.13.2)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.0->pointax==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.0->pointax==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.0->pointax==0.1.0) (1.16.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->pointax==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->pointax==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.6.0->pointax==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.6.0->pointax==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax>=0.0.6->pointax==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->gymnax>=0.0.6->pointax==0.1.0) (0.0.4)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (24.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (5.29.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (3.20.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn->gymnax>=0.0.6->pointax==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.6.0->pointax==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax>=0.0.6->pointax==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn->gymnax>=0.0.6->pointax==0.1.0) (2025.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.6.0->pointax==0.1.0) (3.23.0)\n",
            "Building wheels for collected packages: pointax\n",
            "  Building wheel for pointax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pointax: filename=pointax-0.1.0-py3-none-any.whl size=16198 sha256=8cff59383b16845d9d399e7f2b859dcdca106ff741a8a3261a92b7213ab7ca57\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ae_j_q7t/wheels/97/5e/c9/4e1d110271f38b95ad05dfbdc4f388efc7dc3d7259ca7ca7ef\n",
            "Successfully built pointax\n",
            "Installing collected packages: pointax\n",
            "Successfully installed pointax-0.1.0\n",
            "Collecting ml_collections\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml_collections) (6.0.2)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml_collections\n",
            "Successfully installed ml_collections-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnax\n",
        "!pip install distrax\n",
        "!pip install ogbench\n",
        "!pip install git+https://github.com/riiswa/pointax.git\n",
        "!pip install ml_collections"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper"
      ],
      "metadata": {
        "id": "MbXVIHPyIyu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnax.environments import environment, spaces\n",
        "from pointax.types import EnvState, EnvParams\n",
        "import jax\n",
        "\n",
        "class PMwrapper(environment.Environment[EnvState, EnvParams]):\n",
        "    def __init__(self, pm_env):\n",
        "        super().__init__()\n",
        "        self.pm_env = pm_env\n",
        "\n",
        "    def default_params(self):\n",
        "        # return EnvParams()\n",
        "        return self.pm_env.default_params\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        obs, state, reward, done, info = self.pm_env.step_env(key, state, action, params)\n",
        "        n_obs = obs[:4]\n",
        "        goal = obs[4:]\n",
        "        info[\"goal_position\"] = goal\n",
        "        return n_obs, state, reward, done, info\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        obs, state = self.pm_env.reset_env(key, params)\n",
        "        obs = obs[:4]\n",
        "        return obs, state\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        obs = self.pm_env.get_obs(state, params)\n",
        "        obs = obs[:4]\n",
        "        return obs\n",
        "\n",
        "    def name(self):\n",
        "        suffix = \"Dense\" if self.pm_env.reward_type_str == \"dense\" else \"\"\n",
        "        return f\"pointax/PointMaze_{self.pm_env.maze_id}{suffix}\"\n",
        "\n",
        "    def num_actions(self):\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params=None):\n",
        "        return self.pm_env.action_space(params)\n",
        "\n",
        "    def observation_space(self, params):\n",
        "        return spaces.Box(low=-jnp.inf, high=jnp.inf, shape=(4,), dtype=jnp.float32)\n"
      ],
      "metadata": {
        "id": "LKrZbSUWI06I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pointax\n",
        "env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "params = env.default_params()\n",
        "\n",
        "# Reset and step\n",
        "key = jax.random.PRNGKey(42)\n",
        "obs, state = env.reset_env(key, params)\n",
        "\n",
        "action = jax.numpy.array([0.5, 0.0])  # Move right\n",
        "obs, state, reward, done, info = env.step_env(key, state, action, params)\n",
        "\n",
        "print(obs, obs.shape)\n",
        "print(f\"Reward: {reward}, Success: {info['is_success']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNv0jJ4GkWh",
        "outputId": "02138020-a7ed-4e4a-9e5e-dfc9af0530aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0201304  0.90668106 0.5        0.        ] (4,)\n",
            "Reward: 0.0, Success: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6LAZ_lUbbC"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aHxBAU0DucTa"
      },
      "outputs": [],
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import pointax\n",
        "\n",
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def update(self,obs,actions,next_obs,dones,info):\n",
        "      #update variable parameters\n",
        "        return #{\"kl\":KL} MI = E KL\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __call__(self,observations,rng):\n",
        "\n",
        "        return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "from gymnax.environments.environment import Environment\n",
        "class CustomRolloutWrapper:\n",
        "    \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_or_name: Union[str,Environment] = \"Pendulum-v1\",\n",
        "        num_env_steps: Optional[int] = None,\n",
        "        env_kwargs: Any | None = None,\n",
        "        env_params: Any | None = None,\n",
        "    ):\n",
        "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "        # Define the RL environment & network forward function\n",
        "        if env_kwargs is None:\n",
        "            env_kwargs = {}\n",
        "        if env_params is None:\n",
        "            env_params = {}\n",
        "        if isinstance(env_or_name,Environment):\n",
        "            self.env = env_or_name\n",
        "            self.env_params = env_or_name.default_params\n",
        "        else:\n",
        "            # Umaze\n",
        "            self.env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "            self.env_params = self.env.default_params()\n",
        "        self.env_params = self.env_params.replace(**env_params)\n",
        "\n",
        "        if num_env_steps is None:\n",
        "            self.num_env_steps = self.env_params.max_steps_in_episode\n",
        "        else:\n",
        "            self.num_env_steps = num_env_steps\n",
        "\n",
        "#    @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_reset(self,rng_input):\n",
        "        batch_reset = jax.vmap(self.single_reset_state)\n",
        "        return batch_reset(rng_input)\n",
        "\n",
        " #   @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def single_reset_state(self,rng_input):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "        obs, state = self.env.reset(rng_reset, self.env_params)\n",
        "        return state\n",
        "\n",
        "   # @functools.partial(nnx.jit, static_argnums=(0,4))\n",
        "    def batch_rollout(self, rng_eval, model:UnsupervisedExplorer,\n",
        "                      env_state=None,num_steps=1):\n",
        "        \"\"\"Evaluate a generation of networks on RL/Supervised/etc. task.\"\"\"\n",
        "        # vmap over different MC fitness evaluations for single network\n",
        "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0, None,0,None))\n",
        "        return batch_rollout(rng_eval, model, env_state,num_steps)\n",
        "\n",
        "    # @functools.partial(nnx.jit, static_argnums=(0,4))\n",
        "    def single_rollout(self, rng_input, model:UnsupervisedExplorer,\n",
        "                       env_state=None,num_steps=1):\n",
        "        \"\"\"Rollout a pendulum episode with lax.scan.\"\"\"\n",
        "        # Reset the environment\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "\n",
        "        if env_state is None:\n",
        "            obs, env_state = self.env.reset(rng_reset, self.env_params)\n",
        "        else:\n",
        "            obs = self.env.get_obs(env_state)\n",
        "\n",
        "        def policy_step(state_input, _):\n",
        "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "            obs, state,  rng, cum_reward, valid_mask = state_input\n",
        "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "            if model is not None:\n",
        "                action,info = model(obs, rng_net)\n",
        "            else:\n",
        "                action = self.env.action_space(self.env_params).sample(rng_net)\n",
        "                info = {}\n",
        "        #    print (\"policy step action\",action.shape)\n",
        "            next_obs, next_state, reward, done, step_info = self.env.step(\n",
        "                rng_step, state, action, self.env_params\n",
        "            )\n",
        "            info.update(step_info)\n",
        "            new_cum_reward = cum_reward + reward * valid_mask\n",
        "            new_valid_mask = valid_mask * (1 - done)\n",
        "            carry = [\n",
        "                next_obs,\n",
        "                next_state,\n",
        "                rng,\n",
        "                new_cum_reward,\n",
        "                new_valid_mask,\n",
        "            ]\n",
        "            y = [obs, action, reward, next_obs, done, state, info]\n",
        "            return carry, y\n",
        "\n",
        "        # Scan over episode step loop\n",
        "        carry_out, scan_out = jax.lax.scan(\n",
        "            policy_step,\n",
        "            [\n",
        "                obs,\n",
        "                env_state,\n",
        "                rng_episode,\n",
        "                jnp.array([0.0]),\n",
        "                jnp.array([1.0]),\n",
        "            ],\n",
        "            (),\n",
        "            num_steps,\n",
        "        )\n",
        "        # Return the sum of rewards accumulated by agent in episode rollout\n",
        "        obs, action, reward, next_obs, done, state, info = scan_out\n",
        "        cum_return = carry_out[-2]\n",
        "        info[\"last_state\"] = carry_out[1]\n",
        "        return obs, action, reward, next_obs, done,state, info, cum_return\n",
        "\n",
        "class UnsupervisedRolloutWrapper(CustomRolloutWrapper):\n",
        "\n",
        " #   @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_update(self, rng_update,model, obs, action,next_obs,done,info):\n",
        "        if model is None: return {}\n",
        "        return model.update(rng_update,obs, action,next_obs,done,info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ekign_g124jb"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as nn\n",
        "from flax import nnx\n",
        "\n",
        "jnp.set_printoptions(precision=3,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "\n",
        "def batch_random_split(batch_key,num=2):\n",
        "    split_keys = jax.vmap(jax.random.split,in_axes=(0,None))(batch_key,num)\n",
        "    return [split_keys[:, i]  for i in range(num) ]\n",
        "@jax.jit\n",
        "def compute_info_gain_normal(mean,prec,l_prec, next_obs):\n",
        "    \"\"\"\n",
        "    mean: (batch, obs_dim)\n",
        "    prec: (batch, obs_dim)  N(u;mean(s,a),(prec(s,a))^-0.5) N(next_obs;u,(l_prec(s,a))^-0.5)\n",
        "    l_prec: (batch, obs_dim)    likelihood_precision\n",
        "    next_obs: (batch, obs_dim)\n",
        "\n",
        "    output: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "    prec = jnp.maximum(prec, 1e-6)\n",
        "    posteior_prec = prec + l_prec\n",
        "    prec_ratio = prec / posteior_prec\n",
        "\n",
        "    posterior_mean = (prec * mean + next_obs * l_prec) /posteior_prec\n",
        "\n",
        "    delta_mean =  next_obs - posterior_mean\n",
        "\n",
        "    kl  = delta_mean * delta_mean * prec   #* ( l_prec / posteior_prec ) ** 2\n",
        "    kl = kl + prec_ratio - jnp.log(prec_ratio) - 1\n",
        "    kl = 0.5 * jnp.sum(kl,axis=-1)\n",
        "    return kl, delta_mean\n",
        "\n",
        "@jax.jit\n",
        "def compute_expected_info_gain_normal(prec,l_prec):\n",
        "    \"\"\"\n",
        "    prec: (batch, obs_dim)\n",
        "    l_prec: (batch, obs_dim)    likelihood_precision\n",
        "\n",
        "    output: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    prec = jnp.maximum(prec, 1e-6)\n",
        "    prec_ratio = l_prec / prec\n",
        "    mi_matrix = 0.5 * jnp.sum( jnp.log(1+prec_ratio),axis=-1)\n",
        "    return mi_matrix\n",
        "\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "    def __init__(self, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.linear1 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm1 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm2 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm3 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array,rng):\n",
        "        dist_distrax =  distrax.MultivariateNormalDiag(x,1e-1*jnp.ones_like(x))\n",
        "        x = dist_distrax.sample(seed=rng, sample_shape=())\n",
        "        x = self.layer_norm0(x)\n",
        "        h0 = self.linear1(x)\n",
        "        h = nn.relu(h0)\n",
        "        h = self.layer_norm1(h) +h0\n",
        "        h0 = self.linear2(h)\n",
        "        h = self.layer_norm2(h)+h0\n",
        "        return  self.layer_norm3(h)\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.linear = nnx.Linear(input_dim,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        h = self.linear(x)\n",
        "        return  self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "    def __init__(self, num_actions: int, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.embed = nnx.Embed(num_actions,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "    def __call__(self, x: jax.Array):\n",
        "        return  self.layer_norm0(self.embed(x))\n",
        "\n",
        "from jax import lax\n",
        "import distrax\n",
        "\n",
        "\n",
        "class MLP(nnx.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, rngs):\n",
        "        self.linear1 = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(hidden_dim, output_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = jax.nn.relu(self.linear1(x))\n",
        "        return self.linear2(h)\n",
        "\n",
        "class Actor(nnx.Module):\n",
        "    log_std_min: float = -4\n",
        "    log_std_max: float = 2\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim,hidden_dim, rngs: nnx.Rngs):\n",
        "\n",
        "        self.linear = nnx.Linear(obs_dim, hidden_dim, rngs=rngs)\n",
        "\n",
        "        self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "        self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "   #     self.linear1 = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        x = self.linear(x)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x)\n",
        "        log_std = jnp.clip(log_std, self.log_std_min, self.log_std_max)\n",
        "        # probably need stablize here\n",
        "        pi = distrax.MultivariateNormalDiag(mean, jnp.exp(log_std))\n",
        "        return pi\n",
        "\n",
        "class Critic(nnx.Module):\n",
        "  def __init__(self, obs_dim, action_dim,hidden_dim, rngs: nnx.Rngs):\n",
        "    self.ln = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.act = nnx.relu(hidden_dim, 1, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.array):\n",
        "    x = self.ln(x)\n",
        "    x = self.act(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class TwinCritic(nnx.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "        self.trunk1 = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "        self.trunk2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "        self.q1 = nnx.Linear(hidden_dim, 1, rngs=rngs)\n",
        "        self.q2 = nnx.Linear(hidden_dim, 1, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        h = jax.nn.relu(self.trunk1(x))\n",
        "        h = jax.nn.relu(self.trunk2(h))\n",
        "        q1 = self.q1(h)\n",
        "        q2 = self.q2(h)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "class Likelihood_Prec(nnx.Module):\n",
        "    log_std_min: float = -2\n",
        "    log_std_max: float = 2\n",
        "\n",
        "    def __init__(self, obs_dim, hidden_dim, rngs: nnx.Rngs):\n",
        "\n",
        "        self.linear = nnx.Linear(hidden_dim, obs_dim, rngs=rngs)\n",
        "   #     self.linear1 = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        log_std = self.linear(x)\n",
        "        log_std = jnp.clip(log_std, self.log_std_min, self.log_std_max)\n",
        "        return jnp.exp(-log_std)\n",
        "\n",
        "\n",
        "def show_variable(model,text):\n",
        "\n",
        "    graphdef, params, vars,others = nnx.split(model, nnx.Param, nnx.Variable,...)\n",
        "\n",
        "    print(text,vars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ddpg"
      ],
      "metadata": {
        "id": "cZyI4STOV9Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPGExplorer(UnsupervisedExplorer):\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs) -> None:\n",
        "      self.trainable_actor = Actor(obs_dim, action_dim, hidden_dim, rngs)\n",
        "      self.trainable_critic = TwinCritic(obs_dim + action_dim, hidden_dim, rngs)\n",
        "      self.trainable_critic_target = TwinCritic(obs_dim + action_dim, hidden_dim, rngs)\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    pi = self.trainable_actor(observations)\n",
        "    # if eval_mode:\n",
        "    #   action = pi.mean()\n",
        "    # else:\n",
        "    #   action = pi.sample(seed=rng)\n",
        "    action = pi.sample(seed=rng)\n",
        "    return action\n",
        "\n",
        "  def update(self,rng,obs,actions,next_obs,dones,info):\n",
        "    return {}\n",
        "\n",
        "  def batch_critic_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    rng, rng_act = jax.random.split(rng)\n",
        "    pi = self.trainable_actor(obs)\n",
        "    next_actions = pi.sample(seed=rng_act)\n",
        "    # if clip_action:\n",
        "    #   nexy_actions = jnp.clip(next_actions, -1.0, 1.0)\n",
        "    reward = info[\"reward\"]\n",
        "    discount = info[\"discount\"]\n",
        "\n",
        "    tq1, tq2 = self.trainable_critic_target(jnp.concatenate([next_obs, next_actions], axis=-1))\n",
        "    target_v = jnp.minimum(tq1, tq2)\n",
        "    target_q = reward + discount * target_v\n",
        "\n",
        "    q1, q2 = self.trainable_critic(jnp.concatenate([obs, actions], axis=-1))\n",
        "    mse1 = (q1 - target_q) ** 2\n",
        "    mse2 = (q2 - target_q) ** 2\n",
        "    loss = jnp.mean(mse1 + mse2)\n",
        "\n",
        "    # metrics = {\n",
        "    #     \"critic_loss\": loss,\n",
        "    #     \"critic_q1\": jnp.mean(q1),\n",
        "    #     \"critic_q2\": jnp.mean(q2),\n",
        "    #     \"critic_target_q\": jnp.mean(target_q),\n",
        "    # }\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def batch_actor_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "    rng, rng_act = jax.random.split(rng)\n",
        "    pi = self.trainable_actor(obs)\n",
        "    actions = pi.sample(seed=rng_act)\n",
        "    q1, q2 = self.trainable_critic(jnp.concatenate([obs, actions], axis=-1))\n",
        "    q = jnp.minimum(q1, q2)\n",
        "    loss = -jnp.mean(q)\n",
        "\n",
        "    # # 可选指标\n",
        "    # log_prob = pi.log_prob(action)\n",
        "    # if log_prob.ndim == 2:\n",
        "    #     log_prob = jnp.sum(log_prob, axis=-1, keepdims=True)\n",
        "    # try:\n",
        "    #     ent = pi.entropy()\n",
        "    #     if ent.ndim == 2:\n",
        "    #         ent = jnp.sum(ent, axis=-1, keepdims=True)\n",
        "    #     ent_mean = jnp.mean(ent)\n",
        "    # except Exception:\n",
        "    #     ent_mean = jnp.nan\n",
        "\n",
        "    # metrics = {\n",
        "    #     \"actor_loss\": loss,\n",
        "    #     \"actor_logprob\": jnp.mean(log_prob),\n",
        "    #     \"actor_ent\": ent_mean,\n",
        "    # }\n",
        "    # return loss, metrics, rng\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "class Disagreement(nnx.Module):\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs, n_models=5):\n",
        "    self.n_models = n_models\n",
        "    self.obs_dim = obs_dim\n",
        "\n",
        "    self.ensemble = []\n",
        "    for i in range(n_models):\n",
        "      model_rngs = nnx.Rngs(i)\n",
        "      model = MLP(obs_dim + action_dim, hidden_dim, obs_dim, model_rngs)\n",
        "      self.ensemble.append(model)\n",
        "\n",
        "  def __call__(self, obs, actions, next_obs):\n",
        "    errors = []\n",
        "    inputs = jnp.concatenate([obs, actions], axis=-1)\n",
        "\n",
        "    for model in self.ensemble:\n",
        "      next_obs_hat = model(inputs)\n",
        "      model_err = jnp.linalg.norm(next_obs - next_obs_hat, axis=-1, keepdims=True)\n",
        "      errors.append(model_err)\n",
        "\n",
        "    return jnp.concatenate(errors, axis=1)\n",
        "\n",
        "  def get_disagreement(self, obs, actions, next_obs):\n",
        "    preds = []\n",
        "    inputs = jnp.concatenate([obs, actions], axis=-1)\n",
        "\n",
        "    for model in self.ensemble:\n",
        "      next_obs_hat = model(inputs)\n",
        "      preds.append(next_obs_hat)\n",
        "\n",
        "    preds = jnp.stack(preds, axis=0)\n",
        "    return jnp.var(preds, axis=0).mean(axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "class DisagreementExplorer(UnsupervisedExplorer):\n",
        "  def __init__(self, obs_dim, action_dim, hidden_dim, rngs):\n",
        "    self.trainable_actor = Actor(obs_dim, action_dim, hidden_dim, rngs)\n",
        "    self.trainable_critic = TwinCritic(obs_dim + action_dim, hidden_dim, rngs)\n",
        "    self.trainable_critic_target = TwinCritic(obs_dim + action_dim, hidden_dim, rngs)\n",
        "    self.trainable_disagreement = Disagreement(obs_dim, action_dim, hidden_dim, rngs)\n",
        "\n",
        "    self.obs_dim = obs_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "\n",
        "  def dissagreement_loss(self, obs, actions, next_obs):\n",
        "    error = self.trainable_disagreement(obs, actions, next_obs)\n",
        "    loss = jnp.mean(error)\n",
        "    return loss\n",
        "\n",
        "  def compute_intr_reward(self, obs, actions, next_obs):\n",
        "    reward = self.disagreement.get_disagreement(obs, actions, next_obs)\n",
        "    return jnp.expand_dims(reward, axis=-1)\n",
        "\n",
        "  def batch_critic_loss_with_intrinsic(self, rng, obs, actions, next_obs, dones, info):\n",
        "    reward = self.compute_intr_reward(obs, actions, next_obs)\n",
        "\n",
        "    discount = info[\"discount\"]\n",
        "    rng, rng_act = jax.random.split(rng)\n",
        "    pi = self.trainable_actor(obs)\n",
        "    next_actions = pi.sample(seed=rng_act)\n",
        "\n",
        "\n",
        "    tq1, tq2 = self.trainable_critic_target(jnp.concatenate([next_obs, next_actions], axis=-1))\n",
        "    target_v = jnp.minimum(tq1, tq2)\n",
        "    target_q = reward + discount * target_v\n",
        "\n",
        "    q1, q2 = self.trainable_critic(jnp.concatenate([obs, actions], axis=-1))\n",
        "    mse1 = (q1 - target_q) ** 2\n",
        "    mse2 = (q2 - target_q) ** 2\n",
        "    loss = jnp.mean(mse1 + mse2)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def batch_actor_loss_with_intrinsic(self, rng, obs, actions, next_obs, dones, info):\n",
        "    rng, rng_act = jax.random.split(rng)\n",
        "    pi = self.trainable_actor(obs)\n",
        "    next_actions = pi.sample(seed=rng_act)\n",
        "\n",
        "    q1, q2 = self.trainable_critic(jnp.concatenate([obs, actions], axis=-1))\n",
        "    q = jnp.minimum(q1, q2)\n",
        "    loss = -jnp.mean(q)\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "SkNoqTMnXqGt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random & deepbayesian"
      ],
      "metadata": {
        "id": "c6VB-RUzrKam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, action_dim):\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "    def update(self,rng,obs,action,next_obs,done,info):\n",
        "      #update variable parameters\n",
        "        return {} #MI = E KL\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "        if observations.ndim == 1:\n",
        "            actions = jax.random.uniform(rng, shape=(self.action_dim,), minval=-1.0, maxval=1.0)\n",
        "            return actions, {}\n",
        "        actions = jax.random.uniform(rng, shape=(observations.shape[0], self.action_dim), minval=-1.0, maxval=1.0)\n",
        "        return actions, {}\n",
        "\n",
        "class DBActor(nnx.Module):\n",
        "    log_std_min: float = -4\n",
        "    log_std_max: float = 2\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim,hidden_dim, rngs: nnx.Rngs):\n",
        "\n",
        "        self.mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "        self.log_std = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "   #     self.linear1 = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        print (\"x\",x.shape)\n",
        "        mean = self.mean(x)\n",
        "        print (\"mean\",mean.shape)\n",
        "        log_std = self.log_std(x)\n",
        "        print (\"log_std\",log_std.shape)\n",
        "        log_std = jnp.clip(log_std, self.log_std_min, self.log_std_max)\n",
        "        return mean, log_std\n",
        "\n",
        "class DBJointEncoder(nnx.Module):\n",
        "    def __init__(self, hidden_dims: int, rngs: nnx.Rngs):\n",
        "        self.linear1 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(hidden_dims,hidden_dims,rngs=rngs)\n",
        "        self.layer_norm0 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm1 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm2 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "        self.layer_norm3 = nnx.LayerNorm(hidden_dims,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jax.Array):\n",
        "        x = self.layer_norm0(x)\n",
        "        h = self.linear1(x)\n",
        "        h = nn.relu(h)\n",
        "        h = self.layer_norm1(h)\n",
        "        h = self.linear2(h)\n",
        "        h = self.layer_norm2(h)\n",
        "        return  self.layer_norm3(nn.relu(h + x))\n",
        "\n",
        "\n",
        "class DeepBayesianExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, obs_dim, action_dim,hidden_dim, rngs: nnx.Rngs\n",
        "                ,l_prec=1.0,weight_decay=1e-2,ent_lambda=1e-3,depth=2):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.prec_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)))\n",
        "        self.mean_w = nnx.Variable(jnp.zeros((hidden_dim, obs_dim)))\n",
        "\n",
        "        self.trainable_likelihood_prec = Likelihood_Prec(obs_dim,hidden_dim,rngs)\n",
        "\n",
        "        self.trainable_actor = DBActor(obs_dim, action_dim,hidden_dim, rngs=rngs)\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.obs_embeds = Encoder(obs_dim,hidden_dim,rngs)\n",
        "        self.action_embeds = Encoder(action_dim,hidden_dim,rngs)\n",
        "        self.joint_embeds =DBJointEncoder(hidden_dim,rngs)\n",
        "\n",
        "        self.ent_lambda = ent_lambda\n",
        "\n",
        "    def update(self,rng,obs,action,next_obs,done,info):\n",
        "    #   next_obs = next_obs[\"observation\"]\n",
        "        mean = info[\"mean\"]\n",
        "        prec = info[\"prec\"]\n",
        "        l_prec = jnp.clip( 1 / jnp.pow(mean - next_obs,2), max=10)\n",
        "\n",
        "        deepkl, delta_mean = compute_info_gain_normal(mean,prec,l_prec, next_obs)\n",
        "\n",
        "        def _likelihood_loss(rng, T,mean, prec,next_obs):\n",
        "\n",
        "\n",
        "            # . x embed_size\n",
        "            l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "            mu = mean\n",
        "            sigma = jnp.sqrt( 1 / l_prec + 1 / prec)\n",
        "\n",
        "            dist_distrax =  distrax.MultivariateNormalDiag(mu,sigma)\n",
        "\n",
        "            dist_distrax.log_prob(next_obs)\n",
        "\n",
        "            return - dist_distrax.log_prob(next_obs), l_prec\n",
        "\n",
        "        predictive_loss, l_prec = _likelihood_loss(rng, info[\"T\"],mean, prec,next_obs)\n",
        "\n",
        "        # mean_error = mean - next_obs\n",
        "        # mean_error = mean_error * mean_error\n",
        "        # mean_error = jnp.sum(mean_error,axis=-1)\n",
        "        deepkl, delta_mean = compute_info_gain_normal(mean,prec,l_prec, next_obs)\n",
        "        #batch x  num_hidden\n",
        "        T = info[\"T\"].reshape(-1,self.hidden_dim)\n",
        "\n",
        "        #batch x  obs_dim\n",
        "        l_prec = l_prec.reshape(-1,self.obs_dim)\n",
        "        delta_mean = delta_mean.reshape(-1,self.obs_dim)\n",
        "\n",
        "        # jax.debug.print(\"{}\", T_theta)\n",
        "        T_T = jnp.transpose(T)\n",
        "\n",
        "        covariance = T @ T_T\n",
        "        inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "        T_Map =  T_T @ inv_covariance\n",
        "\n",
        "        delta_precW = T_Map @ l_prec\n",
        "        self.prec_w.value = (self.prec_w.value + delta_precW) * (1-self.weight_decay)\n",
        "\n",
        "        delta_meanW = T_Map @ delta_mean\n",
        "        self.mean_w.value = (self.mean_w.value + delta_meanW) * (1-self.weight_decay)\n",
        "\n",
        "        return {\"kl\":deepkl,\"predictive_loss\":predictive_loss}\n",
        "\n",
        "  # @nnx.jit\n",
        "    def loss(self,rng, obs,action,next_obs,done,info):\n",
        "      #  next_obs = next_obs[\"observation\"]\n",
        "        def _likelihood_loss(rng,T,mean, prec,next_obs):\n",
        "\n",
        "\n",
        "            # . x embed_size\n",
        "            l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "            mu = mean\n",
        "            sigma = jnp.sqrt( 1 / l_prec + 1 / prec)\n",
        "\n",
        "            dist_distrax =  distrax.MultivariateNormalDiag(mu,sigma)\n",
        "\n",
        "            return -dist_distrax.log_prob(next_obs) #.sum(-1)\n",
        "\n",
        "        def _sac_loss(rng, obs_embed):\n",
        "            #  num_actions\n",
        "            mean, log_std = self.trainable_actor(obs_embed)\n",
        "\n",
        "            dist_distrax =  distrax.MultivariateNormalDiag(mean,jnp.exp(log_std))\n",
        "            actions = dist_distrax.sample(seed=rng, sample_shape=())\n",
        "\n",
        "            # . x embed_size\n",
        "            action_embed = self.action_embeds(actions)#.value\n",
        "            #. x embed_size\n",
        "            embed = action_embed+obs_embed\n",
        "\n",
        "            # . x embed_size\n",
        "            T = self.joint_embeds(embed)\n",
        "            prec = T @ self.prec_w\n",
        "            l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "            MI = compute_expected_info_gain_normal(prec,l_prec)\n",
        "\n",
        "            print (\"MI\",MI.shape)\n",
        "            print (\"dist_distrax.entropy()\",dist_distrax.entropy().shape)\n",
        "            return - MI - self.ent_lambda* dist_distrax.entropy()\n",
        "\n",
        "        rng_sac ,rng_likelihood = jax.random.split(rng, 2)\n",
        "        obs_embed = info[\"obs_embed\"]\n",
        "        sac_loss = _sac_loss(rng_sac,obs_embed)\n",
        "        print (\"sac_loss\",sac_loss)\n",
        "        T, mean, prec = info[\"T\"],info[\"mean\"],info[\"prec\"]\n",
        "        likelihood_loss = _likelihood_loss(rng_likelihood,T,mean, prec,next_obs)\n",
        "        print (\"likelihood_loss\",likelihood_loss)\n",
        "        return sac_loss + likelihood_loss\n",
        "\n",
        "    def batch_loss(self,rng, obs,action,next_obs,done,info):\n",
        "        vmapped = jax.vmap(self.loss)\n",
        "        return vmapped(rng, obs,action,next_obs,done,info)\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "\n",
        "        # obs_dim\n",
        "        obs_embed = self.obs_embeds(observations)#.squeeze()\n",
        "\n",
        "        print (\"obs_embed\",obs_embed.shape)\n",
        "        #  num_actions\n",
        "        mean, log_std = self.trainable_actor(obs_embed)\n",
        "\n",
        "        print (\"mean\",mean.shape)\n",
        "        print (\"log_std\",log_std.shape)\n",
        "        dist_distrax =  distrax.MultivariateNormalDiag(mean*0,jnp.exp(log_std))\n",
        "        actions = dist_distrax.sample(seed=rng, sample_shape=())\n",
        "\n",
        "        print (\"actions\",actions.shape)\n",
        "        # . x embed_size\n",
        "        action_embed = self.action_embeds(actions)#.value\n",
        "        print (\"action_embed\",action_embed.shape)\n",
        "        #. x embed_size\n",
        "        embed = action_embed+obs_embed\n",
        "\n",
        "        # . x embed_size\n",
        "        T = self.joint_embeds(embed)\n",
        "        print (\"T\",T.shape)\n",
        "        prec = jnp.maximum(T @ self.prec_w ,1e-3)\n",
        "        mean = T @ self.mean_w\n",
        "        l_prec = self.trainable_likelihood_prec(T)\n",
        "\n",
        "        MI = compute_expected_info_gain_normal(prec,l_prec)\n",
        "\n",
        "        return actions, {\"mi\":MI,\"T\":T,\"obs_embed\":obs_embed,\"l_prec\":l_prec,\n",
        "                         \"prec\":prec,\"mean\":mean}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99gNpdlLq4fo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ppo"
      ],
      "metadata": {
        "id": "UBVO8MlhtUbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "\n",
        "class ActorCritic(nnx.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden_dim, depth, rngs: nnx.Rngs):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 共享的特征提取层\n",
        "        self.feature_extractor = [\n",
        "            nnx.Linear(obs_dim, hidden_dim, rngs=rngs),\n",
        "            nnx.tanh\n",
        "        ]\n",
        "        for _ in range(depth-1):\n",
        "            self.feature_extractor.append(nnx.Linear(hidden_dim, hidden_dim, rngs=rngs))\n",
        "            self.feature_extractor.append(nnx.tanh)\n",
        "\n",
        "        # Actor头 (策略)\n",
        "        self.actor_mean = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "        self.actor_logstd = nnx.Linear(hidden_dim, action_dim, rngs=rngs)\n",
        "\n",
        "\n",
        "        # Critic头 (值函数)\n",
        "        self.critic_head = nnx.Linear(hidden_dim, 1, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.feature_extractor:\n",
        "            x = layer(x)\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd(x)\n",
        "        action_std = jnp.exp(jnp.clip(action_logstd, -20, 2))\n",
        "        value = self.critic_head(x)\n",
        "\n",
        "        pi = distrax.MultivariateNormalDiag(action_mean, action_std)\n",
        "\n",
        "        return pi, jnp.squeeze(value, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "class PPOExplorer(UnsupervisedExplorer):\n",
        "  def __init__(self, obs_dim,\n",
        "                     action_dim,\n",
        "                     hidden_dim,\n",
        "                     rngs: nnx.Rngs,\n",
        "                     depth:int = 2,\n",
        "                     gamma: float = 0.99,\n",
        "                     gae_lambda: float = 0.95,\n",
        "                     clip_eps: float = 0.2,\n",
        "                     ent_coef: float = 0.01,\n",
        "                     vf_coef: float = 0.5,\n",
        "                     max_grad_norm: float = 0.5,\n",
        "                     num_steps: int = 128,\n",
        "                     num_envs: int = 4,\n",
        "                     lr: float = 2.5e-4):\n",
        "\n",
        "    self.obs_dim = obs_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.depth = depth\n",
        "    self.gamma = gamma\n",
        "    self.gae_lambda = gae_lambda\n",
        "    self.clip_eps = clip_eps\n",
        "    self.ent_coef = ent_coef\n",
        "    self.vf_coef = vf_coef\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "    self.num_steps = num_steps\n",
        "    self.num_envs = num_envs\n",
        "\n",
        "    self.trainable_network = ActorCritic(obs_dim, action_dim,hidden_dim, depth, rngs)\n",
        "    # self.optimizer = nnx.Optimizer(self.trainable_network, optax.adam(1e-3), wrt=nnx.Param)\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    pi, value = self.trainable_network(observations)\n",
        "\n",
        "    action = pi.sample(seed=rng)\n",
        "    log_prob = pi.log_prob(action)\n",
        "\n",
        "    return action, {\"log_prob\": log_prob, \"value\": value}\n",
        "\n",
        "  def update(self,rng,obs,actions,next_obs,dones,info):\n",
        "    return {}\n",
        "\n",
        "  def batch_loss(self, rng, obs, actions, next_obs, dones, info):\n",
        "\n",
        "    _, value = self.trainable_network(obs)\n",
        "    reward = info[\"reward\"]\n",
        "    transition = (obs, actions, next_obs, dones, reward, info)\n",
        "\n",
        "    #GAE\n",
        "    def _calculate_gae(transition, value):\n",
        "      def _get_advantages(gae_and_next_value, transition):\n",
        "        gae, next_value = gae_and_next_value\n",
        "        obs, actions, next_obs, dones, reward, info = transition\n",
        "        delta = reward + self.gamma * next_value * (1 - dones) - info[\"value\"]\n",
        "        gae = (delta + self.gamma * self.gae_lambda * (1 - dones) * gae)\n",
        "        return (gae, value), gae\n",
        "\n",
        "      _, advantages = jax.lax.scan(\n",
        "            _get_advantages,\n",
        "            (jnp.zeros_like(info[\"value\"]), info[\"value\"]),\n",
        "            transition,\n",
        "            reverse=True,\n",
        "            unroll=16,\n",
        "      )\n",
        "      return advantages, advantages + info[\"value\"]\n",
        "\n",
        "    gae, targets = _calculate_gae(transition, value)\n",
        "\n",
        "    #Rerun Network\n",
        "    pi, next_value = self.trainable_network(next_obs)\n",
        "    log_prob = pi.log_prob(actions)\n",
        "\n",
        "    # value loss\n",
        "    value_pred_clipped = value + (next_value - value).clip(-self.clip_eps, self.clip_eps)\n",
        "    value_losses = jnp.square(next_value - targets)\n",
        "    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "    value_loss = (0.5 * jnp.maximum(value_losses, value_losses_clipped).mean())\n",
        "\n",
        "    # actor loss\n",
        "    ratio = jnp.exp(log_prob - info[\"log_prob\"])\n",
        "    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
        "    loss_actor1 = ratio * gae\n",
        "    loss_actor2 = (\n",
        "        jnp.clip(\n",
        "            ratio,\n",
        "            1.0 - self.clip_eps,\n",
        "            1.0 + self.clip_eps,\n",
        "        )\n",
        "        * gae\n",
        "    )\n",
        "    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
        "    loss_actor = loss_actor.mean()\n",
        "    entropy = pi.entropy().mean()\n",
        "\n",
        "    total_loss = loss_actor + self.vf_coef * value_loss - self.ent_coef * entropy\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "7q1bAyD9tT0g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_Z_V2JSg_h"
      },
      "source": [
        "Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wYwuxCahRDTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e5460b78-3d64-4781-8926-e4039b94f590"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nresult = {}\\npdfs = []\\n#for i in [8,16,32,64,128,256]:\\n  #  result[i] = {}\\nfor MODEL_NAME in [\"BayesianConjugate-v1\",\"DeepBayesianConjugate-v1\",\"DynamicSACBayesianExplorer-v1\",\\n                   \"DeepSACBayesianConjugate-v1\",\"DeepRandomBayesianConjugate-v1\"]:\\n    config[\"MODEL_NAME\"] = MODEL_NAME\\n    result[MODEL_NAME] =[]\\n    for seed in range(5):\\n        config[\"SEED\"] = 423+seed\\n        out , big ,pdf = experiment(config)\\n        result[MODEL_NAME].append(big)\\n        pdfs.append(pdf)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from jax.scipy.special import digamma, gammaln, kl_div\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import flax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any, Dict\n",
        "import distrax\n",
        "import gymnax\n",
        "import functools\n",
        "from gymnax.environments import spaces\n",
        "from gymnax.wrappers import FlattenObservationWrapper, LogWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optax\n",
        "from flax.nnx.helpers import TrainState\n",
        "\n",
        "class MyTrainState(TrainState):\n",
        "    vars: nnx.Variable\n",
        "    others: nnx.State\n",
        "\n",
        "    @property\n",
        "    def need_train(self):\n",
        "        return len(self.params) > 0\n",
        "\n",
        "is_trainable = lambda path, node: (\n",
        "    node.type == nnx.Param and \\\n",
        "    any('trainable' in p_elem for p_elem in path if isinstance(p_elem, str))\n",
        ")\n",
        "\n",
        "def train_state_from_model(model,tx=optax.adam(0.02)):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    print(trainable_params)\n",
        "\n",
        "    state = MyTrainState.create(\n",
        "      tx=tx,\n",
        "      graphdef=graphdef,\n",
        "      params=trainable_params,\n",
        "      vars=vars,\n",
        "      others=others,\n",
        "    )\n",
        "    return state\n",
        "\n",
        "def train_state_update_model(model,state):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    return state.replace(vars=vars,others=others)\n",
        "\n",
        "def model_from_train_state(state):\n",
        "    return nnx.merge(state.graphdef, state.params, state.vars,state.others)\n",
        "# prompt: draw heatmap given sequence of states for MountainCar\n",
        "#state.position, state.velocity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reshape(arr):\n",
        "    if arr.ndim < 3:\n",
        "        raise ValueError(\"Input array must have at least 3 dimensions (n, b, c, ...).\")\n",
        "\n",
        "    # Get the original shape components\n",
        "    n, b, c, *x_dims = arr.shape\n",
        "\n",
        "    # Transpose the first two axes (n, b) to (b, n)\n",
        "    # We construct the axes tuple dynamically for flexibility\n",
        "    transpose_axes = (1, 0) + tuple(range(2, arr.ndim))\n",
        "    transposed_arr = jnp.transpose(arr, axes=transpose_axes)\n",
        "\n",
        "    # Reshape into (b, n*c, x0, x1, ...)\n",
        "    new_shape = (b, n * c, *x_dims)\n",
        "    reshaped_arr = jnp.reshape(transposed_arr, new_shape)\n",
        "\n",
        "    return reshaped_arr\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "# Define a type alias for PyTree for better readability\n",
        "PyTree = Any\n",
        "def unpack_pytree_by_first_index(pytree: PyTree) -> List[PyTree]:\n",
        "    \"\"\"\n",
        "    Unpacks a PyTree of JAX arrays along their first dimension (id).\n",
        "\n",
        "    This function assumes that all JAX arrays within the PyTree\n",
        "    have a consistent first dimension (the 'id' dimension) and that\n",
        "    you want to create a separate PyTree for each 'id'.\n",
        "\n",
        "    Args:\n",
        "        pytree: A JAX PyTree where the leaves are JAX arrays\n",
        "                with a leading 'id' dimension.\n",
        "\n",
        "    Returns:\n",
        "        A list of PyTrees, where each PyTree corresponds to a single\n",
        "        'id' from the original PyTree.\n",
        "    \"\"\"\n",
        "    # Get the size of the first dimension from any leaf array\n",
        "    # We assume all arrays have the same first dimension size.\n",
        "    first_leaf = jax.tree_util.tree_leaves(pytree)[0]\n",
        "    num_ids = first_leaf.shape[0]\n",
        "\n",
        "    # Create a list to store the unpacked PyTrees\n",
        "    unpacked_pytrees = []\n",
        "\n",
        "    # Iterate through each ID\n",
        "    for i in range(num_ids):\n",
        "        # Use tree_map to slice each array in the PyTree at the current ID\n",
        "        sliced_pytree = jax.tree_util.tree_map(lambda x: x[i], pytree)\n",
        "        unpacked_pytrees.append(sliced_pytree)\n",
        "\n",
        "    return unpacked_pytrees\n",
        "def unpack_states(pytree):\n",
        "    return unpack_pytree_by_first_index(jax.tree.map(reshape, pytree))\n",
        "def draw_mountain_car_heatmap(state,config = {}):\n",
        "    \"\"\"\n",
        "    Draws a heatmap representing the trajectory of the MountainCar environment.\n",
        "\n",
        "    Args:\n",
        "        state_sequence: A sequence of JAX arrays representing the states\n",
        "                        of the MountainCar environment. Each state is expected\n",
        "                        to be a 2-element array [position, velocity].\n",
        "                        ['CartPole-v1',\"MountainCar-v0\",\"Acrobot-v1\"]\n",
        "    \"\"\"\n",
        "    title = config[\"ENV_NAME\"] +' MountainCar Heatmap ' +config[\"MODEL_NAME\"]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if config[\"ENV_NAME\"] == \"MountainCar-v0\":\n",
        "\n",
        "        positions = state.position\n",
        "        velocities = state.velocity\n",
        "\n",
        "        plt.scatter(positions, velocities, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Position')\n",
        "        plt.ylabel('Velocity')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"CartPole-v1\":\n",
        "        x = state.x\n",
        "        theta = state.theta\n",
        "        plt.scatter(x, theta, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('theta')\n",
        "        plt.grid(True)\n",
        "    elif config[\"ENV_NAME\"] == \"Acrobot-v1\":\n",
        "        joint_angle1 = state.joint_angle1\n",
        "        joint_angle2 = state.joint_angle2\n",
        "        plt.scatter(joint_angle1, joint_angle2, c=range(len(state.time )), cmap='viridis', s=10)\n",
        "        plt.colorbar(label='Time Steps')\n",
        "        plt.xlabel('Angle1')\n",
        "        plt.ylabel('Angle2')\n",
        "        plt.grid(True)\n",
        "    if \"TOTAL_TIMESTEPS\" in config:\n",
        "        title += \"_TOTAL_TIMESTEPS_\"+str(config[\"TOTAL_TIMESTEPS\"])\n",
        "    if \"DEPTH\" in config:\n",
        "        title += \"_DEPTH_\"+str(config[\"DEPTH\"])\n",
        "    if \"NUM_HIDDEN\" in config:\n",
        "        title += \"_NUM_HIDDEN_\"+str(config[\"NUM_HIDDEN\"])\n",
        "    plt.title(title)\n",
        "    plt.savefig(title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return plt\n",
        "\n",
        "\n",
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_train(config):\n",
        "\n",
        "    config[\"NUM_UPDATES\"] = (config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"]// config[\"NUM_ENVS\"])\n",
        "\n",
        "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "    rng_batch = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "    manager = UnsupervisedRolloutWrapper(config[\"ENV_NAME\"])\n",
        "    # it serves as action_dim which is 2 now\n",
        "    num_actions = manager.env.num_actions()\n",
        "    obs_dim = manager.env.observation_space(manager.env_params).shape[0]\n",
        "\n",
        "\n",
        "    low = manager.env.observation_space(manager.env_params).low\n",
        "    high = manager.env.observation_space(manager.env_params).high\n",
        "\n",
        "    print (\"low\",low)\n",
        "    print (\"high\",high )\n",
        "    if config[\"MODEL_NAME\"] == \"DeepBayesianExplorer\":\n",
        "        model = DeepBayesianExplorer(obs_dim, num_actions,config[\"NUM_HIDDEN\"],\n",
        "                                    nnx.Rngs(config[\"SEED\"]),weight_decay=config[\"WD\"],depth=config[\"DEPTH\"])\n",
        "\n",
        "    if config[\"MODEL_NAME\"] == \"PPOExplorer\":\n",
        "        model = PPOExplorer(obs_dim,num_actions,config[\"NUM_HIDDEN\"],nnx.Rngs(config[\"SEED\"]),)\n",
        "\n",
        "    if config[\"MODEL_NAME\"] == \"RandomExplorer\":\n",
        "        model = RandomExplorer(num_actions)\n",
        "\n",
        "    @nnx.jit\n",
        "    def _train_step(state:MyTrainState, rng_loss, obs, action,next_obs,done,info):\n",
        "\n",
        "      def loss_fn(graphdef,params,vars,others):\n",
        "        model = nnx.merge(graphdef, params, vars,others)\n",
        "        return model.batch_loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "\n",
        "      def opt_step(state,unused):\n",
        "        grads = jax.grad(loss_fn,1)(state.graphdef, state.params, state.vars,state.others)\n",
        "        return state.apply_gradients(grads=grads),None\n",
        "      state, _ = jax.lax.scan(opt_step, state, None, config[\"OPT_STEPS\"])\n",
        "\n",
        "      return state\n",
        "    @nnx.jit\n",
        "    def _rollout_and_update_step(runner_state, unused):\n",
        "        # we have to use train_state for jax.lax.scan\n",
        "        train_state,  rng_batch,last_state = runner_state\n",
        "\n",
        "        model = model_from_train_state(train_state)\n",
        "        rng_batch, rng_step,rng_update,rng_loss = batch_random_split(rng_batch,4)\n",
        "\n",
        "        rollout_results = manager.batch_rollout( rng_batch,model,env_state=last_state,num_steps =  config[\"NUM_STEPS\"])\n",
        "        obs, action, reward, next_obs, done,state,info, cum_ret = rollout_results\n",
        "\n",
        "        # obs: num_envs x\n",
        "        transition = Transition(obs, action, reward, next_obs, done,info)\n",
        "\n",
        "        last_state = info[\"last_state\"]\n",
        "        info[\"reward\"] = reward\n",
        "        update_info = manager.batch_update(rng_update, model,obs, action,next_obs,done,info)\n",
        "        info.update(update_info)\n",
        "        train_state = train_state_update_model(model,train_state)\n",
        "\n",
        "        if train_state.need_train:\n",
        "            train_state = _train_step(train_state, rng_loss, obs, action,next_obs,done,info)\n",
        "\n",
        "        #works for tensors\n",
        "        runner_state = (train_state, rng_batch,last_state)\n",
        "        return runner_state, (transition, state)\n",
        "\n",
        "    def train(rng_batch,model,manager):\n",
        "        # training loop\n",
        "\n",
        "        rng_batch,  rng_reset = batch_random_split(rng_batch, 2)\n",
        "        start_state = manager.batch_reset(rng_reset)\n",
        "\n",
        "        if config[\"TX\"] == \"adamw\":\n",
        "            tx = optax.adamw(config[\"LR\"])\n",
        "        elif config[\"TX\"] == \"sgd\":\n",
        "            tx = optax.sgd(config[\"LR\"])\n",
        "        else:\n",
        "            tx = None\n",
        "            assert False, config[\"TX\"] + \" is not avaliable\"\n",
        "        train_state = train_state_from_model(model,tx)\n",
        "      #  rng, _rng = jax.random.split(rng)\n",
        "        runner_state = (train_state,  rng_batch,start_state)\n",
        "        runner_state, output= jax.lax.scan(_rollout_and_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
        "\n",
        "        transitions,states = output\n",
        "        return {\"runner_state\": runner_state, \"transitions\": transitions,\"states\":states}\n",
        "        # return {\"runner_state\": runner_state, \"collect_data\": collect_data, \"max_mi_history\": max_mi_history}\n",
        "\n",
        "    return train,model, manager,rng_batch\n",
        "\n",
        "def experiment(config):\n",
        "    print(config)\n",
        "    train_fn,model, manager,rng_batch = make_train(config)\n",
        "    train_jit = nnx.jit(train_fn)\n",
        "\n",
        " #   show_variable(model,\"explorer before\")\n",
        "    out = jax.block_until_ready(train_fn(rng_batch,model,manager))\n",
        "    #data shape: rollout groups = [TOTAL_TIMESTEPS//NUM_ENVS //NUM_STEPS] x NUM_ENVS x NUM_STEPS\n",
        "    print(\"data shape:\", jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))\n",
        "\n",
        "    train_state,  rng_batch, last_state = out[\"runner_state\"]\n",
        "\n",
        "    model = model_from_train_state(train_state)\n",
        "    #print (\"model\",model)\n",
        "\n",
        "\n",
        "\n",
        "    # if \"mi\" in out[\"transitions\"].info:\n",
        "    # # Create figure and axis\n",
        "    #     plt.figure(figsize=(10, 6))\n",
        "    #     # Sample JAX NumPy arrays (replace these with your actual arrays)\n",
        "    #     #  print (out[\"transitions\"].info)\n",
        "    #     eig_array = out[\"transitions\"].info[\"mi\"].reshape(-1)\n",
        "    #     big_array = out[\"transitions\"].info[\"kl\"].reshape(-1)\n",
        "    #     # Plot both arrays\n",
        "    #     plt.plot(eig_array, label='EIG', marker='o', linestyle='-', color='blue')\n",
        "    #     plt.plot(big_array, label='BIG', marker='s', linestyle='-', color='red')\n",
        "\n",
        "    #     if \"smi\" in out[\"transitions\"].info:\n",
        "    #         smi_array = out[\"transitions\"].info[\"smi\"].reshape(-1)\n",
        "    #         plt.plot(smi_array, label='SMI', marker='^', linestyle='-', color='green')\n",
        "\n",
        "    #     # Add labels and title\n",
        "    #     plt.xlabel('Num of Updates')\n",
        "    #     plt.ylabel('Information Gain')\n",
        "    #     Title = \"InfoGains for \"+  config[\"MODEL_NAME\"]\n",
        "    #     Title = Title + \"Total InfoGains\" +\"{:10.4f}\".format(big_array.sum().item())\n",
        "    #     Title = Title +  \" with Seed\" +str(config[\"SEED\"])\n",
        "    #     plt.title(Title)\n",
        "\n",
        "    #     # Add grid and legend\n",
        "    #     plt.grid(alpha=0.3)\n",
        "    #     plt.legend()\n",
        "    #     # Show the plot\n",
        "    #   # plt.ylim(0, 40)\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    #     plt.show()\n",
        "    # if \"l_prec\" in  out[\"transitions\"].info:\n",
        "    #     l_prec_mean = out[\"transitions\"].info[\"l_prec\"].mean(axis=(1,2,3),keepdims=False)\n",
        "    #  #   prec_mean = out[\"transitions\"].info[\"prec\"].mean(axis=(1,2,3),keepdims=False)\n",
        "    #     mean_error = out[\"transitions\"].info[\"mean_error\"].mean(axis=(1,2),keepdims=False)\n",
        "\n",
        "    #     # Create figure and axis\n",
        "    #     plt.figure(figsize=(10, 6))\n",
        "\n",
        "    #     # Plot both arrays\n",
        "    #     plt.plot(l_prec_mean, label='l_prec', marker='o', linestyle='-', color='blue')\n",
        "    # #    plt.plot(prec_mean, label='prec', marker='s', linestyle='-', color='red')\n",
        "    #     plt.plot(mean_error, label='mean_error', marker='p', linestyle='-', color='yellow')\n",
        "\n",
        "    #     # Add labels and title\n",
        "    #     plt.xlabel('Num of Updates')\n",
        "    #     plt.ylabel('Mean Precision')\n",
        "    #     Title = \"Comparison of Mean Precisions\"\n",
        "\n",
        "    #     plt.title(Title)\n",
        "\n",
        "    #     # Add grid and legend\n",
        "    #     plt.grid(alpha=0.3)\n",
        "    #     plt.legend()\n",
        "    #     # Show the plot\n",
        "    #     plt.tight_layout()\n",
        "    #     plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "    #     plt.show()\n",
        "\n",
        "    # draw_mountain_car_heatmap( unpack_states(out[\"states\"])[0],config)\n",
        "    return out\n",
        "'''\n",
        "\n",
        "result = {}\n",
        "pdfs = []\n",
        "#for i in [8,16,32,64,128,256]:\n",
        "  #  result[i] = {}\n",
        "for MODEL_NAME in [\"BayesianConjugate-v1\",\"DeepBayesianConjugate-v1\",\"DynamicSACBayesianExplorer-v1\",\n",
        "                   \"DeepSACBayesianConjugate-v1\",\"DeepRandomBayesianConjugate-v1\"]:\n",
        "    config[\"MODEL_NAME\"] = MODEL_NAME\n",
        "    result[MODEL_NAME] =[]\n",
        "    for seed in range(5):\n",
        "        config[\"SEED\"] = 423+seed\n",
        "        out , big ,pdf = experiment(config)\n",
        "        result[MODEL_NAME].append(big)\n",
        "        pdfs.append(pdf)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}\n",
        "\n",
        "env_name = 'Umaze'  # @param [\"Umaze\"] {\"type\":\"raw\"}\n",
        "NUM_ENVS = 4 # @param [1,2,4,8,16,32] {\"type\":\"raw\"}\n",
        "TOTAL_TIMESTEPS = 2048 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 16 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"DeepBayesianExplorer\"  #@param [\"DeepBayesianExplorer\",\"RandomExplorer\",\"PPOExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}\n",
        "\n",
        "\n",
        "out = experiment(config)"
      ],
      "metadata": {
        "id": "K02J3fdONsho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAC"
      ],
      "metadata": {
        "id": "SsxysYYM91uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "7OkH51n9_2Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import glob\n",
        "import os\n",
        "import pickle\n",
        "from typing import Any, Dict, Mapping, Sequence\n",
        "\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "nonpytree_field = functools.partial(flax.struct.field, pytree_node=False)\n",
        "\n",
        "\n",
        "class ModuleDict(nn.Module):\n",
        "    \"\"\"A dictionary of modules.\n",
        "\n",
        "    This allows sharing parameters between modules and provides a convenient way to access them.\n",
        "\n",
        "    Attributes:\n",
        "        modules: Dictionary of modules.\n",
        "    \"\"\"\n",
        "\n",
        "    modules: Dict[str, nn.Module]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, *args, name=None, **kwargs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        For initialization, call with `name=None` and provide the arguments for each module in `kwargs`.\n",
        "        Otherwise, call with `name=<module_name>` and provide the arguments for that module.\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            if kwargs.keys() != self.modules.keys():\n",
        "                raise ValueError(\n",
        "                    f'When `name` is not specified, kwargs must contain the arguments for each module. '\n",
        "                    f'Got kwargs keys {kwargs.keys()} but module keys {self.modules.keys()}'\n",
        "                )\n",
        "            out = {}\n",
        "            for key, value in kwargs.items():\n",
        "                if isinstance(value, Mapping):\n",
        "                    out[key] = self.modules[key](**value)\n",
        "                elif isinstance(value, Sequence):\n",
        "                    out[key] = self.modules[key](*value)\n",
        "                else:\n",
        "                    out[key] = self.modules[key](value)\n",
        "            return out\n",
        "\n",
        "        return self.modules[name](*args, **kwargs)\n",
        "\n",
        "\n",
        "class TrainState(flax.struct.PyTreeNode):\n",
        "    \"\"\"Custom train state for models.\n",
        "\n",
        "    Attributes:\n",
        "        step: Counter to keep track of the training steps. It is incremented by 1 after each `apply_gradients` call.\n",
        "        apply_fn: Apply function of the model.\n",
        "        model_def: Model definition.\n",
        "        params: Parameters of the model.\n",
        "        tx: optax optimizer.\n",
        "        opt_state: Optimizer state.\n",
        "    \"\"\"\n",
        "\n",
        "    step: int\n",
        "    apply_fn: Any = nonpytree_field()\n",
        "    model_def: Any = nonpytree_field()\n",
        "    params: Any\n",
        "    tx: Any = nonpytree_field()\n",
        "    opt_state: Any\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, model_def, params, tx=None, **kwargs):\n",
        "        \"\"\"Create a new train state.\"\"\"\n",
        "        if tx is not None:\n",
        "            opt_state = tx.init(params)\n",
        "        else:\n",
        "            opt_state = None\n",
        "\n",
        "        return cls(\n",
        "            step=1,\n",
        "            apply_fn=model_def.apply,\n",
        "            model_def=model_def,\n",
        "            params=params,\n",
        "            tx=tx,\n",
        "            opt_state=opt_state,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def __call__(self, *args, params=None, method=None, **kwargs):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        When `params` is not provided, it uses the stored parameters.\n",
        "\n",
        "        The typical use case is to set `params` to `None` when you want to *stop* the gradients, and to pass the current\n",
        "        traced parameters when you want to flow the gradients. In other words, the default behavior is to stop the\n",
        "        gradients, and you need to explicitly provide the parameters to flow the gradients.\n",
        "\n",
        "        Args:\n",
        "            *args: Arguments to pass to the model.\n",
        "            params: Parameters to use for the forward pass. If `None`, it uses the stored parameters, without flowing\n",
        "                the gradients.\n",
        "            method: Method to call in the model. If `None`, it uses the default `apply` method.\n",
        "            **kwargs: Keyword arguments to pass to the model.\n",
        "        \"\"\"\n",
        "        if params is None:\n",
        "            params = self.params\n",
        "        variables = {'params': params}\n",
        "        if method is not None:\n",
        "            method_name = getattr(self.model_def, method)\n",
        "        else:\n",
        "            method_name = None\n",
        "\n",
        "        return self.apply_fn(variables, *args, method=method_name, **kwargs)\n",
        "\n",
        "    def select(self, name):\n",
        "        \"\"\"Helper function to select a module from a `ModuleDict`.\"\"\"\n",
        "        return functools.partial(self, name=name)\n",
        "\n",
        "    def apply_gradients(self, grads, **kwargs):\n",
        "        \"\"\"Apply the gradients and return the updated state.\"\"\"\n",
        "        updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)\n",
        "        new_params = optax.apply_updates(self.params, updates)\n",
        "\n",
        "        return self.replace(\n",
        "            step=self.step + 1,\n",
        "            params=new_params,\n",
        "            opt_state=new_opt_state,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def apply_loss_fn(self, loss_fn):\n",
        "        \"\"\"Apply the loss function and return the updated state and info.\n",
        "\n",
        "        It additionally computes the gradient statistics and adds them to the dictionary.\n",
        "        \"\"\"\n",
        "        grads, info = jax.grad(loss_fn, has_aux=True)(self.params)\n",
        "\n",
        "        grad_max = jax.tree_util.tree_map(jnp.max, grads)\n",
        "        grad_min = jax.tree_util.tree_map(jnp.min, grads)\n",
        "        grad_norm = jax.tree_util.tree_map(jnp.linalg.norm, grads)\n",
        "\n",
        "        grad_max_flat = jnp.concatenate([jnp.reshape(x, -1) for x in jax.tree_util.tree_leaves(grad_max)], axis=0)\n",
        "        grad_min_flat = jnp.concatenate([jnp.reshape(x, -1) for x in jax.tree_util.tree_leaves(grad_min)], axis=0)\n",
        "        grad_norm_flat = jnp.concatenate([jnp.reshape(x, -1) for x in jax.tree_util.tree_leaves(grad_norm)], axis=0)\n",
        "\n",
        "        final_grad_max = jnp.max(grad_max_flat)\n",
        "        final_grad_min = jnp.min(grad_min_flat)\n",
        "        final_grad_norm = jnp.linalg.norm(grad_norm_flat, ord=1)\n",
        "\n",
        "        info.update(\n",
        "            {\n",
        "                'grad/max': final_grad_max,\n",
        "                'grad/min': final_grad_min,\n",
        "                'grad/norm': final_grad_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return self.apply_gradients(grads=grads), info\n",
        "\n",
        "\n",
        "def save_agent(agent, save_dir, epoch):\n",
        "    \"\"\"Save the agent to a file.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent.\n",
        "        save_dir: Directory to save the agent.\n",
        "        epoch: Epoch number.\n",
        "    \"\"\"\n",
        "\n",
        "    save_dict = dict(\n",
        "        agent=flax.serialization.to_state_dict(agent),\n",
        "    )\n",
        "    save_path = os.path.join(save_dir, f'params_{epoch}.pkl')\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(save_dict, f)\n",
        "\n",
        "    print(f'Saved to {save_path}')\n",
        "\n",
        "\n",
        "def restore_agent(agent, restore_path, restore_epoch):\n",
        "    \"\"\"Restore the agent from a file.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent.\n",
        "        restore_path: Path to the directory containing the saved agent.\n",
        "        restore_epoch: Epoch number.\n",
        "    \"\"\"\n",
        "    candidates = glob.glob(restore_path)\n",
        "\n",
        "    assert len(candidates) == 1, f'Found {len(candidates)} candidates: {candidates}'\n",
        "\n",
        "    restore_path = candidates[0] + f'/params_{restore_epoch}.pkl'\n",
        "\n",
        "    with open(restore_path, 'rb') as f:\n",
        "        load_dict = pickle.load(f)\n",
        "\n",
        "    agent = flax.serialization.from_state_dict(agent, load_dict['agent'])\n",
        "\n",
        "    print(f'Restored from {restore_path}')\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# network\n",
        "from typing import Any, Optional, Sequence\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "def default_init(scale=1.0):\n",
        "    \"\"\"Default kernel initializer.\"\"\"\n",
        "    return nn.initializers.variance_scaling(scale, 'fan_avg', 'uniform')\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    \"\"\"Ensemblize a module.\"\"\"\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={'params': 0},\n",
        "        split_rngs={'params': True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    \"\"\"Identity layer.\"\"\"\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-layer perceptron.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        activations: Activation function.\n",
        "        activate_final: Whether to apply activation to the final layer.\n",
        "        kernel_init: Kernel initializer.\n",
        "        layer_norm: Whether to apply layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Any = nn.gelu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Any = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for i, size in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(size, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                x = self.activations(x)\n",
        "                if self.layer_norm:\n",
        "                    x = nn.LayerNorm()(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LengthNormalize(nn.Module):\n",
        "    \"\"\"Length normalization layer.\n",
        "\n",
        "    It normalizes the input along the last dimension to have a length of sqrt(dim).\n",
        "    \"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return x / jnp.linalg.norm(x, axis=-1, keepdims=True) * jnp.sqrt(x.shape[-1])\n",
        "\n",
        "\n",
        "class Param(nn.Module):\n",
        "    \"\"\"Scalar parameter module.\"\"\"\n",
        "\n",
        "    init_value: float = 0.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self):\n",
        "        return self.param('value', init_fn=lambda key: jnp.full((), self.init_value))\n",
        "\n",
        "\n",
        "class LogParam(nn.Module):\n",
        "    \"\"\"Scalar parameter module with log scale.\"\"\"\n",
        "\n",
        "    init_value: float = 1.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self):\n",
        "        log_value = self.param('log_value', init_fn=lambda key: jnp.full((), jnp.log(self.init_value)))\n",
        "        return jnp.exp(log_value)\n",
        "\n",
        "\n",
        "class TransformedWithMode(distrax.Transformed):\n",
        "    \"\"\"Transformed distribution with mode calculation.\"\"\"\n",
        "\n",
        "    def mode(self):\n",
        "        return self.bijector.forward(self.distribution.mode())\n",
        "\n",
        "\n",
        "class RunningMeanStd(flax.struct.PyTreeNode):\n",
        "    \"\"\"Running mean and standard deviation.\n",
        "\n",
        "    Attributes:\n",
        "        eps: Epsilon value to avoid division by zero.\n",
        "        mean: Running mean.\n",
        "        var: Running variance.\n",
        "        clip_max: Clip value after normalization.\n",
        "        count: Number of samples.\n",
        "    \"\"\"\n",
        "\n",
        "    eps: Any = 1e-6\n",
        "    mean: Any = 1.0\n",
        "    var: Any = 1.0\n",
        "    clip_max: Any = 10.0\n",
        "    count: int = 0\n",
        "\n",
        "    def normalize(self, batch):\n",
        "        batch = (batch - self.mean) / jnp.sqrt(self.var + self.eps)\n",
        "        batch = jnp.clip(batch, -self.clip_max, self.clip_max)\n",
        "        return batch\n",
        "\n",
        "    def unnormalize(self, batch):\n",
        "        return batch * jnp.sqrt(self.var + self.eps) + self.mean\n",
        "\n",
        "    def update(self, batch):\n",
        "        batch_mean, batch_var = jnp.mean(batch, axis=0), jnp.var(batch, axis=0)\n",
        "        batch_count = len(batch)\n",
        "\n",
        "        delta = batch_mean - self.mean\n",
        "        total_count = self.count + batch_count\n",
        "\n",
        "        new_mean = self.mean + delta * batch_count / total_count\n",
        "        m_a = self.var * self.count\n",
        "        m_b = batch_var * batch_count\n",
        "        m_2 = m_a + m_b + delta**2 * self.count * batch_count / total_count\n",
        "        new_var = m_2 / total_count\n",
        "\n",
        "        return self.replace(mean=new_mean, var=new_var, count=total_count)\n",
        "\n",
        "\n",
        "class GCActor(nn.Module):\n",
        "    \"\"\"Goal-conditioned actor.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        action_dim: Action dimension.\n",
        "        log_std_min: Minimum value of log standard deviation.\n",
        "        log_std_max: Maximum value of log standard deviation.\n",
        "        tanh_squash: Whether to squash the action with tanh.\n",
        "        state_dependent_std: Whether to use state-dependent standard deviation.\n",
        "        const_std: Whether to use constant standard deviation.\n",
        "        final_fc_init_scale: Initial scale of the final fully-connected layer.\n",
        "        gc_encoder: Optional GCEncoder module to encode the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5\n",
        "    log_std_max: Optional[float] = 2\n",
        "    tanh_squash: bool = False\n",
        "    state_dependent_std: bool = False\n",
        "    const_std: bool = True\n",
        "    final_fc_init_scale: float = 1e-2\n",
        "    gc_encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        self.actor_net = MLP(self.hidden_dims, activate_final=True)\n",
        "        self.mean_net = nn.Dense(self.action_dim, kernel_init=default_init(self.final_fc_init_scale))\n",
        "        if self.state_dependent_std:\n",
        "            self.log_std_net = nn.Dense(self.action_dim, kernel_init=default_init(self.final_fc_init_scale))\n",
        "        else:\n",
        "            if not self.const_std:\n",
        "                self.log_stds = self.param('log_stds', nn.initializers.zeros, (self.action_dim,))\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        observations,\n",
        "        goals=None,\n",
        "        goal_encoded=False,\n",
        "        temperature=1.0,\n",
        "    ):\n",
        "        \"\"\"Return the action distribution.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals (optional).\n",
        "            goal_encoded: Whether the goals are already encoded.\n",
        "            temperature: Scaling factor for the standard deviation.\n",
        "        \"\"\"\n",
        "        if self.gc_encoder is not None:\n",
        "            inputs = self.gc_encoder(observations, goals, goal_encoded=goal_encoded)\n",
        "        else:\n",
        "            inputs = [observations]\n",
        "            if goals is not None:\n",
        "                inputs.append(goals)\n",
        "            inputs = jnp.concatenate(inputs, axis=-1)\n",
        "        outputs = self.actor_net(inputs)\n",
        "\n",
        "        means = self.mean_net(outputs)\n",
        "        if self.state_dependent_std:\n",
        "            log_stds = self.log_std_net(outputs)\n",
        "        else:\n",
        "            if self.const_std:\n",
        "                log_stds = jnp.zeros_like(means)\n",
        "            else:\n",
        "                log_stds = self.log_stds\n",
        "\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(loc=means, scale_diag=jnp.exp(log_stds) * temperature)\n",
        "        if self.tanh_squash:\n",
        "            distribution = TransformedWithMode(distribution, distrax.Block(distrax.Tanh(), ndims=1))\n",
        "\n",
        "        return distribution\n",
        "\n",
        "\n",
        "class GCDiscreteActor(nn.Module):\n",
        "    \"\"\"Goal-conditioned actor for discrete actions.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        action_dim: Action dimension.\n",
        "        final_fc_init_scale: Initial scale of the final fully-connected layer.\n",
        "        gc_encoder: Optional GCEncoder module to encode the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    final_fc_init_scale: float = 1e-2\n",
        "    gc_encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        self.actor_net = MLP(self.hidden_dims, activate_final=True)\n",
        "        self.logit_net = nn.Dense(self.action_dim, kernel_init=default_init(self.final_fc_init_scale))\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        observations,\n",
        "        goals=None,\n",
        "        goal_encoded=False,\n",
        "        temperature=1.0,\n",
        "    ):\n",
        "        \"\"\"Return the action distribution.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals (optional).\n",
        "            goal_encoded: Whether the goals are already encoded.\n",
        "            temperature: Inverse scaling factor for the logits (set to 0 to get the argmax).\n",
        "        \"\"\"\n",
        "        if self.gc_encoder is not None:\n",
        "            inputs = self.gc_encoder(observations, goals, goal_encoded=goal_encoded)\n",
        "        else:\n",
        "            inputs = [observations]\n",
        "            if goals is not None:\n",
        "                inputs.append(goals)\n",
        "            inputs = jnp.concatenate(inputs, axis=-1)\n",
        "        outputs = self.actor_net(inputs)\n",
        "\n",
        "        logits = self.logit_net(outputs)\n",
        "\n",
        "        distribution = distrax.Categorical(logits=logits / jnp.maximum(1e-6, temperature))\n",
        "\n",
        "        return distribution\n",
        "\n",
        "\n",
        "class GCValue(nn.Module):\n",
        "    \"\"\"Goal-conditioned value/critic function.\n",
        "\n",
        "    This module can be used for both value V(s, g) and critic Q(s, a, g) functions.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        layer_norm: Whether to apply layer normalization.\n",
        "        ensemble: Whether to ensemble the value function.\n",
        "        gc_encoder: Optional GCEncoder module to encode the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = True\n",
        "    ensemble: bool = True\n",
        "    gc_encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        mlp_module = MLP\n",
        "        if self.ensemble:\n",
        "            mlp_module = ensemblize(mlp_module, 2)\n",
        "        value_net = mlp_module((*self.hidden_dims, 1), activate_final=False, layer_norm=self.layer_norm)\n",
        "\n",
        "        self.value_net = value_net\n",
        "\n",
        "    def __call__(self, observations, goals=None, actions=None):\n",
        "        \"\"\"Return the value/critic function.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals (optional).\n",
        "            actions: Actions (optional).\n",
        "        \"\"\"\n",
        "        if self.gc_encoder is not None:\n",
        "            inputs = [self.gc_encoder(observations, goals)]\n",
        "        else:\n",
        "            inputs = [observations]\n",
        "            if goals is not None:\n",
        "                inputs.append(goals)\n",
        "        if actions is not None:\n",
        "            inputs.append(actions)\n",
        "        inputs = jnp.concatenate(inputs, axis=-1)\n",
        "\n",
        "        v = self.value_net(inputs).squeeze(-1)\n",
        "\n",
        "        return v\n",
        "\n",
        "\n",
        "class GCDiscreteCritic(GCValue):\n",
        "    \"\"\"Goal-conditioned critic for discrete actions.\"\"\"\n",
        "\n",
        "    action_dim: int = None\n",
        "\n",
        "    def __call__(self, observations, goals=None, actions=None):\n",
        "        actions = jnp.eye(self.action_dim)[actions]\n",
        "        return super().__call__(observations, goals, actions)\n",
        "\n",
        "\n",
        "class GCBilinearValue(nn.Module):\n",
        "    \"\"\"Goal-conditioned bilinear value/critic function.\n",
        "\n",
        "    This module computes the value function as V(s, g) = phi(s)^T psi(g) / sqrt(d) or the critic function as\n",
        "    Q(s, a, g) = phi(s, a)^T psi(g) / sqrt(d), where phi and psi output d-dimensional vectors.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        latent_dim: Latent dimension.\n",
        "        layer_norm: Whether to apply layer normalization.\n",
        "        ensemble: Whether to ensemble the value function.\n",
        "        value_exp: Whether to exponentiate the value. Useful for contrastive learning.\n",
        "        state_encoder: Optional state encoder.\n",
        "        goal_encoder: Optional goal encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    latent_dim: int\n",
        "    layer_norm: bool = True\n",
        "    ensemble: bool = True\n",
        "    value_exp: bool = False\n",
        "    state_encoder: nn.Module = None\n",
        "    goal_encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        mlp_module = MLP\n",
        "        if self.ensemble:\n",
        "            mlp_module = ensemblize(mlp_module, 2)\n",
        "\n",
        "        self.phi = mlp_module((*self.hidden_dims, self.latent_dim), activate_final=False, layer_norm=self.layer_norm)\n",
        "        self.psi = mlp_module((*self.hidden_dims, self.latent_dim), activate_final=False, layer_norm=self.layer_norm)\n",
        "\n",
        "    def __call__(self, observations, goals, actions=None, info=False):\n",
        "        \"\"\"Return the value/critic function.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals.\n",
        "            actions: Actions (optional).\n",
        "            info: Whether to additionally return the representations phi and psi.\n",
        "        \"\"\"\n",
        "        if self.state_encoder is not None:\n",
        "            observations = self.state_encoder(observations)\n",
        "        if self.goal_encoder is not None:\n",
        "            goals = self.goal_encoder(goals)\n",
        "\n",
        "        if actions is None:\n",
        "            phi_inputs = observations\n",
        "        else:\n",
        "            phi_inputs = jnp.concatenate([observations, actions], axis=-1)\n",
        "\n",
        "        phi = self.phi(phi_inputs)\n",
        "        psi = self.psi(goals)\n",
        "\n",
        "        v = (phi * psi / jnp.sqrt(self.latent_dim)).sum(axis=-1)\n",
        "\n",
        "        if self.value_exp:\n",
        "            v = jnp.exp(v)\n",
        "\n",
        "        if info:\n",
        "            return v, phi, psi\n",
        "        else:\n",
        "            return v\n",
        "\n",
        "\n",
        "class GCDiscreteBilinearCritic(GCBilinearValue):\n",
        "    \"\"\"Goal-conditioned bilinear critic for discrete actions.\"\"\"\n",
        "\n",
        "    action_dim: int = None\n",
        "\n",
        "    def __call__(self, observations, goals=None, actions=None, info=False):\n",
        "        actions = jnp.eye(self.action_dim)[actions]\n",
        "        return super().__call__(observations, goals, actions, info)\n",
        "\n",
        "\n",
        "class GCMRNValue(nn.Module):\n",
        "    \"\"\"Metric residual network (MRN) value function.\n",
        "\n",
        "    This module computes the value function as the sum of a symmetric Euclidean distance and an asymmetric\n",
        "    L^infinity-based quasimetric.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        latent_dim: Latent dimension.\n",
        "        layer_norm: Whether to apply layer normalization.\n",
        "        encoder: Optional state/goal encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    latent_dim: int\n",
        "    layer_norm: bool = True\n",
        "    encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        self.phi = MLP((*self.hidden_dims, self.latent_dim), activate_final=False, layer_norm=self.layer_norm)\n",
        "\n",
        "    def __call__(self, observations, goals, is_phi=False, info=False):\n",
        "        \"\"\"Return the MRN value function.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals.\n",
        "            is_phi: Whether the inputs are already encoded by phi.\n",
        "            info: Whether to additionally return the representations phi_s and phi_g.\n",
        "        \"\"\"\n",
        "        if is_phi:\n",
        "            phi_s = observations\n",
        "            phi_g = goals\n",
        "        else:\n",
        "            if self.encoder is not None:\n",
        "                observations = self.encoder(observations)\n",
        "                goals = self.encoder(goals)\n",
        "            phi_s = self.phi(observations)\n",
        "            phi_g = self.phi(goals)\n",
        "\n",
        "        sym_s = phi_s[..., : self.latent_dim // 2]\n",
        "        sym_g = phi_g[..., : self.latent_dim // 2]\n",
        "        asym_s = phi_s[..., self.latent_dim // 2 :]\n",
        "        asym_g = phi_g[..., self.latent_dim // 2 :]\n",
        "        squared_dist = ((sym_s - sym_g) ** 2).sum(axis=-1)\n",
        "        quasi = jax.nn.relu((asym_s - asym_g).max(axis=-1))\n",
        "        v = jnp.sqrt(jnp.maximum(squared_dist, 1e-12)) + quasi\n",
        "\n",
        "        if info:\n",
        "            return v, phi_s, phi_g\n",
        "        else:\n",
        "            return v\n",
        "\n",
        "\n",
        "class GCIQEValue(nn.Module):\n",
        "    \"\"\"Interval quasimetric embedding (IQE) value function.\n",
        "\n",
        "    This module computes the value function as an IQE-based quasimetric.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dims: Hidden layer dimensions.\n",
        "        latent_dim: Latent dimension.\n",
        "        dim_per_component: Dimension of each component in IQE (i.e., number of intervals in each group).\n",
        "        layer_norm: Whether to apply layer normalization.\n",
        "        encoder: Optional state/goal encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    hidden_dims: Sequence[int]\n",
        "    latent_dim: int\n",
        "    dim_per_component: int\n",
        "    layer_norm: bool = True\n",
        "    encoder: nn.Module = None\n",
        "\n",
        "    def setup(self):\n",
        "        self.phi = MLP((*self.hidden_dims, self.latent_dim), activate_final=False, layer_norm=self.layer_norm)\n",
        "        self.alpha = Param()\n",
        "\n",
        "    def __call__(self, observations, goals, is_phi=False, info=False):\n",
        "        \"\"\"Return the IQE value function.\n",
        "\n",
        "        Args:\n",
        "            observations: Observations.\n",
        "            goals: Goals.\n",
        "            is_phi: Whether the inputs are already encoded by phi.\n",
        "            info: Whether to additionally return the representations phi_s and phi_g.\n",
        "        \"\"\"\n",
        "        alpha = jax.nn.sigmoid(self.alpha())\n",
        "        if is_phi:\n",
        "            phi_s = observations\n",
        "            phi_g = goals\n",
        "        else:\n",
        "            if self.encoder is not None:\n",
        "                observations = self.encoder(observations)\n",
        "                goals = self.encoder(goals)\n",
        "            phi_s = self.phi(observations)\n",
        "            phi_g = self.phi(goals)\n",
        "\n",
        "        x = jnp.reshape(phi_s, (*phi_s.shape[:-1], -1, self.dim_per_component))\n",
        "        y = jnp.reshape(phi_g, (*phi_g.shape[:-1], -1, self.dim_per_component))\n",
        "        valid = x < y\n",
        "        xy = jnp.concatenate(jnp.broadcast_arrays(x, y), axis=-1)\n",
        "        ixy = xy.argsort(axis=-1)\n",
        "        sxy = jnp.take_along_axis(xy, ixy, axis=-1)\n",
        "        neg_inc_copies = jnp.take_along_axis(valid, ixy % self.dim_per_component, axis=-1) * jnp.where(\n",
        "            ixy < self.dim_per_component, -1, 1\n",
        "        )\n",
        "        neg_inp_copies = jnp.cumsum(neg_inc_copies, axis=-1)\n",
        "        neg_f = -1.0 * (neg_inp_copies < 0)\n",
        "        neg_incf = jnp.concatenate([neg_f[..., :1], neg_f[..., 1:] - neg_f[..., :-1]], axis=-1)\n",
        "        components = (sxy * neg_incf).sum(axis=-1)\n",
        "        v = alpha * components.mean(axis=-1) + (1 - alpha) * components.max(axis=-1)\n",
        "\n",
        "        if info:\n",
        "            return v, phi_s, phi_g\n",
        "        else:\n",
        "            return v"
      ],
      "metadata": {
        "id": "2rQc5CgH_klE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### agent"
      ],
      "metadata": {
        "id": "ldB7q0_h_5ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from typing import Any\n",
        "\n",
        "import flax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import ml_collections\n",
        "import optax\n",
        "\n",
        "# from utils.flax_utils import ModuleDict, TrainState, nonpytree_field\n",
        "# from utils.networks import GCActor, GCValue, LogParam\n",
        "\n",
        "\n",
        "class SACAgent(flax.struct.PyTreeNode):\n",
        "    \"\"\"Soft actor-critic (SAC) agent.\"\"\"\n",
        "\n",
        "    rng: Any\n",
        "    network: Any\n",
        "    config: Any = nonpytree_field()\n",
        "\n",
        "    def critic_loss(self, batch, grad_params, rng):\n",
        "        \"\"\"Compute the SAC critic loss.\"\"\"\n",
        "        next_dist = self.network.select('actor')(batch['next_observations'])\n",
        "        next_actions, next_log_probs = next_dist.sample_and_log_prob(seed=rng)\n",
        "\n",
        "        next_qs = self.network.select('target_critic')(batch['next_observations'], actions=next_actions)\n",
        "        if self.config['min_q']:\n",
        "            next_q = jnp.min(next_qs, axis=0)\n",
        "        else:\n",
        "            next_q = jnp.mean(next_qs, axis=0)\n",
        "\n",
        "        target_q = batch['rewards'] + self.config['discount'] * batch['masks'] * next_q\n",
        "        target_q = target_q - self.config['discount'] * batch['masks'] * next_log_probs * self.network.select('alpha')()\n",
        "\n",
        "        q = self.network.select('critic')(batch['observations'], actions=batch['actions'], params=grad_params)\n",
        "        critic_loss = jnp.square(q - target_q).mean()\n",
        "\n",
        "        return critic_loss, {\n",
        "            'critic_loss': critic_loss,\n",
        "            'q_mean': q.mean(),\n",
        "            'q_max': q.max(),\n",
        "            'q_min': q.min(),\n",
        "        }\n",
        "\n",
        "    def actor_loss(self, batch, grad_params, rng):\n",
        "        \"\"\"Compute the SAC actor loss.\"\"\"\n",
        "        # Actor loss.\n",
        "        dist = self.network.select('actor')(batch['observations'], params=grad_params)\n",
        "        actions, log_probs = dist.sample_and_log_prob(seed=rng)\n",
        "\n",
        "        qs = self.network.select('critic')(batch['observations'], actions=actions)\n",
        "        if self.config['min_q']:\n",
        "            q = jnp.min(qs, axis=0)\n",
        "        else:\n",
        "            q = jnp.mean(qs, axis=0)\n",
        "\n",
        "        actor_loss = (log_probs * self.network.select('alpha')() - q).mean()\n",
        "\n",
        "        # Entropy loss.\n",
        "        alpha = self.network.select('alpha')(params=grad_params)\n",
        "        entropy = -jax.lax.stop_gradient(log_probs).mean()\n",
        "        alpha_loss = (alpha * (entropy - self.config['target_entropy'])).mean()\n",
        "\n",
        "        total_loss = actor_loss + alpha_loss\n",
        "\n",
        "        if self.config['tanh_squash']:\n",
        "            action_std = dist._distribution.stddev()\n",
        "        else:\n",
        "            action_std = dist.stddev().mean()\n",
        "\n",
        "        return total_loss, {\n",
        "            'total_loss': total_loss,\n",
        "            'actor_loss': actor_loss,\n",
        "            'alpha_loss': alpha_loss,\n",
        "            'alpha': alpha,\n",
        "            'entropy': -log_probs.mean(),\n",
        "            'std': action_std.mean(),\n",
        "        }\n",
        "\n",
        "    @jax.jit\n",
        "    def total_loss(self, batch, grad_params, rng=None):\n",
        "        \"\"\"Compute the total loss.\"\"\"\n",
        "        info = {}\n",
        "        rng = rng if rng is not None else self.rng\n",
        "\n",
        "        rng, actor_rng, critic_rng = jax.random.split(rng, 3)\n",
        "\n",
        "        critic_loss, critic_info = self.critic_loss(batch, grad_params, critic_rng)\n",
        "        for k, v in critic_info.items():\n",
        "            info[f'critic/{k}'] = v\n",
        "\n",
        "        actor_loss, actor_info = self.actor_loss(batch, grad_params, actor_rng)\n",
        "        for k, v in actor_info.items():\n",
        "            info[f'actor/{k}'] = v\n",
        "\n",
        "        loss = critic_loss + actor_loss\n",
        "        return loss, info\n",
        "\n",
        "    def target_update(self, network, module_name):\n",
        "        \"\"\"Update the target network.\"\"\"\n",
        "        new_target_params = jax.tree_util.tree_map(\n",
        "            lambda p, tp: p * self.config['tau'] + tp * (1 - self.config['tau']),\n",
        "            self.network.params[f'modules_{module_name}'],\n",
        "            self.network.params[f'modules_target_{module_name}'],\n",
        "        )\n",
        "        network.params[f'modules_target_{module_name}'] = new_target_params\n",
        "\n",
        "    @jax.jit\n",
        "    def update(self, batch):\n",
        "        \"\"\"Update the agent and return a new agent with information dictionary.\"\"\"\n",
        "        new_rng, rng = jax.random.split(self.rng)\n",
        "\n",
        "        def loss_fn(grad_params):\n",
        "            return self.total_loss(batch, grad_params, rng=rng)\n",
        "\n",
        "        new_network, info = self.network.apply_loss_fn(loss_fn=loss_fn)\n",
        "        self.target_update(new_network, 'critic')\n",
        "\n",
        "        return self.replace(network=new_network, rng=new_rng), info\n",
        "\n",
        "    @jax.jit\n",
        "    def sample_actions(\n",
        "        self,\n",
        "        observations,\n",
        "        goals=None,\n",
        "        seed=None,\n",
        "        temperature=1.0,\n",
        "    ):\n",
        "        \"\"\"Sample actions from the actor.\"\"\"\n",
        "        dist = self.network.select('actor')(observations, goals, temperature=temperature)\n",
        "        actions = dist.sample(seed=seed)\n",
        "        actions = jnp.clip(actions, -1, 1)\n",
        "        return actions\n",
        "\n",
        "    @classmethod\n",
        "    def create(\n",
        "        cls,\n",
        "        seed,\n",
        "        ex_observations,\n",
        "        ex_actions,\n",
        "        config,\n",
        "    ):\n",
        "        \"\"\"Create a new agent.\n",
        "\n",
        "        Args:\n",
        "            seed: Random seed.\n",
        "            ex_observations: Example batch of observations.\n",
        "            ex_actions: Example batch of actions.\n",
        "            config: Configuration dictionary.\n",
        "        \"\"\"\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, init_rng = jax.random.split(rng, 2)\n",
        "\n",
        "        action_dim = ex_actions.shape[-1]\n",
        "\n",
        "        if config['target_entropy'] is None:\n",
        "            config['target_entropy'] = -config['target_entropy_multiplier'] * action_dim\n",
        "\n",
        "        # Define critic and actor networks.\n",
        "        critic_def = GCValue(\n",
        "            hidden_dims=config['value_hidden_dims'],\n",
        "            layer_norm=config['layer_norm'],\n",
        "            ensemble=True,\n",
        "        )\n",
        "\n",
        "        actor_def = GCActor(\n",
        "            hidden_dims=config['actor_hidden_dims'],\n",
        "            action_dim=action_dim,\n",
        "            log_std_min=-5,\n",
        "            tanh_squash=config['tanh_squash'],\n",
        "            state_dependent_std=config['state_dependent_std'],\n",
        "            const_std=False,\n",
        "            final_fc_init_scale=config['actor_fc_scale'],\n",
        "        )\n",
        "\n",
        "        # Define the dual alpha variable.\n",
        "        alpha_def = LogParam()\n",
        "\n",
        "        network_info = dict(\n",
        "            critic=(critic_def, (ex_observations, None, ex_actions)),\n",
        "            target_critic=(copy.deepcopy(critic_def), (ex_observations, None, ex_actions)),\n",
        "            actor=(actor_def, (ex_observations, None)),\n",
        "            alpha=(alpha_def, ()),\n",
        "        )\n",
        "        networks = {k: v[0] for k, v in network_info.items()}\n",
        "        network_args = {k: v[1] for k, v in network_info.items()}\n",
        "\n",
        "        network_def = ModuleDict(networks)\n",
        "        network_tx = optax.adam(learning_rate=config['lr'])\n",
        "        network_params = network_def.init(init_rng, **network_args)['params']\n",
        "        network = TrainState.create(model_def = network_def, params = network_params, tx=network_tx)\n",
        "\n",
        "        params = network.params\n",
        "        params['modules_target_critic'] = params['modules_critic']\n",
        "\n",
        "        return cls(rng, network=network, config=flax.core.FrozenDict(**config))\n",
        "\n",
        "\n",
        "def get_config():\n",
        "    config = ml_collections.ConfigDict(\n",
        "        dict(\n",
        "            agent_name='sac',  # Agent name.\n",
        "            lr=1e-4,  # Learning rate.\n",
        "            batch_size=256,  # Batch size.\n",
        "            actor_hidden_dims=(256, 256),  # Actor network hidden dimensions.\n",
        "            value_hidden_dims=(256, 256),  # Value network hidden dimensions.\n",
        "            layer_norm=False,  # Whether to use layer normalization.\n",
        "            discount=0.99,  # Discount factor.\n",
        "            tau=0.005,  # Target network update rate.\n",
        "            target_entropy=ml_collections.config_dict.placeholder(float),  # Target entropy (None for automatic tuning).\n",
        "            target_entropy_multiplier=0.5,  # Multiplier to dim(A) for target entropy.\n",
        "            tanh_squash=True,  # Whether to squash actions with tanh.\n",
        "            state_dependent_std=True,  # Whether to use state-dependent standard deviations for actor.\n",
        "            actor_fc_scale=0.01,  # Final layer initialization scale for actor.\n",
        "            min_q=True,  # Whether to use min Q (True) or mean Q (False).\n",
        "        )\n",
        "    )\n",
        "    return config"
      ],
      "metadata": {
        "id": "MqC7faJP93QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN0bkGgnWFFW"
      },
      "source": [
        "## Integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-dk74R8YRFy3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "e6f28bb9-f575-4530-e20f-256d17672296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NUM_ENVS': 4, 'WD': 0.1, 'NUM_STEPS': 16, 'SAC_D_STEPS': 4, 'ENV_NAME': 'Umaze', 'SAC_STEP_SIZE': 1.0, 'SEED': 423, 'TOTAL_TIMESTEPS': 2048, 'NUM_HIDDEN': 128, 'TX': 'adamw', 'DEPTH': 1, 'LR': 0.0002, 'OPT_STEPS': 8, 'MODEL_NAME': 'DeepBayesianExplorer', 'DEBUG': False}\n",
            "low -inf\n",
            "high inf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'model' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1027793278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1656322416.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0mtrain_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1656322416.py\u001b[0m in \u001b[0;36mmake_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# return {\"runner_state\": runner_state, \"collect_data\": collect_data, \"max_mi_history\": max_mi_history}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'model' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}\n",
        "\n",
        "env_name = 'Umaze'  # @param [\"Umaze\"] {\"type\":\"raw\"}\n",
        "NUM_ENVS = 4 # @param [1,2,4,8,16,32] {\"type\":\"raw\"}\n",
        "TOTAL_TIMESTEPS = 2048 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 16 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"DeepBayesianExplorer\"  #@param [\"DeepBayesianExplorer\",\"RandomExplorer\",\"PPOExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}\n",
        "\n",
        "\n",
        "out = experiment(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNuufgVpCf0z",
        "outputId": "6907b46c-8743-4d45-d7fe-7585eee39768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition(obs=(64, 4, 8, 4), action=(64, 4, 8, 2), reward=(64, 4, 8), next_obs=(64, 4, 8, 4), done=(64, 4, 8), info={'discount': (64, 4, 8), 'goal_position': (64, 4, 8, 2), 'is_success': (64, 4, 8), 'last_state': EnvState(time=(64, 4), position=(64, 4, 2), velocity=(64, 4, 2), desired_goal=(64, 4, 2)), 'log_prob': (64, 4, 8), 'reward': (64, 4, 8), 'value': (64, 4, 8)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out[\"transitions\"].action)"
      ],
      "metadata": {
        "id": "O6eoOanoJmNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from typing import Any, Dict\n",
        "\n",
        "\n",
        "def apply_her_with_trajectory(batch: Dict[str, jnp.ndarray], her_ratio=0.8, rng=None) -> Dict[str, jnp.ndarray]:\n",
        "\n",
        "    batch_size, num_envs, seq_len = batch['rewards'].shape\n",
        "    goal_dim = batch['goals'].shape[-1]\n",
        "\n",
        "    if rng is None:\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "    rng, mask_rng = jax.random.split(rng)\n",
        "    her_mask = jax.random.uniform(mask_rng, (batch_size, num_envs, seq_len)) < her_ratio\n",
        "\n",
        "    max_offset = jnp.arange(seq_len - 1, -1, -1)\n",
        "\n",
        "    rng, offset_rng = jax.random.split(rng)\n",
        "    rand = jax.random.uniform(offset_rng, (batch_size, num_envs, seq_len))\n",
        "\n",
        "    offset = jnp.where(max_offset > 0, (rand * max_offset).astype(jnp.int32) + 1, 0)\n",
        "\n",
        "    batch_idx = jnp.arange(batch_size)[:, None, None]\n",
        "    env_idx = jnp.arange(num_envs)[None, :, None]\n",
        "    time_idx = jnp.arange(seq_len)[None, None, :]\n",
        "\n",
        "    future_time_idx = jnp.minimum(time_idx + offset, seq_len - 1)\n",
        "\n",
        "    future_idx = (\n",
        "        batch_idx,\n",
        "        env_idx,\n",
        "        future_time_idx\n",
        "    )\n",
        "\n",
        "    new_goals = batch['next_observations'][future_idx][..., :goal_dim]\n",
        "\n",
        "    goals = jnp.where(her_mask[..., None], new_goals, batch['goals'])\n",
        "\n",
        "    observations = batch['observations'].copy()\n",
        "    obs_content = observations[..., :-goal_dim]\n",
        "    obs_goals = observations[..., -goal_dim:]\n",
        "    new_obs_goals = jnp.where(her_mask[..., None], new_goals, obs_goals)\n",
        "    observations = jnp.concatenate([obs_content, new_obs_goals], axis=-1)\n",
        "\n",
        "    next_observations = batch['next_observations'].copy()\n",
        "    next_obs_content = next_observations[..., :-goal_dim]\n",
        "    next_obs_goals = next_observations[..., -goal_dim:]\n",
        "    new_next_obs_goals = jnp.where(her_mask[..., None], new_goals, next_obs_goals)\n",
        "    next_observations = jnp.concatenate([next_obs_content, new_next_obs_goals], axis=-1)\n",
        "\n",
        "\n",
        "    achieved_goal = next_observations[..., :goal_dim]\n",
        "    dist_to_goal = jnp.linalg.norm(achieved_goal - goals, axis=-1)\n",
        "    new_rewards = (dist_to_goal < 0.05).astype(jnp.float32)\n",
        "\n",
        "\n",
        "    rewards = jnp.where(her_mask, new_rewards, batch['rewards'])\n",
        "\n",
        "    return {\n",
        "        **batch,\n",
        "        'goals': goals,\n",
        "        'observations': observations,\n",
        "        'next_observations': next_observations,\n",
        "        'rewards': rewards\n",
        "    }\n",
        "\n",
        "def prepare_dataset_for_sac(transition: Any, use_her=True, her_ratio=0.8, rng=None) -> Dict[str, jnp.ndarray]:\n",
        "\n",
        "    batch_size, num_envs, seq_len = transition.obs.shape[:3]\n",
        "    obs_dim = transition.obs.shape[-1]\n",
        "\n",
        "    observations = transition.obs.reshape(batch_size, num_envs, seq_len, obs_dim)\n",
        "    next_observations = transition.next_obs.reshape(batch_size, num_envs, seq_len, obs_dim)\n",
        "    actions = transition.action.reshape(batch_size, num_envs, seq_len, 1)\n",
        "    rewards = transition.reward.reshape(batch_size, num_envs, seq_len)\n",
        "    done = transition.done.reshape(batch_size, num_envs, seq_len)\n",
        "\n",
        "\n",
        "    if 'goal_position' in transition.info:\n",
        "        goal_dim = transition.info['goal_position'].shape[-1]\n",
        "        goals = transition.info['goal_position'].reshape(batch_size, num_envs, seq_len, goal_dim)\n",
        "    else:\n",
        "        goal_dim = 0\n",
        "        goals = None\n",
        "\n",
        "    batch = {\n",
        "        'observations': jnp.concatenate([observations, goals], axis=-1) if goals is not None else observations,\n",
        "        'actions': actions,\n",
        "        'rewards': rewards,\n",
        "        'next_observations': jnp.concatenate([next_observations, goals], axis=-1) if goals is not None else next_observations,\n",
        "        'dones': done,\n",
        "        'masks': 1.0 - done\n",
        "    }\n",
        "\n",
        "    if goals is not None:\n",
        "        batch['goals'] = goals\n",
        "\n",
        "    if use_her and goals is not None:\n",
        "        print(\"using her\")\n",
        "        batch = apply_her_with_trajectory(batch, her_ratio, rng)\n",
        "\n",
        "    flat_batch = {\n",
        "        'observations': batch['observations'].reshape(-1, batch['observations'].shape[-1]),\n",
        "        'actions': batch['actions'].reshape(-1, 1),\n",
        "        'rewards': batch['rewards'].reshape(-1),\n",
        "        'next_observations': batch['next_observations'].reshape(-1, batch['next_observations'].shape[-1]),\n",
        "        'masks': batch['masks'].reshape(-1)\n",
        "    }\n",
        "\n",
        "    if goals is not None:\n",
        "        flat_batch['goals'] = batch['goals'].reshape(-1, goal_dim)\n",
        "\n",
        "    return flat_batch"
      ],
      "metadata": {
        "id": "gsOtlOyeFDHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = prepare_dataset_for_sac(out[\"transitions\"])\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, buffer))"
      ],
      "metadata": {
        "id": "endGNWcrG_su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tqdm\n",
        "from typing import Dict, Any, Optional\n",
        "import wandb\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class SACConfig:\n",
        "    # 训练参数\n",
        "    seed: int = 42\n",
        "    num_epochs: int = 100\n",
        "    batch_size: int = 256\n",
        "    log_freq: int = 1\n",
        "    eval_freq: int = 10\n",
        "    eval_episodes: int = 1\n",
        "\n",
        "    # wandb参数\n",
        "    run_group: str = \"offline_sac\"\n",
        "    project: str = \"OfflineSAC\"\n",
        "\n",
        "    # 评估参数\n",
        "    eval_temperature: float = 1.0\n",
        "    eval_on_cpu: bool = False\n",
        "\n",
        "    # SAC算法参数\n",
        "    lr: float = 1e-4\n",
        "    actor_hidden_dims: tuple = (256, 256)\n",
        "    value_hidden_dims: tuple = (256, 256)\n",
        "    layer_norm: bool = False\n",
        "    discount: float = 0.99\n",
        "    tau: float = 0.005\n",
        "    target_entropy: Optional[float] = None\n",
        "    target_entropy_multiplier: float = 0.5\n",
        "    tanh_squash: bool = True\n",
        "    state_dependent_std: bool = True\n",
        "    actor_fc_scale: float = 0.01\n",
        "    min_q: bool = True\n",
        "    her_ratio: float = 0.8\n",
        "\n",
        "def get_exp_name(seed):\n",
        "    \"\"\"Generate experiment name.\"\"\"\n",
        "    return f\"offline_sac_seed_{seed}\"\n",
        "\n",
        "def setup_wandb(project, group, name):\n",
        "    \"\"\"Setup wandb logging.\"\"\"\n",
        "    wandb.init(project=project, group=group, name=name)\n",
        "\n",
        "def sample_batch_from_data(data, batch_size, rng):\n",
        "    \"\"\"Sample batch from offline data.\"\"\"\n",
        "    data_size = data['observations'].shape[0]\n",
        "    indices = jax.random.randint(rng, (batch_size,), 0, data_size)\n",
        "\n",
        "    batch = {}\n",
        "    for key in data.keys():\n",
        "        if data[key] is not None:\n",
        "            batch[key] = jnp.array(data[key][indices])\n",
        "\n",
        "    return batch\n",
        "\n",
        "def evaluate_agent(agent, env, env_params, key, config, num_eval_episodes=10, eval_temperature=1.0, max_steps=10):\n",
        "    \"\"\"Evaluate agent performance.\"\"\"\n",
        "    eval_rewards = []\n",
        "    eval_lengths = []\n",
        "\n",
        "\n",
        "    key, eval_key = jax.random.split(key)\n",
        "\n",
        "\n",
        "    for _ in range(num_eval_episodes):\n",
        "\n",
        "        eval_key, reset_key, sample_key, step_key = jax.random.split(eval_key, 4)\n",
        "\n",
        "        observations, state = env.pm_env.reset_env(reset_key, env_params)\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        done = False\n",
        "\n",
        "        while not done and episode_length < max_steps:\n",
        "            actions = agent.sample_actions(\n",
        "                observations=observations,\n",
        "                goals=None,\n",
        "                seed=sample_key,\n",
        "                temperature=eval_temperature\n",
        "            )\n",
        "\n",
        "            observations, state, rewards, dones, _ = env.pm_env.step_env(step_key, state, actions, env_params)\n",
        "            episode_reward += rewards.sum()\n",
        "            episode_length += 1\n",
        "            done = dones.all()\n",
        "\n",
        "        eval_rewards.append(episode_reward)\n",
        "        eval_lengths.append(episode_length)\n",
        "\n",
        "    return {\n",
        "        'reward_mean': np.mean(eval_rewards),\n",
        "        'reward_std': np.std(eval_rewards),\n",
        "        'length_mean': np.mean(eval_lengths),\n",
        "        'length_std': np.std(eval_lengths),\n",
        "    }\n",
        "\n",
        "def offline_training_with_wandb(\n",
        "    agent: SACAgent,\n",
        "    replay_buffer: Dict[str, np.ndarray],\n",
        "    config: SACConfig,\n",
        "    eval_env_params,\n",
        "    key,\n",
        "    eval_env=None,\n",
        "):\n",
        "\n",
        "    buffer_size = replay_buffer['observations'].shape[0]\n",
        "    steps_per_epoch = buffer_size // config.batch_size\n",
        "\n",
        "    print(f\"Buffer size: {buffer_size}\")\n",
        "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"Total training steps: {config.num_epochs * steps_per_epoch}\")\n",
        "\n",
        "\n",
        "    # wandb.config.update({\n",
        "    #     'buffer_size': buffer_size,\n",
        "    #     'steps_per_epoch': steps_per_epoch,\n",
        "    #     'total_steps': config.num_epochs * steps_per_epoch,\n",
        "    #     'config': config.__dict__\n",
        "    # })\n",
        "\n",
        "    start_time = time.time()\n",
        "    key, train_key, eval_key = jax.random.split(key, 3)\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(config.num_epochs), smoothing=0.1, dynamic_ncols=True):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_losses = []\n",
        "\n",
        "        for step in range(steps_per_epoch):\n",
        "            batch = sample_batch_from_data(\n",
        "                replay_buffer,\n",
        "                config.batch_size,\n",
        "                agent.rng\n",
        "            )\n",
        "\n",
        "            batch = apply_her(\n",
        "                batch,\n",
        "                her_ratio=config.her_ratio,\n",
        "                rng = agent.rng\n",
        "            )\n",
        "\n",
        "            agent, info = agent.update(batch)\n",
        "            epoch_losses.append(info)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        if epoch_losses:\n",
        "            avg_losses = {}\n",
        "            for key in epoch_losses[0].keys():\n",
        "                values = [loss[key] for loss in epoch_losses if key in loss]\n",
        "                if values:\n",
        "                    avg_losses[key] = np.mean(values)\n",
        "\n",
        "        if epoch % config.log_freq == 0:\n",
        "\n",
        "            train_metrics = {\n",
        "                'epoch': epoch,\n",
        "                'time/epoch_time': epoch_time,\n",
        "                'time/total_time': time.time() - start_time,\n",
        "                'time/avg_step_time': epoch_time / steps_per_epoch,\n",
        "            }\n",
        "\n",
        "\n",
        "            if avg_losses:\n",
        "                for key, value in avg_losses.items():\n",
        "                    train_metrics[f'training/{key}'] = value\n",
        "\n",
        "            train_metrics['training/learning_rate'] = config.lr\n",
        "\n",
        "\n",
        "            # wandb.log(train_metrics, step=epoch)\n",
        "\n",
        "\n",
        "            print(f\"Epoch {epoch}/{config.num_epochs}: \"\n",
        "                  f\"Time: {epoch_time:.2f}s, \"\n",
        "                  f\"Critic Loss: {avg_losses.get('critic/critic_loss', 0):.4f}, \"\n",
        "                  f\"Actor Loss: {avg_losses.get('actor/actor_loss', 0):.4f}, \"\n",
        "                  f\"Alpha: {avg_losses.get('actor/alpha', 0):.4f}\")\n",
        "\n",
        "        if epoch % config.eval_freq == 0 and eval_env is not None:\n",
        "            if config.eval_on_cpu:\n",
        "                eval_agent = jax.device_put(agent, device=jax.devices('cpu')[0])\n",
        "            else:\n",
        "                eval_agent = agent\n",
        "\n",
        "            eval_info = evaluate_agent(\n",
        "                agent=eval_agent,\n",
        "                env=eval_env,\n",
        "                config=config,\n",
        "                num_eval_episodes=config.eval_episodes,\n",
        "                eval_temperature=config.eval_temperature,\n",
        "                env_params=eval_env_params,\n",
        "                key=eval_key\n",
        "            )\n",
        "\n",
        "            eval_metrics = {f'evaluation/{k}': v for k, v in eval_info.items()}\n",
        "            eval_metrics['evaluation/epoch'] = epoch\n",
        "\n",
        "            # wandb.log(eval_metrics, step=epoch)\n",
        "\n",
        "            print(f\"Evaluation at epoch {epoch}: \"\n",
        "                  f\"Reward = {eval_info['reward_mean']:.2f} ± {eval_info['reward_std']:.2f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    return agent\n",
        "\n",
        "def main():\n",
        "    config = SACConfig(\n",
        "        seed=42,\n",
        "        num_epochs=100,\n",
        "        batch_size=256,\n",
        "        project=\"YourProject\",\n",
        "        run_group=\"experiment_group\"\n",
        "    )\n",
        "\n",
        "    # Set up wandb\n",
        "    exp_name = get_exp_name(config.seed)\n",
        "    # setup_wandb(config.project, config.run_group, exp_name)\n",
        "\n",
        "\n",
        "    eval_env = None\n",
        "    if config.eval_freq > 0:\n",
        "        eval_env = PMwrapper(pointax.make_umaze(reward_type=\"sparse\"))\n",
        "        eval_env_params = eval_env.default_params()\n",
        "\n",
        "\n",
        "    replay_buffer = buffer\n",
        "\n",
        "    example_transition = dict(\n",
        "        observations=replay_buffer['observations'][0],\n",
        "        actions=replay_buffer['actions'][0],\n",
        "        rewards=replay_buffer['rewards'][0],\n",
        "        masks=replay_buffer['masks'][0],\n",
        "        next_observations=replay_buffer['next_observations'][0],\n",
        "    )\n",
        "    if replay_buffer['goals'] is not None:\n",
        "        example_transition['goals'] = replay_buffer['goals'][0]\n",
        "\n",
        "\n",
        "    sac_config = {\n",
        "        'agent_name': 'sac',\n",
        "        'lr': config.lr,\n",
        "        'batch_size': config.batch_size,\n",
        "        'actor_hidden_dims': config.actor_hidden_dims,\n",
        "        'value_hidden_dims': config.value_hidden_dims,\n",
        "        'layer_norm': config.layer_norm,\n",
        "        'discount': config.discount,\n",
        "        'tau': config.tau,\n",
        "        'target_entropy': config.target_entropy,\n",
        "        'target_entropy_multiplier': config.target_entropy_multiplier,\n",
        "        'tanh_squash': config.tanh_squash,\n",
        "        'state_dependent_std': config.state_dependent_std,\n",
        "        'actor_fc_scale': config.actor_fc_scale,\n",
        "        'min_q': config.min_q,\n",
        "        'her_ratio': config.her_ratio\n",
        "    }\n",
        "\n",
        "\n",
        "    agent = SACAgent.create(\n",
        "        config.seed,\n",
        "        example_transition['observations'],\n",
        "        example_transition['actions'],\n",
        "        sac_config,\n",
        "    )\n",
        "\n",
        "\n",
        "    trained_agent = offline_training_with_wandb(\n",
        "        agent=agent,\n",
        "        replay_buffer=replay_buffer,\n",
        "        config=config,\n",
        "        eval_env=eval_env,\n",
        "        eval_env_params=eval_env_params,\n",
        "        key=jax.random.PRNGKey(config.seed)\n",
        "    )\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KcAUi8PhcwXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCelkyGes0Cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}