{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJjXifWFe9K"
      },
      "outputs": [],
      "source": [
        "%pip install jax\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install xminigrid\n",
        "%pip install gymnax\n",
        "%pip install distrax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gdi3TeiF481"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as jtu\n",
        "import numpy as np\n",
        "import distrax\n",
        "\n",
        "import timeit\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "from flax import nnx\n",
        "import xminigrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L7duDLiJ9Wa"
      },
      "outputs": [],
      "source": [
        "# class TimeStep(struct.PyTreeNode):\n",
        "#     # hidden environment state, such as grid, agent, goal, etc\n",
        "#     state: State\n",
        "\n",
        "#     # similar to the dm_env enterface\n",
        "#     step_type: StepType\n",
        "#     reward: jax.Array\n",
        "#     discount: jax.Array\n",
        "#     observation: jax.Array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSjqwPwiXxCf"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYB7Sa8iEGod"
      },
      "source": [
        "### Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJHHxdaGEJan"
      },
      "outputs": [],
      "source": [
        "from gymnax.environments.environment import Environment\n",
        "import abc\n",
        "from typing import Any, Generic, Optional, TypeVar\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "import xminigrid\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState:\n",
        "    time: int\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams:\n",
        "    max_steps_in_episode: int = 1\n",
        "\n",
        "class XMiniGridGymnaxWrapper(Environment):\n",
        "    \"\"\"\n",
        "    将 xminigrid 环境包装为 gymnax 风格环境\n",
        "    \"\"\"\n",
        "    def __init__(self, xminigrid_env):\n",
        "        super().__init__()\n",
        "        self.xminigrid_env = xminigrid_env\n",
        "\n",
        "    @property\n",
        "    def default_params(self):\n",
        "        # return EnvParams()\n",
        "        return self.xminigrid_env.default_params()\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        timestep = self.xminigrid_env.step(params, state, action)\n",
        "        obs = timestep.observation\n",
        "        reward = timestep.reward\n",
        "        done = (timestep.step_type == 2)  # StepType.LAST == 2\n",
        "        info = {\"discount\": timestep.discount}\n",
        "        state = timestep\n",
        "        return obs, state, reward, done, info\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        timestep = self.xminigrid_env.reset(params, key)\n",
        "        obs = timestep.observation\n",
        "        return obs, timestep\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        return state.observation\n",
        "\n",
        "    def is_terminal(self, state, params):\n",
        "        return state.step_type == 2\n",
        "\n",
        "    # useless here\n",
        "    @property\n",
        "    def name(self):\n",
        "      return \"xminigrid\"\n",
        "\n",
        "    @property\n",
        "    # potential issue here\n",
        "    #implementation of xland:\n",
        "      #def num_actions(self, params: EnvParamsT) -> int:\n",
        "      # return int(NUM_ACTIONS)\n",
        "    def num_actions(self):\n",
        "        return 6\n",
        "\n",
        "    def action_space(self, params):\n",
        "        return spaces.Discrete(6)\n",
        "\n",
        "    def observation_shape(self, params):\n",
        "        return self.xminigrid_env.observation_shape(params)\n",
        "\n",
        "    def observation_space(self, params):\n",
        "        shape = self.observation_shape(params)\n",
        "        return spaces.Box(low=0, high=255, shape=shape, dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYSuZTE1Y4NA"
      },
      "source": [
        "### Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZlsjAvVXzLc"
      },
      "outputs": [],
      "source": [
        "import jax.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.linear(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class ActionEncoder(nnx.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.embed = nnx.Embed(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    h = self.embed(x)\n",
        "    return self.layer_norm0(h)\n",
        "\n",
        "class JointEncoder(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.layer_norm0 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "    self.layer_norm3 = nnx.LayerNorm(hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array, rng):\n",
        "    dist_distrax = distrax.MultivariateNormalDiag(loc=x, scale_diag=1e-1*jnp.ones_like(x))\n",
        "    # potential shape issue\n",
        "    x = dist_distrax.sample(seed=rng, sample_shape=(1,))\n",
        "    x = self.layer_norm0(x)\n",
        "    h0 = self.linear1(x)\n",
        "    h = nn.relu(h0)\n",
        "    h = self.layer_norm1(h) + h0\n",
        "    h0 = self.linear2(h)\n",
        "    h = self.layer_norm2(h) + h0\n",
        "    return self.layer_norm3(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mszBnNX2Ue5p"
      },
      "source": [
        "### Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNvIDPb1Uhpz"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax import nnx\n",
        "\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from flax.training import train_state\n",
        "from jax.scipy.special import gamma,digamma, gammaln, kl_div\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "@nnx.jit\n",
        "def compute_info_gain_dirichlet(alpha,next_obs):\n",
        "    \"\"\"\n",
        "    计算互信息矩阵\n",
        "    alpha: (batch, num_states)\n",
        "    next_obs: (batch)  index\n",
        "    输出形状: (batch)\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = jnp.maximum(alpha, 1e-6)\n",
        "    sum_alpha = jnp.sum(alpha, axis=-1,keepdims=False)\n",
        "\n",
        "    next_obs = jnp.expand_dims(next_obs, -1).astype(\"int\")\n",
        "    post_alpha = jnp.take_along_axis(alpha,next_obs,-1)\n",
        "    post_alpha = post_alpha.squeeze(-1)\n",
        "\n",
        "    probs = post_alpha / sum_alpha\n",
        "    log_probs = jnp.log(probs)\n",
        "\n",
        "    # ( batch)\n",
        "    entropy = - log_probs\n",
        "\n",
        "    # ( batch)\n",
        "    posterior_digamma = digamma(post_alpha+1)\n",
        "\n",
        "    # ( batch)\n",
        "    sum_digamma = digamma(sum_alpha+1)\n",
        "\n",
        "    # ( batch)\n",
        "    negative_posterior_entropy = posterior_digamma - sum_digamma\n",
        "\n",
        "    # (batch)\n",
        "    posterior_kl = entropy + negative_posterior_entropy\n",
        "    return posterior_kl\n",
        "\n",
        "@nnx.jit\n",
        "def compute_mi_dirichlet(alpha):\n",
        "    \"\"\"\n",
        "    计算互信息矩阵\n",
        "    输入形状: (num_actions, num_states)\n",
        "    输出形状: (num_actions)\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = jnp.maximum(alpha, 1e-6)\n",
        "    num_states = alpha.shape[-1]\n",
        "    sum_alpha = jnp.sum(alpha, axis=-1,keepdims=True)\n",
        "\n",
        "    probs = alpha / sum_alpha\n",
        "\n",
        "    log_probs = jnp.log(probs)\n",
        "\n",
        "    # ( num_actions)\n",
        "    entropy = - jnp.sum(probs * log_probs,axis=-1)\n",
        "\n",
        "    # ( num_actions, num_states)\n",
        "    posterior_digamma = digamma(alpha+1)\n",
        "\n",
        "    # ( num_actions)\n",
        "    sum_digamma = digamma(sum_alpha+1).squeeze(-1)\n",
        "\n",
        "    # ( num_actions)\n",
        "    negative_posterior_entropy = (probs * posterior_digamma).sum(axis=-1) - sum_digamma\n",
        "\n",
        "    # (num_actions)\n",
        "    mi_matrix = entropy + negative_posterior_entropy\n",
        "    return mi_matrix\n",
        "\n",
        "@nnx.jit\n",
        "def optimal_action_and_MI_from_alpha(alphas,rng):\n",
        "\n",
        "    # 计算互信息矩阵\n",
        "    # 4x8x8x2\n",
        "    mi_matrix = compute_mi_dirichlet(alphas)  # ( num_actions)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    random_perturb = 1e-4*jax.random.normal(_rng,mi_matrix.shape)\n",
        "    mi_matrix = mi_matrix + random_perturb\n",
        "\n",
        "    # sum over\n",
        "    mi_matrix_sum = mi_matrix.sum(axis=(1,2,3))\n",
        "\n",
        "    optimal_actions = jnp.argmax(mi_matrix_sum, axis=-1)\n",
        "    return optimal_actions, mi_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util"
      ],
      "metadata": {
        "id": "2oVvO_lUWW6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_random_split(batch_key,num=2):\n",
        "    split_keys = jax.vmap(jax.random.split,in_axes=(0,None))(batch_key,num)\n",
        "    return [split_keys[:, i]  for i in range(num) ]"
      ],
      "metadata": {
        "id": "YsnNLTPoWYD6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck6iG5qLTJTE"
      },
      "source": [
        "## Unsupervised Explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XYxtYAbZTLyM"
      },
      "outputs": [],
      "source": [
        "from gymnax.experimental import RolloutWrapper\n",
        "# action = self.model_forward(policy_params, obs, rng_net)\n",
        "import functools\n",
        "import gymnax\n",
        "from typing import Union,Optional,Any\n",
        "import abc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UI0b0A4UmqOK"
      },
      "outputs": [],
      "source": [
        "class CustomRolloutWrapper:\n",
        "    \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_or_name: Union[str,Environment] = \"Pendulum-v1\",\n",
        "        num_env_steps: Optional[int] = None,\n",
        "        env_kwargs: Any | None = None,\n",
        "        env_params: Any | None = None,\n",
        "    ):\n",
        "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
        "        # Define the RL environment & network forward function\n",
        "        if env_kwargs is None:\n",
        "            env_kwargs = {}\n",
        "        if env_params is None:\n",
        "            env_params = {}\n",
        "        if isinstance(env_or_name,Environment):\n",
        "            self.env = env_or_name\n",
        "            self.env_params = env_or_name.default_params\n",
        "        else:\n",
        "            self.env, self.env_params = xminigrid.make(env_or_name, view_size=8)\n",
        "            self.env = XMiniGridGymnaxWrapper(self.env)\n",
        "\n",
        "        self.env_params = self.env_params.replace(**env_params)\n",
        "\n",
        "        if num_env_steps is None:\n",
        "            self.num_env_steps = self.env_params.max_steps\n",
        "        else:\n",
        "            self.num_env_steps = num_env_steps\n",
        "\n",
        "    def batch_reset(self, rng_input):\n",
        "        batch_rest = jax.vmap(self.single_reset_state)\n",
        "        return batch_rest(rng_input)\n",
        "\n",
        "    # state vs. timestep, potential issue here\n",
        "    def single_reset_state(self, rng_input):\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "        obs, timestep = self.env.reset(rng_reset, self.env_params)\n",
        "        return timestep\n",
        "\n",
        "    # @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_rollout(self, rng_eval, model:UnsupervisedExplorer, env_state=None):\n",
        "        \"\"\"Evaluate a generation of networks on RL/Supervised/etc. task.\"\"\"\n",
        "        # vmap over different MC fitness evaluations for single network\n",
        "        # potential issue here\n",
        "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0, None,0))\n",
        "        return batch_rollout(rng_eval, model, env_state)\n",
        "\n",
        "    # @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def single_rollout(self, rng_input, model:UnsupervisedExplorer, env_state=None):\n",
        "        \"\"\"Rollout a pendulum episode with lax.scan.\"\"\"\n",
        "        # Reset the environment\n",
        "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
        "\n",
        "        if env_state is None:\n",
        "            obs, env_state = self.env.reset(rng_reset, self.env_params)\n",
        "        else:\n",
        "            obs = self.env.get_obs(env_state)\n",
        "\n",
        "        def policy_step(state_input, _):\n",
        "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
        "            obs, state,  rng, cum_reward, valid_mask = state_input\n",
        "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
        "            if model is not None:\n",
        "                temp,info = model( obs, rng_net)\n",
        "                action = self.env.action_space(self.env_params).sample(rng_net)\n",
        "            else:\n",
        "                action = self.env.action_space(self.env_params).sample(rng_net)\n",
        "                info = {}\n",
        "        #    print (\"policy step action\",action.shape)\n",
        "            next_obs, next_state, reward, done, step_info = self.env.step(\n",
        "                rng_step, state, action, self.env_params\n",
        "            )\n",
        "            step_info[\"state\"] = state\n",
        "            step_info[\"next_state\"] = next_state\n",
        "            info.update(step_info)\n",
        "            new_cum_reward = cum_reward + reward * valid_mask\n",
        "            new_valid_mask = valid_mask * (1 - done)\n",
        "            carry = [\n",
        "                next_obs,\n",
        "                next_state,\n",
        "                rng,\n",
        "                new_cum_reward,\n",
        "                new_valid_mask,\n",
        "            ]\n",
        "            y = [obs, action, reward, next_obs, done,info]\n",
        "            return carry, y\n",
        "\n",
        "        # Scan over episode step loop\n",
        "        carry_out, scan_out = jax.lax.scan(\n",
        "            policy_step,\n",
        "            [\n",
        "                obs,\n",
        "                env_state,\n",
        "                rng_episode,\n",
        "                jnp.array([0.0]),\n",
        "                jnp.array([1.0]),\n",
        "            ],\n",
        "            (),\n",
        "            self.num_env_steps,\n",
        "        )\n",
        "        # Return the sum of rewards accumulated by agent in episode rollout\n",
        "        obs, action, reward, next_obs, done, info = scan_out\n",
        "        cum_return = carry_out[-2]\n",
        "        return obs, action, reward, next_obs, done,info, cum_return\n",
        "\n",
        "class UnsupervisedRolloutWrapper(CustomRolloutWrapper):\n",
        "\n",
        "    @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def batch_update(self, rng_update,model, obs, action,next_obs,done,info):\n",
        "        if model is None: return {}\n",
        "        return model.update(obs, action,next_obs,done,info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cN03-aDZUGex"
      },
      "outputs": [],
      "source": [
        "class UnsupervisedExplorer(nnx.Module):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def update(self, obs, actions, next_obs, dones, info):\n",
        "    # update variable parameters\n",
        "    return #{'kl':KL} MI= E[KL]\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def __call__(self, observations, rng):\n",
        "    return #actions, {\"mi\":mi_matrix}\n",
        "\n",
        "class RandomExplorer(UnsupervisedExplorer):\n",
        "\n",
        "  def __init__(self, num_actions):\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def update(self, rng, obs, actions, next_obs, dones, info):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, observations, rng):\n",
        "    if observations.ndim == 1:\n",
        "      # possible shape issue here\n",
        "      actions = jax.random.randint(rng, shape=(1,), minval=0, maxval=self.num_actions)\n",
        "      return actions, {}\n",
        "    actions = jax.random.randint(rng, shape=(observations.shape[0],), minval=0, maxval=self.num_actions)\n",
        "    return actions, {}\n",
        "\n",
        "class BayesianExplorer(UnsupervisedExplorer):\n",
        "\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions\n",
        "        self.num_states = num_states\n",
        "        self.alphas = nnx.Variable(jnp.ones((num_states, num_actions, num_states))/2)\n",
        "\n",
        "    def update(self,obs,action,next_obs,done,info):\n",
        "\n",
        "        prior_alphas = self.alphas[obs, action]\n",
        "        kl=compute_info_gain_dirichlet(prior_alphas,next_obs)\n",
        "        self.alphas.value = self.alphas.value.at[obs, action,next_obs].add(1)\n",
        "        return {\"kl\":kl}\n",
        "        #big =\n",
        "       # return {\"big\":}\n",
        "\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "\n",
        "      #  alpha = jnp.take(self.alphas,observations.astype(jnp.int32),axis=0)\n",
        "        alpha = self.alphas[observations.astype(jnp.int32)]\n",
        "        actions, mi_matrix = optimal_action_and_MI_from_alpha(alpha,rng)\n",
        "        MI = mi_matrix[actions]\n",
        "        return actions, {\"mi\":MI}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3mhEYbCamAQb"
      },
      "outputs": [],
      "source": [
        "class obs_embedder(nnx.Module):\n",
        "  def __init__(self, num_type: int, embed_dim: int, hidden_dim:int, rngs: nnx.Rngs):\n",
        "    self.embed1 = nnx.Embed(num_type, embed_dim, rngs=rngs)\n",
        "    self.embed2 = nnx.Embed(num_type, embed_dim, rngs=rngs)\n",
        "    hidden_input_dim = embed_dim * 8 * 8 * 2\n",
        "    self.linear = nnx.Linear(hidden_input_dim, hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, obs):\n",
        "    t_ids = obs[..., 0]\n",
        "    c_ids = obs[..., 1]\n",
        "\n",
        "    # 8 x 8 x  embed_dim\n",
        "    t_embed = self.embed1(t_ids)\n",
        "    c_embed = self.embed2(c_ids)\n",
        "    print(\"c_embed\",c_embed.shape)\n",
        "\n",
        "    hidden_in =  jnp.reshape(jnp.concatenate([t_embed, c_embed], axis=-1), -1)\n",
        "    print(\"hidden_in\",hidden_in.shape)\n",
        "    print(\"self.linear \",self.linear )\n",
        "    return self.linear(hidden_in)\n",
        "\n",
        "class Joint_MLP(nnx.Module):\n",
        "  def __init__(self, hidden_dim: int, rngs: nnx.Rngs):\n",
        "    self.linear1 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray):\n",
        "    h = self.linear1(x)\n",
        "    h = nn.relu(h)\n",
        "    out = self.linear2(h)\n",
        "    return out\n",
        "\n",
        "class DeepBayesianExplorer(BayesianExplorer):\n",
        "\n",
        "    def __init__(self, num_states, num_actions, num_hidden,embed_dim=4):\n",
        "        super().__init__(num_states, num_actions)\n",
        "        self.num_hidden = num_hidden\n",
        "        obs_dim = num_states\n",
        "        self.obs_embeds = obs_embedder(num_states,embed_dim=embed_dim,hidden_dim=num_hidden, rngs=nnx.Rngs(0))\n",
        "        self.action_embeds = nnx.Embed(num_embeddings=num_actions, features=num_hidden, rngs=nnx.Rngs(0))\n",
        "        self.joint_embeds = Joint_MLP(num_hidden,rngs=nnx.Rngs(0))\n",
        "    #    self.linear = nnx.Linear(self.embed_size,num_hidden,rngs=nnx.Rngs(0))\n",
        "        self.w = nnx.Variable(jnp.zeros((8, 8, 2, num_states, 512)))\n",
        "        self.b = nnx.Variable(jnp.ones((8, 8, 2, num_states)) / 2)\n",
        "\n",
        "    def update(self,obs,action,next_obs,done,info):\n",
        "        #not necessary just here to log the kl if we were classical bayesian\n",
        "      #  kl = super().update(obs,action,next_obs,done,info)[\"kl\"]\n",
        "        alpha = info[\"alpha\"]\n",
        "        # alpha 8 x 8 x 2 x 13\n",
        "        # next_obs 8 x 8 x 2\n",
        "        # 8 x 8 x 2\n",
        "        kl=compute_info_gain_dirichlet(alpha,next_obs).sum(axis=(-1,-2,-3))\n",
        "        print(\"kl\",kl.shape,kl)\n",
        "        # 128(batch_size) * 16 = 4 * 512(2*8*8*2*num_hidden)\n",
        "        #batch x  num_hidden\\\n",
        "        # T bd\n",
        "        # T = info[\"T\"]\n",
        "        T = info[\"T\"].reshape(-1,512)\n",
        "        # self.w hwcsd\n",
        "        # T b(d+1)\n",
        "        ones = jnp.ones_like(T[...,:1])\n",
        "\n",
        "        # T: b(d+1)\n",
        "        T = jnp.concatenate([T,ones],axis=-1)\n",
        "        print(\"T\",T.shape)\n",
        "\n",
        "        #batch x num_states\n",
        "        #so it should acts seperately on each block\n",
        "        y = jax.nn.one_hot(next_obs.astype(jnp.int32),self.num_states)\n",
        "        # #mod here\n",
        "        y_reshaped = y.reshape(y.shape[0] * y.shape[1], *y.shape[2:])\n",
        "        # y: b(hwcs)\n",
        "        #now, we solve for the posterior update\n",
        "        # b hwcs      b hwcs    b(hwcs)\n",
        "        # T @ w_new = T  @ w + y\n",
        "\n",
        "        # (d+1) b\n",
        "        T_T = jnp.transpose(T)\n",
        "        # b x b\n",
        "        covariance = T @ T_T\n",
        "        # b x b\n",
        "        inv_covariance = jnp.linalg.pinv(covariance)\n",
        "\n",
        "\n",
        "        def update_discre(T,w,y):\n",
        "          # (num_hidden*2+1) x num_states\n",
        "          delta =  T_T @ inv_covariance @ y\n",
        "          return delta\n",
        "\n",
        "        print(\"T_T\",T_T.shape)\n",
        "        print(\"inv_covariance\",inv_covariance.shape)\n",
        "        print(\"T_T @ inv_covariance\",(T_T @ inv_covariance).shape)\n",
        "        print(\"y\",y.shape)\n",
        "        print(\"y_reshaped\",y_reshaped.shape)\n",
        "\n",
        "        delta = jnp.einsum(\"db,bhwcs ->hwcsd\", T_T @ inv_covariance,y_reshaped)\n",
        "\n",
        "        # hwcsd\n",
        "        delta_W = delta[...,:-1]\n",
        "        # hwcs\n",
        "        delta_b = delta[...,-1]\n",
        "\n",
        "        print(\"delta_W\",delta_W.shape)\n",
        "        print(\"delta_b\",delta_b.shape)\n",
        "        print(\"self.w\",self.w.shape)\n",
        "        print(\"self.b\",self.b.shape)\n",
        "\n",
        "        self.w.value = self.w.value + delta_W\n",
        "        self.b.value = self.b.value + delta_b\n",
        "\n",
        "        return {\"kl\":kl}\n",
        "\n",
        "  #  @functools.partial(nnx.jit, static_argnums=(0,))\n",
        "    def embed_joint(self,action_embed,obs_embed):\n",
        "        join = lambda x,y :  jnp.concatenate([x,y],axis=-1)\n",
        "        vmapped = jax.vmap(join,in_axes=(0,None))\n",
        "        return vmapped(action_embed,obs_embed)\n",
        "\n",
        "    def __call__(self,observations,rng):\n",
        "        #( num_actions x num_hidden )\n",
        "\n",
        "        # 8x8x2\n",
        "        observations = observations.astype(jnp.int32)\n",
        "\n",
        "      #  print (\"observations\",observations.shape)\n",
        "        # observations: 8 x 8 x 2\n",
        "        # obs_embed: 8 x 8 x 4 use nn.Embedding\n",
        "        # flattent to 256\n",
        "\n",
        "        # 8x8x4\n",
        "        obs_embed = self.obs_embeds(observations)\n",
        "        # 256\n",
        "        obs_embed_flat = obs_embed.reshape(-1)\n",
        "\n",
        "        # 4 x 512\n",
        "        action_embed = self.action_embeds(jnp.arange(self.num_actions))\n",
        "\n",
        "        #num_actions x embed_size    4 x 512\n",
        "        embed = action_embed+jnp.expand_dims(obs_embed_flat,0)\n",
        "\n",
        "        # num_actions x num_hidden\n",
        "        # 4 x 512 using mlp\n",
        "        T = self.joint_embeds(embed)\n",
        "\n",
        "        #  num_actions x num_states\n",
        "       # print (\"self.b \",self.b .shape)\n",
        "\n",
        "        # self.b  8 x 8 x 2 x 13\n",
        "        # alpha  4 x  8 x 8 x 2 x 13\n",
        "        # self.w 8 x 8 x 2 x 13 x 512\n",
        "        # T @ self.w needs reshape\n",
        "\n",
        "\n",
        "        # 1664 x 512\n",
        "      #  w_reshaped = self.w.reshape(-1, 512)\n",
        "        # 4 x 1664\n",
        "        print(\"self.w\",self.w.shape)\n",
        "        print(\"T\",T.shape)\n",
        "        Tw = jnp.einsum(\"hwcsd,bd->bhwcs\", self.w,T)\n",
        "        print(\"Tw\",Tw.shape)\n",
        "        alpha = Tw + self.b\n",
        "        #the current code will give\n",
        "        # MI the shape of 4 x  8 x 8 x 2, you need to sum over 8 x 8 x 2\n",
        "        #then take the argmax over action\n",
        "        actions, mi_matrix = optimal_action_and_MI_from_alpha(alpha,rng)\n",
        "        print(\"action\",actions.shape)\n",
        "        print(\"mi\", mi_matrix.shape)\n",
        "        # 512\n",
        "        T = T[actions]\n",
        "        # 8 x 8 x 2 x 13\n",
        "        alpha = alpha[actions]\n",
        "        # 1\n",
        "        mi_matrix = mi_matrix[actions]\n",
        "\n",
        "        return actions, {\"mi\":mi_matrix,\"T\":T,\"alpha\":alpha}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    alphas = jnp.zeros((16, 4, 16))\n",
        "    # Define rollout manager for pendulum env\n",
        "    manager = UnsupervisedRolloutWrapper(env_or_name=\"MiniGrid-EmptyRandom-8x8\",num_env_steps=2)\n",
        "\n",
        "    obs_shape = (8, 8, 2)\n",
        "    num_states = 13\n",
        "    num_actions = 4\n",
        "    num_hidden = 512\n",
        "\n",
        "    # 创建 explorer 实例\n",
        "    explorer = DeepBayesianExplorer(num_states, num_actions, num_hidden)\n",
        "\n",
        "    # Simple single episode rollout for policy\n",
        "    obs, action, reward, next_obs, done, info, cum_ret = manager.single_rollout(rng, model=explorer)\n",
        "\n",
        "    # print (\"single action\",action)\n",
        "    # print (\"obs\",obs)\n",
        "    # print (\"next_obs\",next_obs)\n",
        "    # Multiple rollouts for same network (different rng, e.g. eval)\n",
        "    rng_batch = jax.random.split(rng, 4)\n",
        "    # print (\"reset_state\",manager.batch_reset(rng_batch))\n",
        "\n",
        "    obs, action, reward, next_obs, done, info, cum_ret = manager.batch_rollout(\n",
        "        rng_batch, model=explorer\n",
        "    )\n",
        "    rng_batch,rng_update = batch_random_split(rng_batch, 2)\n",
        "    update_info = manager.batch_update(rng_update, explorer, obs, action,next_obs,done,info)\n",
        "    info.update(update_info)\n",
        "   # show_variable(model,\"explorer after\")\n",
        "    print (\"info mi\",info[\"mi\"])\n",
        "    print (\"info kl\",info[\"kl\"])\n",
        "\n",
        "    next_state = info[\"next_state\"]\n",
        "    # print(\"next_state\",next_state)\n",
        "    print(jax.tree_util.tree_map(lambda x: x.shape, next_state))\n",
        "\n",
        "    def get_last_state(timestep):\n",
        "      return jax.tree_util.tree_map(\n",
        "          lambda x: x[:, -1, ...] if isinstance(x, jnp.ndarray) and x.ndim >= 2 else x,\n",
        "          timestep\n",
        "      )\n",
        "\n",
        "    last_state = get_last_state(next_state)\n",
        "    obs, action, reward, next_obs, done, info, cum_ret = manager.batch_rollout(\n",
        "        rng_batch, model=explorer, env_state=last_state\n",
        "    )\n",
        "    info.update(update_info)\n",
        "    print (\"after info mi\",info[\"mi\"])\n",
        "    print (\"after info kl\",info[\"kl\"])\n",
        "    return info\n",
        "\n",
        "    # Multiple rollouts for different networks + rng (e.g. for ES)\n",
        "    batch_params = jax.tree_map(  # Stack parameters or use different\n",
        "        lambda x: jnp.tile(x, (2, 1)).reshape(2, *x.shape), alphas\n",
        "    )\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "_bwwf8Rr9LgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj1F4BxZi6zl"
      },
      "source": [
        "## Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N51zTDR4uVBz"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jnp.set_printoptions(precision=2,suppress=True)\n",
        "from jax.scipy.special import digamma, gammaln, kl_div\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "import time\n",
        "import flax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from typing import Sequence, NamedTuple, Any, Dict\n",
        "import distrax\n",
        "import gymnax\n",
        "import functools\n",
        "from gymnax.environments import spaces\n",
        "from gymnax.wrappers import FlattenObservationWrapper, LogWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optax\n",
        "from flax.nnx.helpers import TrainState\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWRVL5GouaZy"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_JNx-HGKjCFg"
      },
      "outputs": [],
      "source": [
        "class MyTrainState(TrainState):\n",
        "    vars: nnx.Variable\n",
        "    others: nnx.State\n",
        "\n",
        "    @property\n",
        "    def need_train(self):\n",
        "        return len(self.params) > 0\n",
        "\n",
        "is_trainable = lambda path, node: ( node.type == nnx.Param and\n",
        "    True in [ 'trainable' in t for t in path] )\n",
        "\n",
        "def train_state_from_model(model, tx=optax.adam(0.02)):\n",
        "  graphdef, trainable_params, vars, others = nnx.split(model, is_trainable, nnx.Variable,...)\n",
        "  return MyTrainState.create(params=trainable_params, tx=tx, vars=vars, others=others, graphdef=graphdef)\n",
        "\n",
        "def train_state_update_model(model,state):\n",
        "    graphdef, trainable_params, vars, others = nnx.split(model,is_trainable, nnx.Variable,...)\n",
        "    return state.replace(vars=vars,others=others)\n",
        "\n",
        "def model_from_train_state(state):\n",
        "    return nnx.merge(state.graphdef, state.params, state.vars,state.others)\n",
        "\n",
        "# NUM_UPDATES x NUM_ENVS x NUM_STEPS\n",
        "class Transition(NamedTuple):\n",
        "    obs: jnp.ndarray\n",
        "    action: jnp.ndarray\n",
        "    reward: jnp.ndarray\n",
        "    next_obs: jnp.ndarray\n",
        "    done: jnp.ndarray\n",
        "    info: {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3evYa485dgB"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KadyzIyZ5f3J"
      },
      "outputs": [],
      "source": [
        "def make_train(config):\n",
        "  config[\"NUM_UPDATES\"] = (config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"]// config[\"NUM_ENVS\"])\n",
        "\n",
        "  rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "  rng_batch = jax.random.split(rng, config[\"NUM_ENVS\"])\n",
        "\n",
        "  # manager = CustomRolloutWrapper(env_or_name=\"MiniGrid-EmptyRandom-8x8\", num_env_steps=3)\n",
        "  manager = UnsupervisedRolloutWrapper(env_or_name=\"MiniGrid-EmptyRandom-8x8\",num_env_steps=3)\n",
        "\n",
        "  obs_shape = (8, 8, 2)\n",
        "  num_states = 13\n",
        "  num_actions = 4\n",
        "  num_hidden = 512\n",
        "\n",
        "  # model\n",
        "  if config[\"MODEL_NAME\"] == \"XlandDeepSACBayesianExplorer\":\n",
        "    model = DeepBayesianExplorer(num_states, num_actions, num_hidden)\n",
        "\n",
        "  @nnx.jit\n",
        "  def _train_step(state:MyTrainState, rng_loss, obs, action,next_obs,done,info):\n",
        "\n",
        "    def loss_fn(graphdef, params, vars, others):\n",
        "      model = nnx.merge(graphdef, params, vars, others)\n",
        "      return model.loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "      # return model.batch_loss(rng_loss,obs, action,next_obs,done,info).mean()\n",
        "\n",
        "    def opt_step(state:MyTrainState, unused):\n",
        "      grads = jax.grad(loss_fn, 1)(state.graphdef, state.params, state.vars, state.others)\n",
        "      return state.apply_gradients(grads=grads), None\n",
        "\n",
        "    state, _ = jax.lax.scan(opt_step, state, None, config[\"OPT_STEPS\"])\n",
        "    return state\n",
        "\n",
        "  @nnx.jit\n",
        "  def _rollout_and_update_step(runner_state, unused):\n",
        "    train_state, env_state, rng_batch= runner_state\n",
        "\n",
        "    model = model_from_train_state(train_state)\n",
        "    rng_batch, rng_step, rng_update, rng_loss = batch_random_split(rng_batch, 4)\n",
        "\n",
        "    rollout_results = manager.batch_rollout(rng_batch, model, env_state)\n",
        "    obs, action, reward, next_obs, done, info, cum_return = rollout_results\n",
        "\n",
        "    next_state = info[\"next_state\"]\n",
        "\n",
        "    def get_last_state(timestep):\n",
        "      return jax.tree_util.tree_map(\n",
        "          lambda x: x[:, -1, ...] if isinstance(x, jnp.ndarray) and x.ndim >= 2 else x,\n",
        "          timestep\n",
        "      )\n",
        "\n",
        "    last_state = get_last_state(next_state)\n",
        "\n",
        "    transition = Transition(obs, action, reward, next_obs, done, info)\n",
        "\n",
        "    update_info = manager.batch_update(rng_update, model, obs, action, next_obs, done, info)\n",
        "    info.update(update_info)\n",
        "    train_state = train_state_update_model(model, train_state)\n",
        "\n",
        "    if train_state.need_train:\n",
        "      train_state = _train_step(train_state, rng_loss, obs, action, next_obs, done, info)\n",
        "\n",
        "    runner_state = (train_state, last_state, rng_batch)\n",
        "    return runner_state, transition\n",
        "\n",
        "  def train(rng_batch, model, manager):\n",
        "\n",
        "    rng_batch, rng_reset = batch_random_split(rng_batch, 2)\n",
        "    start_state = manager.batch_reset(rng_reset)\n",
        "\n",
        "    if config[\"TX\"] == \"adamw\":\n",
        "      tx = optax.adamw(config[\"LR\"])\n",
        "    elif config[\"TX\"] == \"sgd\":\n",
        "      tx = optax.sgd(config[\"LR\"])\n",
        "    else:\n",
        "      tx = None\n",
        "      assert False, config[\"TX\"] + \"is not available\"\n",
        "    train_state = train_state_from_model(model, tx)\n",
        "    runner_state = (train_state, start_state, rng_batch)\n",
        "    runner_state, transitions = jax.lax.scan(_rollout_and_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
        "\n",
        "    return {\"runner_state\": runner_state, \"transitions\": transitions}\n",
        "\n",
        "  return train, model, manager, rng_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bdrRSnBzwQfd"
      },
      "outputs": [],
      "source": [
        "def experiment(config):\n",
        "  print(config)\n",
        "  train_fn, model, manager, rng_batch = make_train(config)\n",
        "  train_jit = nnx.jit(train_fn)\n",
        "\n",
        "  out = jax.block_until_ready(train_fn(rng_batch, model, manager))\n",
        "  print(\"data shape:\", jax.tree_util.tree_map(lambda x: x.shape, out[\"transitions\"]))\n",
        "\n",
        "  train_state, env_state, rng_batch= out[\"runner_state\"]\n",
        "\n",
        "  model = model_from_train_state(train_state)\n",
        "\n",
        "  if \"mi\" in out[\"transitions\"].info:\n",
        "    # Create figure and axis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Sample JAX NumPy arrays (replace these with your actual arrays)\n",
        "        #  print (out[\"transitions\"].info)\n",
        "        eig_array = out[\"transitions\"].info[\"mi\"].reshape(-1)\n",
        "        big_array = out[\"transitions\"].info[\"kl\"].reshape(-1)\n",
        "        # Plot both arrays\n",
        "        plt.plot(eig_array, label='EIG', marker='o', linestyle='-', color='blue')\n",
        "        plt.plot(big_array, label='BIG', marker='s', linestyle='-', color='red')\n",
        "\n",
        "        if \"smi\" in out[\"transitions\"].info:\n",
        "          smi_array = out[\"transitions\"].info[\"smi\"].reshape(-1)\n",
        "          plt.plot(smi_array, label='SMI', marker='^', linestyle='-', color='green')\n",
        "\n",
        "        # add labels and title\n",
        "        plt.xlabel('Num of Updates')\n",
        "        plt.ylabel('Information Gain')\n",
        "        Title = \"InfoGains for\" + config[\"MODEL_NAME\"]\n",
        "        Title = Title + \"Total InfoGains\" + \"{:10.4f}\".format(big_array.sum().item())\n",
        "        Title = Title +  \" with Seed\" +str(config[\"SEED\"])\n",
        "        plt.title(Title)\n",
        "\n",
        "        # add grid and legend\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "  if \"l_prec\" in out[\"transitions\"].info:\n",
        "      l_prec_mean = out[\"transitions\"].info[\"l_prec\"].mean(axis=(1,2,3), keepdims=False)\n",
        "      mean_error = out[\"transitions\"].info[\"mean_error\"].mean(axis=(1,2), keepdims=False)\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.plot(l_prec_mean, label='L_prec', marker='o', linestyle='-', color='blue')\n",
        "      plt.plot(mean_error, label='Mean Error', marker='s', linestyle='-', color='yellow')\n",
        "\n",
        "      plt.xlabel('Num of Updates')\n",
        "      plt.ylabel('Mean Precision')\n",
        "      Title = \"Comparison of Mean Precisions\"\n",
        "\n",
        "      plt.title(Title)\n",
        "\n",
        "      plt.grid(alpha=0.3)\n",
        "      plt.legend()\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(Title.replace(\" \",\"_\")+'.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
        "      plt.show()\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ae4851mIw5tt"
      },
      "outputs": [],
      "source": [
        "env_name = \"MiniGrid-EmptyRandom-8x8\"\n",
        "NUM_ENVS = 1 # @param[1,2,4,8,16,32]\n",
        "TOTAL_TIMESTEPS = 16384 # @param [2048,16384,131072,1048576] {\"type\":\"raw\"}\n",
        "DEPTH = 1 # @param [1,2,4] {\"type\":\"raw\"}\n",
        "NUM_STEPS = 8 # @param [1,2,4,8,16] {\"type\":\"raw\"}\n",
        "NUM_HIDDEN = 128 # @param [32,64,128,256] {\"type\":\"raw\"}\n",
        "WD = 0.1 # @param [0,0.1,0.01,0.001] {\"type\":\"raw\"}\n",
        "MODEL_NAME = \"XlandDeepSACBayesianExplorer\"  #@param [\"DeepSACBayesianExplorer\",\"RandomExplorer\",\"XlandDeepSACBayesianExplorer\"]\n",
        "config = {\n",
        "    \"NUM_ENVS\": NUM_ENVS,    #\n",
        "    \"WD\": WD,\n",
        "    \"NUM_STEPS\": NUM_STEPS,   #steps of roll out between update\n",
        "    \"NUM_OOF\": NUM_HIDDEN, # num hidden for now\n",
        "    \"SAC_D_STEPS\": 4,\n",
        "    \"ENV_NAME\":env_name,\n",
        "    \"SAC_STEP_SIZE\": 1.0,\n",
        "    \"SEED\": 423,         #highly stochastic\n",
        "    \"TOTAL_TIMESTEPS\": TOTAL_TIMESTEPS,   #total steps for all envs\n",
        "    \"NUM_HIDDEN\":NUM_HIDDEN,\n",
        "    \"TX\":\"adamw\",\n",
        "    \"DEPTH\":DEPTH,\n",
        "    \"LR\":2e-4,\n",
        "    \"OPT_STEPS\":8,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DEBUG\": False,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D58-pcyQxuA2"
      },
      "outputs": [],
      "source": [
        "out = experiment(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQSQh8M2lsdJ"
      },
      "source": [
        "## IQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNXxwUJSmMkU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, NamedTuple, Optional, Sequence, Tuple\n",
        "\n",
        "import distrax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "import wandb\n",
        "from flax.training.train_state import TrainState\n",
        "from omegaconf import OmegaConf\n",
        "from pydantic import BaseModel\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_triton_gemm_any=True\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NstW8rlmhcB"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcEIlxtMmkI3"
      },
      "outputs": [],
      "source": [
        "class IQLConfig(BaseModel):\n",
        "    # GENERAL\n",
        "    algo: str = \"IQL\"\n",
        "    project: str = \"train-IQL\"\n",
        "    env_name: str = \"MiniGrid-EmptyRandom-6x6\"\n",
        "    seed: int = 42\n",
        "    eval_episodes: int = 5\n",
        "    log_interval: int = 100\n",
        "    eval_interval: int = 100000\n",
        "    batch_size: int = 256\n",
        "    max_steps: int = int(1e6)\n",
        "    n_jitted_updates: int = 8\n",
        "    # DATASET\n",
        "    data_size: int = int(1e6)\n",
        "    normalize_state: bool = False\n",
        "    normalize_reward: bool = True\n",
        "    # NETWORK\n",
        "    hidden_dims: Tuple[int, int] = (256, 256)\n",
        "    actor_lr: float = 3e-4\n",
        "    value_lr: float = 3e-4\n",
        "    critic_lr: float = 3e-4\n",
        "    layer_norm: bool = True\n",
        "    opt_decay_schedule: bool = True\n",
        "    # IQL SPECIFIC\n",
        "    expectile: float = (\n",
        "        0.7  # FYI: for Hopper-me, 0.5 produce better result. (antmaze: expectile=0.9)\n",
        "    )\n",
        "    beta: float = (\n",
        "        3.0  # FYI: for Hopper-me, 6.0 produce better result. (antmaze: beta=10.0)\n",
        "    )\n",
        "    tau: float = 0.005\n",
        "    discount: float = 0.99\n",
        "\n",
        "    def __hash__(\n",
        "        self,\n",
        "    ):  # make config hashable to be specified as static_argnums in jax.jit.\n",
        "        return hash(self.__repr__())\n",
        "\n",
        "\n",
        "conf_dict = OmegaConf.from_cli()\n",
        "config = IQLConfig(**conf_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW1HYQS-m0BK"
      },
      "source": [
        "### Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nqPOs5Umxhk"
      },
      "outputs": [],
      "source": [
        "def default_init(scale: Optional[float] = jnp.sqrt(2)):\n",
        "    return nn.initializers.orthogonal(scale)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "    activate_final: bool = False\n",
        "    kernel_init: Callable[[Any, Sequence[int], Any], jnp.ndarray] = default_init()\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        for i, hidden_dims in enumerate(self.hidden_dims):\n",
        "            x = nn.Dense(hidden_dims, kernel_init=self.kernel_init)(x)\n",
        "            if i + 1 < len(self.hidden_dims) or self.activate_final:\n",
        "                if self.layer_norm:  # Add layer norm after activation\n",
        "                    x = nn.LayerNorm()(x)\n",
        "                x = self.activations(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    activations: Callable[[jnp.ndarray], jnp.ndarray] = nn.relu\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray, actions: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        actions = jax.nn.one_hot(actions, num_classes=4) #one-hot encoding\n",
        "        flat_observations = observations.reshape(batch_size, -1)\n",
        "        inputs = jnp.concatenate([flat_observations, actions], axis=-1)\n",
        "        critic = MLP((*self.hidden_dims, 1), activations=self.activations)(inputs)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "def ensemblize(cls, num_qs, out_axes=0, **kwargs):\n",
        "    split_rngs = kwargs.pop(\"split_rngs\", {})\n",
        "    return nn.vmap(\n",
        "        cls,\n",
        "        variable_axes={\"params\": 0},\n",
        "        split_rngs={**split_rngs, \"params\": True},\n",
        "        in_axes=None,\n",
        "        out_axes=out_axes,\n",
        "        axis_size=num_qs,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "class ValueCritic(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    layer_norm: bool = False\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observations: jnp.ndarray) -> jnp.ndarray:\n",
        "        batch_size = observations.shape[0]\n",
        "        obs_flat = observations.reshape(batch_size, -1)\n",
        "        critic = MLP((*self.hidden_dims, 1), layer_norm=self.layer_norm)(obs_flat)\n",
        "        return jnp.squeeze(critic, -1)\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    hidden_dims: Sequence[int]\n",
        "    action_dim: int\n",
        "    log_std_min: Optional[float] = -5.0\n",
        "    log_std_max: Optional[float] = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(\n",
        "        self, observations: jnp.ndarray, temperature: float = 1.0\n",
        "    ) -> distrax.Distribution:\n",
        "        outputs = MLP(\n",
        "            self.hidden_dims,\n",
        "            activate_final=True,\n",
        "        )(observations)\n",
        "\n",
        "        means = nn.Dense(\n",
        "            self.action_dim, kernel_init=default_init()\n",
        "        )(outputs)\n",
        "        log_stds = self.param(\"log_stds\", nn.initializers.zeros, (self.action_dim,))\n",
        "        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        distribution = distrax.MultivariateNormalDiag(\n",
        "            loc=means, scale_diag=jnp.exp(log_stds) * temperature\n",
        "        )\n",
        "        return distribution\n",
        "\n",
        "class CatPolicy(nn.Module):\n",
        "  hidden_dims : Sequence[int]\n",
        "  action_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, observations: jnp.ndarray, temperature: float = 1.0) -> distrax.Distribution:\n",
        "    x = observations.reshape(observations.shape[0], -1) # flatten\n",
        "    outputs = MLP(self.hidden_dims, activate_final=True)(x)\n",
        "    logits = nn.Dense(self.action_dim, kernel_init=default_init())(outputs)\n",
        "    distribution = distrax.Categorical(logits=logits)\n",
        "    return distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvkJdJ9EnpNT"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtarzRCfprRp"
      },
      "outputs": [],
      "source": [
        "print(jtu.tree_map(jnp.shape, replay_buffer))\n",
        "print(type(replay_buffer))\n",
        "print(replay_buffer[\"dones\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJxWv8DinrFW"
      },
      "outputs": [],
      "source": [
        "class Transition(NamedTuple):\n",
        "    observations: jnp.ndarray\n",
        "    actions: jnp.ndarray\n",
        "    rewards: jnp.ndarray\n",
        "    next_observations: jnp.ndarray\n",
        "    dones: jnp.ndarray\n",
        "    dones_float: jnp.ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga3QaSnJntei"
      },
      "outputs": [],
      "source": [
        "def get_normalization(dataset: Transition) -> float:\n",
        "    # into numpy.ndarray\n",
        "    dataset = jax.tree_util.tree_map(lambda x: np.array(x), dataset)\n",
        "    returns = []\n",
        "    ret = 0\n",
        "    for r, term in zip(dataset.rewards, dataset.dones_float):\n",
        "        ret += r\n",
        "        if term:\n",
        "            returns.append(ret)\n",
        "            ret = 0\n",
        "    return (max(returns) - min(returns)) / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4-51Dpin7az"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(\n",
        "     dataset: dict, config: IQLConfig, clip_to_eps: bool = True, eps: float = 1e-5\n",
        ") -> Transition:\n",
        "\n",
        "    if clip_to_eps:\n",
        "        lim = 1 - eps\n",
        "        dataset[\"actions\"] = jnp.clip(dataset[\"actions\"], -lim, lim)\n",
        "\n",
        "    # dones_float = np.zeros_like(dataset['dones'])\n",
        "\n",
        "    # # for i in range(len(dones_float) - 1):\n",
        "    # #     print(i)\n",
        "    # #     if np.linalg.norm(dataset['observations'][i + 1] -\n",
        "    # #                         dataset['next_observations'][i]\n",
        "    # #                         ) > 1e-6 or dataset['dones'][i] == True:\n",
        "    # #         dones_float[i] = 1\n",
        "    # #     else:\n",
        "    # #         dones_float[i] = 0\n",
        "    # dones_float[-1] = 1\n",
        "\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    obs = dataset['observations']         # shape: (N, 7, 7, 2)\n",
        "    next_obs = dataset['next_observations']  # shape: (N, 7, 7, 2)\n",
        "    dones = dataset['dones']              # shape: (N,)\n",
        "\n",
        "    # 展平每个 observation\n",
        "    obs_flat = obs[1:].reshape((obs.shape[0] - 1, -1))           # shape: (N-1, 98)\n",
        "    next_obs_flat = next_obs[:-1].reshape((next_obs.shape[0] - 1, -1))  # shape: (N-1, 98)\n",
        "\n",
        "    # 对每个样本求 L2 范数\n",
        "    obs_diff = jnp.linalg.norm(obs_flat - next_obs_flat, axis=1)   # shape: (N-1,)\n",
        "    obs_flag = obs_diff > 1e-6\n",
        "    done_flag = dones[:-1] == True\n",
        "\n",
        "    dones_float = jnp.zeros_like(dones, dtype=jnp.float32)\n",
        "    dones_float = dones_float.at[:-1].set(jnp.logical_or(obs_flag, done_flag).astype(jnp.float32))\n",
        "    dones_float = dones_float.at[-1].set(1.0)\n",
        "\n",
        "    dataset = Transition(\n",
        "        observations=jnp.array(dataset[\"observations\"], dtype=jnp.float32),\n",
        "        actions=jnp.array(dataset[\"actions\"], dtype=jnp.float32),\n",
        "        rewards=jnp.array(dataset[\"rewards\"], dtype=jnp.float32),\n",
        "        next_observations=jnp.array(dataset[\"next_observations\"], dtype=jnp.float32),\n",
        "        dones=jnp.array(dataset[\"dones\"], dtype=jnp.float32),\n",
        "        dones_float=jnp.array(dones_float, dtype=jnp.float32),\n",
        "    )\n",
        "\n",
        "    # normalize states\n",
        "    # obs_mean, obs_std = 0, 1\n",
        "    # if config.normalize_state:\n",
        "    #     obs_mean = dataset.observations.mean(0)\n",
        "    #     obs_std = dataset.observations.std(0)\n",
        "    #     dataset = dataset._replace(\n",
        "    #         observations=(dataset.observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #         next_observations=(dataset.next_observations - obs_mean) / (obs_std + 1e-5),\n",
        "    #     )\n",
        "    # # normalize rewards\n",
        "    # if config.normalize_reward:\n",
        "    #     normalizing_factor = get_normalization(dataset)\n",
        "    #     dataset = dataset._replace(rewards=dataset.rewards / normalizing_factor)\n",
        "\n",
        "    # shuffle data and select the first data_size samples\n",
        "    # data_size = min(config.data_size, len(dataset.observations))\n",
        "    # rng = jax.random.PRNGKey(config.seed)\n",
        "    # rng, rng_permute, rng_select = jax.random.split(rng, 3)\n",
        "    # perm = jax.random.permutation(rng_permute, len(dataset.observations))\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[perm], dataset)\n",
        "    # assert len(dataset.observations) >= data_size\n",
        "    # dataset = jax.tree_util.tree_map(lambda x: x[:data_size], dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlDMKO3toAvl"
      },
      "outputs": [],
      "source": [
        "def expectile_loss(diff, expectile=0.8) -> jnp.ndarray:\n",
        "    weight = jnp.where(diff > 0, expectile, (1 - expectile))\n",
        "    return weight * (diff**2)\n",
        "\n",
        "def target_update(\n",
        "    model: TrainState, target_model: TrainState, tau: float\n",
        ") -> TrainState:\n",
        "    new_target_params = jax.tree_util.tree_map(\n",
        "        lambda p, tp: p * tau + tp * (1 - tau), model.params, target_model.params\n",
        "    )\n",
        "    return target_model.replace(params=new_target_params)\n",
        "\n",
        "\n",
        "def update_by_loss_grad(\n",
        "    train_state: TrainState, loss_fn: Callable\n",
        ") -> Tuple[TrainState, jnp.ndarray]:\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grad = grad_fn(train_state.params)\n",
        "    new_train_state = train_state.apply_gradients(grads=grad)\n",
        "    return new_train_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jfs8WxhoP3b"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3ZiQLAwoV_W"
      },
      "outputs": [],
      "source": [
        "class IQLTrainState(NamedTuple):\n",
        "    rng: jax.random.PRNGKey\n",
        "    critic: TrainState\n",
        "    target_critic: TrainState\n",
        "    value: TrainState\n",
        "    actor: TrainState\n",
        "\n",
        "class IQL(object):\n",
        "\n",
        "    @classmethod\n",
        "    def update_critic(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        next_v = train_state.value.apply_fn(\n",
        "            train_state.value.params, batch.next_observations\n",
        "        )\n",
        "        target_q = batch.rewards + config.discount * (1 - batch.dones) * next_v\n",
        "\n",
        "        def critic_loss_fn(\n",
        "            critic_params: flax.core.FrozenDict[str, Any]\n",
        "        ) -> jnp.ndarray:\n",
        "            q1, q2 = train_state.critic.apply_fn(\n",
        "                critic_params, batch.observations, batch.actions\n",
        "            )\n",
        "            critic_loss = ((q1 - target_q) ** 2 + (q2 - target_q) ** 2).mean()\n",
        "            return critic_loss\n",
        "\n",
        "        new_critic, critic_loss = update_by_loss_grad(\n",
        "            train_state.critic, critic_loss_fn\n",
        "        )\n",
        "        return train_state._replace(critic=new_critic), critic_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_value(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        q1, q2 = train_state.target_critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jax.lax.stop_gradient(jnp.minimum(q1, q2))\n",
        "        def value_loss_fn(value_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            v = train_state.value.apply_fn(value_params, batch.observations)\n",
        "            value_loss = expectile_loss(q - v, config.expectile).mean()\n",
        "            return value_loss\n",
        "\n",
        "        new_value, value_loss = update_by_loss_grad(train_state.value, value_loss_fn)\n",
        "        return train_state._replace(value=new_value), value_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_actor(\n",
        "        self, train_state: IQLTrainState, batch: Transition, config: IQLConfig\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        v = train_state.value.apply_fn(train_state.value.params, batch.observations)\n",
        "        q1, q2 = train_state.critic.apply_fn(\n",
        "            train_state.target_critic.params, batch.observations, batch.actions\n",
        "        )\n",
        "        q = jnp.minimum(q1, q2)\n",
        "        exp_a = jnp.exp((q - v) * config.beta)\n",
        "        exp_a = jnp.minimum(exp_a, 100.0)\n",
        "        def actor_loss_fn(actor_params: flax.core.FrozenDict[str, Any]) -> jnp.ndarray:\n",
        "            dist = train_state.actor.apply_fn(actor_params, batch.observations)\n",
        "            log_probs = dist.log_prob(batch.actions.astype(jnp.int32))\n",
        "            actor_loss = -(exp_a * log_probs).mean()\n",
        "            return actor_loss\n",
        "\n",
        "        new_actor, actor_loss = update_by_loss_grad(train_state.actor, actor_loss_fn)\n",
        "        return train_state._replace(actor=new_actor), actor_loss\n",
        "\n",
        "    @classmethod\n",
        "    def update_n_times(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        dataset: Transition,\n",
        "        rng: jax.random.PRNGKey,\n",
        "        config: IQLConfig,\n",
        "    ) -> Tuple[\"IQLTrainState\", Dict]:\n",
        "        for _ in range(config.n_jitted_updates):\n",
        "            rng, subkey = jax.random.split(rng)\n",
        "            batch_indices = jax.random.randint(\n",
        "                subkey, (config.batch_size,), 0, len(dataset.observations)\n",
        "            )\n",
        "            batch = jax.tree_util.tree_map(lambda x: x[batch_indices], dataset)\n",
        "\n",
        "            train_state, value_loss = self.update_value(train_state, batch, config)\n",
        "            train_state, actor_loss = self.update_actor(train_state, batch, config)\n",
        "            train_state, critic_loss = self.update_critic(train_state, batch, config)\n",
        "            new_target_critic = target_update(\n",
        "                train_state.critic, train_state.target_critic, config.tau\n",
        "            )\n",
        "            train_state = train_state._replace(target_critic=new_target_critic)\n",
        "        return train_state, {\n",
        "            \"value_loss\": value_loss,\n",
        "            \"actor_loss\": actor_loss,\n",
        "            \"critic_loss\": critic_loss,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_action(\n",
        "        self,\n",
        "        train_state: IQLTrainState,\n",
        "        observations: np.ndarray,\n",
        "        seed: jax.random.PRNGKey,\n",
        "        temperature: float = 1.0,\n",
        "        max_action: float = 1.0,\n",
        "    ) -> jnp.ndarray:\n",
        "\n",
        "        # modified for discrete actions\n",
        "        dist = train_state.actor.apply_fn(\n",
        "            train_state.actor.params, observations, temperature=temperature\n",
        "        )\n",
        "        actions = jnp.argmax(dist.logits, axis=-1)\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXpO90HoiJq"
      },
      "source": [
        "### Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZvRadfAokla"
      },
      "outputs": [],
      "source": [
        "def create_iql_train_state(\n",
        "    rng: jax.random.PRNGKey,\n",
        "    observations: jnp.ndarray,\n",
        "    actions: jnp.ndarray,\n",
        "    config: IQLConfig,\n",
        ") -> IQLTrainState:\n",
        "    rng, actor_rng, critic_rng, value_rng = jax.random.split(rng, 4)\n",
        "    # initialize actor\n",
        "    action_dim = 4\n",
        "\n",
        "    # Gaussian Model\n",
        "    # actor_model = GaussianPolicy(\n",
        "    #     config.hidden_dims,\n",
        "    #     action_dim=action_dim,\n",
        "    #     log_std_min=-5.0,\n",
        "    # )\n",
        "\n",
        "    # Cat Model\n",
        "    actor_model = CatPolicy(\n",
        "        config.hidden_dims,\n",
        "        action_dim = action_dim\n",
        "    )\n",
        "\n",
        "    if config.opt_decay_schedule:\n",
        "        schedule_fn = optax.cosine_decay_schedule(-config.actor_lr, config.max_steps)\n",
        "        actor_tx = optax.chain(optax.scale_by_adam(), optax.scale_by_schedule(schedule_fn))\n",
        "    else:\n",
        "        actor_tx = optax.adam(learning_rate=config.actor_lr)\n",
        "    actor = TrainState.create(\n",
        "        apply_fn=actor_model.apply,\n",
        "        params=actor_model.init(actor_rng, observations),\n",
        "        tx=actor_tx,\n",
        "    )\n",
        "    # initialize critic\n",
        "    critic_model = ensemblize(Critic, num_qs=2)(config.hidden_dims)\n",
        "    critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    target_critic = TrainState.create(\n",
        "        apply_fn=critic_model.apply,\n",
        "        params=critic_model.init(critic_rng, observations, actions),\n",
        "        tx=optax.adam(learning_rate=config.critic_lr),\n",
        "    )\n",
        "    # initialize value\n",
        "    value_model = ValueCritic(config.hidden_dims, layer_norm=config.layer_norm)\n",
        "    value = TrainState.create(\n",
        "        apply_fn=value_model.apply,\n",
        "        params=value_model.init(value_rng, observations),\n",
        "        tx=optax.adam(learning_rate=config.value_lr),\n",
        "    )\n",
        "    return IQLTrainState(\n",
        "        rng,\n",
        "        critic=critic,\n",
        "        target_critic=target_critic,\n",
        "        value=value,\n",
        "        actor=actor,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B6VH9JgoptW"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    policy_fn, env, env_params, num_episodes: int, rng\n",
        ") -> float:\n",
        "    print(\"evaluation started\")\n",
        "    episode_returns = []\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      episode_return = 0\n",
        "\n",
        "      timestep = env.reset(env_params, _rng)\n",
        "      done = timestep.step_type == 2\n",
        "      observation = timestep.observation\n",
        "\n",
        "      while not done:\n",
        "          # potential case issue\n",
        "          obs = observation[None, ...]\n",
        "          action = policy_fn(observations=obs)\n",
        "\n",
        "          if isinstance(action, (jnp.ndarray, np.ndarray)) and action.shape == (1,):\n",
        "            action = int(action[0])\n",
        "\n",
        "          timestep = env.step(env_params, timestep, action)\n",
        "          reward = timestep.reward\n",
        "          done = timestep.step_type == 2\n",
        "          observation = timestep.observation\n",
        "\n",
        "          episode_return += reward\n",
        "      episode_returns.append(episode_return)\n",
        "    return float(jnp.mean(jnp.array(episode_returns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQkNH7rvo2mS"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    wandb.init(config=config, project=config.project)\n",
        "\n",
        "    rng = jax.random.PRNGKey(config.seed)\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "\n",
        "    env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-6x6\")\n",
        "    env = GymAutoResetWrapper(env)\n",
        "\n",
        "    dataset= preprocess_dataset(replay_buffer, config)\n",
        "\n",
        "    # create train_state\n",
        "    example_batch: Transition = jax.tree_util.tree_map(lambda x: x[0], dataset)\n",
        "    train_state: IQLTrainState = create_iql_train_state(\n",
        "        _rng,\n",
        "        example_batch.observations[None, ...],\n",
        "        example_batch.actions[None, ...],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    algo = IQL()\n",
        "    update_fn = jax.jit(algo.update_n_times, static_argnums=(3,))\n",
        "    act_fn = jax.jit(algo.get_action)\n",
        "    num_steps = config.max_steps // config.n_jitted_updates\n",
        "    eval_interval = config.eval_interval // config.n_jitted_updates\n",
        "    for i in tqdm.tqdm(range(1, num_steps + 1), smoothing=0.1, dynamic_ncols=True):\n",
        "        rng, subkey = jax.random.split(rng)\n",
        "        train_state, update_info = update_fn(train_state, dataset, subkey, config)\n",
        "\n",
        "        if i % config.log_interval == 0:\n",
        "            train_metrics = {f\"training/{k}\": v for k, v in update_info.items()}\n",
        "            wandb.log(train_metrics, step=i)\n",
        "\n",
        "        # if i % eval_interval == 0:\n",
        "        #     policy_fn = partial(\n",
        "        #         act_fn,\n",
        "        #         temperature=0.0,\n",
        "        #         seed=jax.random.PRNGKey(0),\n",
        "        #         train_state=train_state,\n",
        "        #     )\n",
        "        #     normalized_score = evaluate(\n",
        "        #         policy_fn,\n",
        "        #         env,\n",
        "        #         env_params,\n",
        "        #         rng = _rng,\n",
        "        #         num_episodes=config.eval_episodes,\n",
        "        #     )\n",
        "        #     print(i, normalized_score)\n",
        "        #     eval_metrics = {f\"{config.env_name}/normalized_score\": normalized_score}\n",
        "        #     wandb.log(eval_metrics, step=i)\n",
        "    # final evaluation\n",
        "    policy_fn = partial(\n",
        "        act_fn,\n",
        "        temperature=0.0,\n",
        "        seed=jax.random.PRNGKey(0),\n",
        "        train_state=train_state,\n",
        "    )\n",
        "    normalized_score = evaluate(\n",
        "        policy_fn,\n",
        "        env,\n",
        "        env_params,\n",
        "        rng = _rng,\n",
        "        num_episodes=config.eval_episodes,\n",
        "    )\n",
        "    print(\"Final Evaluation\", normalized_score)\n",
        "    wandb.log({f\"{config.env_name}/final_normalized_score\": normalized_score})\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNLDJZfm4Hx7"
      },
      "source": [
        "## Collect Rollouts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOVYwUxm70Qg"
      },
      "outputs": [],
      "source": [
        "from xminigrid.wrappers import GymAutoResetWrapper\n",
        "\n",
        "def build_rollout(env, env_params, num_steps):\n",
        "  def rollout(rng):\n",
        "    def _step_fn(carry, _):\n",
        "      rng, timestep = carry\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      action = jax.random.randint(_rng, shape=(), minval=0, maxval=env.num_actions(env_params))\n",
        "\n",
        "      timestep = env.step(env_params, timestep, action)\n",
        "\n",
        "      return (rng, timestep), (timestep,action)\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    timestep = env.reset(env_params, _rng)\n",
        "    rng, (transitions, actions) = jax.lax.scan(_step_fn, (rng, timestep), None, length=num_steps)\n",
        "\n",
        "    return transitions, actions\n",
        "  return rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAmVT6PtTAnA"
      },
      "outputs": [],
      "source": [
        "env, env_params = xminigrid.make(\"MiniGrid-EmptyRandom-8x8\")\n",
        "env = GymAutoResetWrapper(env)\n",
        "\n",
        "rollout_fn = jax.jit(build_rollout(env, env_params, num_steps=1e6))\n",
        "\n",
        "transitions, actions = rollout_fn(jax.random.key(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIgnrL0od9Rt"
      },
      "outputs": [],
      "source": [
        "obs_dim = env.observation_shape(env_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iqTFGUoeAM0"
      },
      "outputs": [],
      "source": [
        "print(obs_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp44_CUcTpXV"
      },
      "outputs": [],
      "source": [
        "print(\"Transitions shapes: \\n\", jtu.tree_map(jnp.shape, transitions))\n",
        "print(\"Actions shape:\", actions.shape)\n",
        "print(type(actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcUGKy_aSeK5"
      },
      "outputs": [],
      "source": [
        "def create_replay_buffer(transitions, actions):\n",
        "\n",
        "  observations = transitions.observation # (T, 7, 7, 2)\n",
        "  rewards = transitions.reward # (T,)\n",
        "  dones = transitions.step_type == 2 # (T,)\n",
        "  next_observations = jnp.concatenate([observations[1:], observations[-1:]], axis=0) #(T, 7, 7, 2)\n",
        "  actions = jnp.array(actions, dtype=jnp.int32) #(T,)\n",
        "\n",
        "  replay_buffer = {'observations': observations,\n",
        "                   'actions': actions,\n",
        "                   'rewards': rewards,\n",
        "                   'next_observations': next_observations,\n",
        "                   'dones': dones}\n",
        "\n",
        "  print(\"=== Replay Buffer 构建完成 ===\")\n",
        "  print(f\"数据点数量: {len(observations)}\")\n",
        "  print(f\"平均奖励: {jnp.mean(rewards):.4f}\")\n",
        "  print(f\"Episode结束次数: {jnp.sum(dones)}\")\n",
        "  print(f\"动作分布: {jnp.bincount(actions)}\")\n",
        "  return replay_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JzlW7Ht0M9w"
      },
      "source": [
        "Potential issue with sparse reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QygjqK1SzZr4"
      },
      "outputs": [],
      "source": [
        "replay_buffer = create_replay_buffer(transitions, actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1lsLd27zekf"
      },
      "outputs": [],
      "source": [
        "def create_batches(replay_buffer, batch_size=32, num_batches=None):\n",
        "  data_size = len(replay_buffer['observations'])\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = max(1, data_size // batch_size)\n",
        "\n",
        "  batches = []\n",
        "\n",
        "  rng = jax.random.PRNGKey(0)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yQSQh8M2lsdJ",
        "hNLDJZfm4Hx7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}